{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LSSI633 : Science des donn\u00e9es Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ) Pr\u00e9sentation du cours La science des donn\u00e9es ou \"data science\" est une discipline au croisement entre les math\u00e9matiques (surtout les statistiques ) et la programmation informatique . Si les contours de cette discipline sont encore mal d\u00e9finis, elle est en g\u00e9n\u00e9ral associ\u00e9e \u00e0 l' explosion des volumes de donn\u00e9es \u00e0 traiter (on parle aussi de \"big data\"), et \u00e0 la mise en place d'outils d' apprentissage automatique (ou \"Machine Learning\"). L'id\u00e9e est la suivante : manipuler des quantit\u00e9s massives de donn\u00e9es est devenu presque impossible pour un humain, et requiert donc des outils informatiques capables d' apprendre des donn\u00e9es de mani\u00e8re automatique. Vous trouverez dans ce cours une initiation aux sciences des donn\u00e9es, et plus particuli\u00e8rement aux 3 grandes m\u00e9thodes d'apprentissage automatique : Chapitre I : Introduction : une introduction aux concepts et enjeux de la science des donn\u00e9es. Chapitre II : Classification supervis\u00e9e : une initiation \u00e0 la classification par apprentissage supervis\u00e9. Chapitre III : R\u00e9gression : une initiation \u00e0 la r\u00e9gression par apprentissage supervis\u00e9. Chapitre IV : Partitionnement : une initiation au partitionnement, aussi connu sous le nom de classification par apprentissage non-supervis\u00e9 (\"clustering\"). L'objectif est qu'\u00e0 la fin de ce cours vous soyez capable de : Prendre du recul sur un jeu de donn\u00e9es, et savoir l' analyser / le pr\u00e9parer en vue d'un apprentissage. Identifier \u00e0 quel type de probl\u00e8me (classification, regression, partitionnement) vous \u00eates confront\u00e9s. R\u00e9fl\u00e9chir aux enjeux li\u00e9s \u00e0 chacun de ces types de probl\u00e8mes, et savoir choisir une m\u00e9thodes pertinente parmi celles vue en cours. Evaluer les performances de votre mod\u00e8le, et les interpr\u00e9ter. Ce cours vous donnera aussi des astuces d'impl\u00e9mentation en Python , principalement bas\u00e9es sur la biblioth\u00e8que Scikit-Learn . Tous les exemples de ce cours sont imaginaires ou grandement simplifi\u00e9s. Leur int\u00e9r\u00eat est purement p\u00e9dagogique. Credits \u00a9 Nicolas OUDART Remerciements \u00e0 C\u00e9cile MALLET","title":"Accueil"},{"location":"#lssi633-science-des-donnees","text":"Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ)","title":"LSSI633 : Science des donn\u00e9es"},{"location":"#presentation-du-cours","text":"La science des donn\u00e9es ou \"data science\" est une discipline au croisement entre les math\u00e9matiques (surtout les statistiques ) et la programmation informatique . Si les contours de cette discipline sont encore mal d\u00e9finis, elle est en g\u00e9n\u00e9ral associ\u00e9e \u00e0 l' explosion des volumes de donn\u00e9es \u00e0 traiter (on parle aussi de \"big data\"), et \u00e0 la mise en place d'outils d' apprentissage automatique (ou \"Machine Learning\"). L'id\u00e9e est la suivante : manipuler des quantit\u00e9s massives de donn\u00e9es est devenu presque impossible pour un humain, et requiert donc des outils informatiques capables d' apprendre des donn\u00e9es de mani\u00e8re automatique. Vous trouverez dans ce cours une initiation aux sciences des donn\u00e9es, et plus particuli\u00e8rement aux 3 grandes m\u00e9thodes d'apprentissage automatique : Chapitre I : Introduction : une introduction aux concepts et enjeux de la science des donn\u00e9es. Chapitre II : Classification supervis\u00e9e : une initiation \u00e0 la classification par apprentissage supervis\u00e9. Chapitre III : R\u00e9gression : une initiation \u00e0 la r\u00e9gression par apprentissage supervis\u00e9. Chapitre IV : Partitionnement : une initiation au partitionnement, aussi connu sous le nom de classification par apprentissage non-supervis\u00e9 (\"clustering\"). L'objectif est qu'\u00e0 la fin de ce cours vous soyez capable de : Prendre du recul sur un jeu de donn\u00e9es, et savoir l' analyser / le pr\u00e9parer en vue d'un apprentissage. Identifier \u00e0 quel type de probl\u00e8me (classification, regression, partitionnement) vous \u00eates confront\u00e9s. R\u00e9fl\u00e9chir aux enjeux li\u00e9s \u00e0 chacun de ces types de probl\u00e8mes, et savoir choisir une m\u00e9thodes pertinente parmi celles vue en cours. Evaluer les performances de votre mod\u00e8le, et les interpr\u00e9ter. Ce cours vous donnera aussi des astuces d'impl\u00e9mentation en Python , principalement bas\u00e9es sur la biblioth\u00e8que Scikit-Learn . Tous les exemples de ce cours sont imaginaires ou grandement simplifi\u00e9s. Leur int\u00e9r\u00eat est purement p\u00e9dagogique.","title":"Pr\u00e9sentation du cours"},{"location":"#credits","text":"\u00a9 Nicolas OUDART Remerciements \u00e0 C\u00e9cile MALLET","title":"Credits"},{"location":"Chap1_Introduction/","text":"Chapitre I : Introduction aux sciences des donn\u00e9es Ce chapitre porte sur les grands concepts et les enjeux des sciences des donn\u00e9es. Analyse de donn\u00e9es L'explosion des capacit\u00e9s de stockage de donn\u00e9es est \u00e0 l'origine d'une explosion de la taille des jeux de donn\u00e9es \u00e0 traiter. D'o\u00f9 la n\u00e9cessit\u00e9 de trouver de nouvelles mani\u00e8res de manipuler, traiter, analyser et interpr\u00e9ter nos donn\u00e9es. La 1\u00e8re \u00e9tape lorsque l'on est confront\u00e9 \u00e0 un vaste jeu de donn\u00e9es est toujours de l'analyser, afin d'essayer de le comprendre : Quels types de donn\u00e9es contient-il ? Ces donn\u00e9es sont-elles de qualit\u00e9 ? Comment ces donn\u00e9es sont-elles r\u00e9parties ? Peut-on tisser des liens entre les diff\u00e9rentes variables ? Peut-on regrouper les diff\u00e9rentes r\u00e9alisations de ces variables en groupes ? Cette \u00e9tape est essentielle si l'on veut par la suite entrainer un mod\u00e8le \u00e0 \"apprendre\" de nos donn\u00e9es. Nature et type des donn\u00e9es Une des difficult\u00e9s rencontr\u00e9es en sciences des donn\u00e9es provient de la grande vari\u00e9t\u00e9s des donn\u00e9es . Donn\u00e9es de diff\u00e9rentes natures Tout d'abord, les variables \u00e9tudi\u00e9es peuvent \u00eatre de nature diff\u00e9rente : Une donn\u00e9e quantitative continue peut prendre n'importe quelle valeur num\u00e9rique : par exemple, le prix d'un kilo de farine. Une donn\u00e9e quantitative discr\u00e8te ne peut prendre qu'un nombre fini de valeurs num\u00e9riques dans un intervalle : par exemple, le nombre de p\u00e9pites de chocolats dans une brioche. Une donn\u00e9e qualitative nominale est descriptive sans ordre hi\u00e9rarchique : par exemple, la r\u00e9gion d'origine d'une p\u00e2tisserie. Une donn\u00e9e qualitative ordinale est descriptive avec un ordre hi\u00e9rarchique : par exemple, le niveau de cuisson d'une baguette de pain (blanche, pas trop cuite, bien cuite). Donn\u00e9es multidimensionnelles Les donn\u00e9es \u00e9tudi\u00e9es peuvent aussi \u00eatre multidimensionnelles . En effet, dans la pluplart des situations, notre jeu de donn\u00e9es peut se mettre sous la forme d'un tableau, dont Les colonnes correspondront aux \" variables \". Les lignes correspondront aux \" individus \" : les diff\u00e9rentes r\u00e9alisations de ces variables. L'ensemble des individus sera nomm\u00e9 \" population \", une s\u00e9lection des individus un \" \u00e9chantillon \". Voici un exemple de jeu de donn\u00e9es multidimensionnelles : Brioche n\u00b01 Poids (g) Nombre de p\u00e9pites de chocolat Prix (\u20ac) 1 70 13 3.5 2 80 17 3.6 3 85 15 3.7 4 83 16 3.4 5 76 18 3.3 6 78 13 3.5 Nous avons ici 6 individus, les brioches, pour lesquelles nous avons mesur\u00e9 3 variables, le poids, le nombre de p\u00e9pites de chocolat, et le prix. Astuce Python Pour stocker puis manipuler des donn\u00e9es multidimensionnelles, on utilise souvent en Python un type de conteneur de la biblioth\u00e8que Pandas : les \" DataFrames \". Les DataFrames se pr\u00e9sentent comme des tableaux pouvant contenir des variables de types diff\u00e9rents, avec un label associ\u00e9 \u00e0 chaque colonne du tableau (variable). Nous reparlerons de Pandas plus loin dans ce chapitre. Donn\u00e9es structur\u00e9es Enfin, les donn\u00e9es \u00e9tudi\u00e9es peuvent \u00eatre structur\u00e9es . On entend par l\u00e0 que des donn\u00e9es peuvent avoir un coh\u00e9rence chronologique (s\u00e9rie temporelle, un son) ou spatiale (une carte, une image, un texte, une vid\u00e9o). Par exemple, dans le cas d'une image : Chaque pixel de l'image doit \u00eatre compris dans le contexte global de l'image. Il est \u00e9vident que changer la position des pixels les uns par rapport aux autres change le jeu de donn\u00e9es : Dans certains cas, l' ordre des donn\u00e9es est donc en soit une information n\u00e9cessaire \u00e0 leur interpr\u00e9tation. Vous l'aurez compris, la nature des donn\u00e9es, leur dimensionnalit\u00e9, ainsi que leur structure, peuvent rendre leur compr\u00e9hension difficile . Nous allons dans la suite voir comment on peut essayer de tirer des informations pertinentes de nos donn\u00e9es. Visualisation graphique La 1\u00e8re \u00e9tape lorsque l'on cherche \u00e0 comprendre ses donn\u00e9es, c'est d'essayer de les visualiser de mani\u00e8re pertinente. Nous allons voir les types de repr\u00e9sentations graphiques les plus classiques pour visualiser un jeu de donn\u00e9es. Courbes et nuages de points : Lorsque l'on veut afficher les diff\u00e9rentes r\u00e9alisations de 2 variables \\(X\\) et \\(Y\\) l'une en fonction de l'autre , on va classiquement utiliser une courbe ou un nuage de points . Si les donn\u00e9es ne sont pas structur\u00e9es , on peut utiliser un nuage de points , qui va simplement afficher chaque r\u00e9alisation comme un point sur le graphique. Si les donn\u00e9es sont structur\u00e9es , on peut tracer des lignes entre les diff\u00e9rentes r\u00e9alisations, dans l'ordre, ce qui va donner une courbe . Diagrammes en barres et histogrammes : Lorsque l'on veut rapidement comparer des quantit\u00e9s les unes aux autres, on va classiquement utiliser un diagramme en barres ou un histogramme . Si on a des individus \\(A\\) , \\(B\\) et \\(C\\) et que l'on veut comparer les valeurs d'une variable \\(X\\) pour ces 3 individus, on peut utiliser un diagramme en barres . On peut \u00e9galement utiliser un diagramme en barres pour afficher le nombre d'occurences d'une \u00e9tiquette d'une variable qualitative. Si on veut repr\u00e9senter la distribution des valeurs d'une variable \\(X\\) parmi les diff\u00e9rents individus d'une population, on va utiliser un histogramme . Un histogramme affiche le nombre d'occurences \\(N\\) des valeurs sur un intervalle de \\(X\\) . Ceci implique donc de diviser au pr\u00e9alable les valeurs de \\(X\\) en intervalles. Bo\u00eetes \u00e0 moustaches : Lorsque l'on veut afficher de mani\u00e8re visuellement compr\u00e9hensible la distribution individus pour diff\u00e9rentes variables quantitatives \\(X\\) , \\(Y\\) et \\(Z\\) , on peut utiliser des bo\u00eetes \u00e0 moustaches . On peut \u00e9galement l'utiliser pour visualiser la distribution d'une m\u00eame variable pour diff\u00e9rentes sous-populations . On appelle \"bo\u00eete \u00e0 moustaches\" une repr\u00e9sentation graphique des principaux indicateurs de distribution d'une population pour une variable donn\u00e9e. En g\u00e9n\u00e9ral : le minimum, le 1er quartile, la m\u00e9diane, le 3\u00e8me quartile et le maximum. Nous reparlerons de ces indicateurs plus loin dans ce chapitre. Kernel Density Estimation (KDE) : La KDE ou \"estimation par noyau\" (Kernel Density Estimation) est une m\u00e9thode non-param\u00e9trique pour estimer la distribution de probabilit\u00e9 d'une variable . L'avantage est que contrairement \u00e0 un histogramme, on a des valeurs continues entre 0 et 1 (on ne divise pas en intervalles). On peut l'utiliser pour afficher sous la forme d'une courbe la densit\u00e9 de probabilit\u00e9 d'une variable \\(X\\) (KDE 1D). On peut aussi afficher la densit\u00e9 de probabilit\u00e9 d'une variable \\(X\\) par rapport \u00e0 une autre variable \\(Y\\) , sous la forme d'une carte 2D avec des isolignes de densit\u00e9 de probabilit\u00e9 (KDE 2D). Diagramme circulaire (camembert) : Lorsque l'on veut afficher des proportions \u00e0 comparer, on utilise souvent le diagramme circulaire , aussi connu sous le nom de \"diagramme camembert\". Il s'agit simplement de diviser un cercle en sections, dont la largeur repr\u00e9sentera une proportion, et le cercle complet la proportion totale. On peut par exemple l'utiliser pour v\u00e9rifier la r\u00e9partition de variables qualitatives au sein d'une base de donn\u00e9es. Graphique en aires : Lorsque l'on veut afficher l'\u00e9volution d'une proportion au cours d'un variable (souvent au cours du temps), on utilise en g\u00e9n\u00e9ral un graphique en aires . On peut par exemple l'utiliser pour v\u00e9rifier la r\u00e9partition de variables qualitatives au sein d'une base de donn\u00e9es, en fonction d'une variable quantitative \\(X\\) , par exemple le temps. En Python Astuce Python La biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, propose une m\u00e9thode \"plot\" \u00e0 ses objets \"DataFrame\" qui permet des affichages graphiques \u00e0 partir de jeux de donn\u00e9es. Il suffit donner le bon param\u00e8tre \"kind\" en entr\u00e9e pour obtenir le type d'affichage voulu : - \"line\" : une courbe. - \"scatter\" : un nuage de points. - \"bar\" : un diagramme en barres vertical. - \"barh\" : un diagramme en barres horizontal. - \"hist\" : un histogramme. - \"box\" : des bo\u00eetes \u00e0 moustaches. - \"kde\" : une \"kernel density estimation\". - \"pie\" : un diagramme circulaire. - \"area\" : un graphique en aires. Statistiques descriptives Toujours dans l'objectif de comprendre notre jeu de donn\u00e9es, on peut essayer de d\u00e9crire chaque variable par des indicateurs statistiques . Nous allons voir ici les indicateurs les plus communs en statistiques descriptives. Il est important de savoir comment ces indicateurs sont d\u00e9finis afin de comprendre les informations qu'ils donnent ou ne donnent pas sur un jeu de donn\u00e9es. Moyenne, m\u00e9diane et mode Lorsque l'on veut connaitre l'ordre de grandeur des valeurs d'une variable, l\u00e0 o\u00f9 se rassemblent la plupart des valeurs, on va utiliser un indicateur de tendance centrale : moyenne, m\u00e9diane ou mode. Il existe plusieurs fa\u00e7on de d\u00e9finir la moyenne, mais la plus connue est la moyenne arithm\u00e9tique : \\(\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i\\) On note en effet souvent \\(\\overline{x}\\) la moyenne d'une variable \\(x\\) . La m\u00e9diane est la valeur s\u00e9parant les valeurs de la variable en 2 groupes de m\u00eame taille : la moiti\u00e9 des valeurs sont sup\u00e9rieures \u00e0 la m\u00e9diane, l'autre moiti\u00e9 lui sont inf\u00e9rieures. Le mode est la valeur la plus repr\u00e9sent\u00e9e dans l'ensemble des valeurs de la variable. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".mean()\": la moyenne. - \".median()\": la m\u00e9diane. - \".mode()\": le mode. Variance et \u00e9cart-type Lorsque l'on veut savoir \u00e0 quel point les valeurs d'une variable fluctuent autour de la valeur centrale, on va utiliser des indicateurs de dispersion . Les valeurs extr\u00eames de la variable, le min et le max , pour connaitre l'\u00e9tendue de la variable. La variance est d\u00e9finie par la moyenne des carr\u00e9es des \u00e9carts \u00e0 la moyenne : \\(\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\\) L' \u00e9cart-type (souvent not\u00e9 \\(\\sigma\\) ) est la racine carr\u00e9e de la variance, soit a moyenne quadratique de \u00e9carts \u00e0 la moyenne. Contrairement \u00e0 la variance, il a l'avantage d'\u00eatre homog\u00e8ne \u00e0 la variable \u00e9tudi\u00e9e . Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".min()\" et \".max()\": le minimum et le maximum. - \".var()\": la variance. - \".std()\": l'\u00e9cart-type. Quantiles Afin d'avoir plus d'informations sur la r\u00e9partition de valeurs d'une variable, on peut g\u00e9n\u00e9raliser la notion de m\u00e9diane en utilisant ce que l'on appelle les quantiles : La division des valeurs de la variables en groupes de tailles \u00e9gales. Quartiles : 3 indicateurs en divisant les valeurs de la variable en 4 groupes (25%,50% et 75%). D\u00e9ciles : 9 indicateurs en divisant les valeurs de la variable en 10 groupes (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%). Centiles : 99 indicateurs en divisant les valeurs de la variable en 100 groupes (1%, 2%, 3%, ..., 98%, 99%). Asym\u00e9trie et kurtosis Enfin, si l'on veut une information sur la r\u00e9partition des valeurs d'une variable, sous la forme d'un indicateur unique, on va utiliser un indicateur de forme . Le coefficient d' asym\u00e9trie (\"skewness\" en anglais, souvent not\u00e9 \\(\\gamma_1\\) ) permet de quantifier le d\u00e9sequilibre de la r\u00e9partition des valeurs de la variable de chaque c\u00f4t\u00e9 de sa valeur centrale. \\(\\gamma_1 = \\frac{1}{N \\sigma^3} \\sum_{i=1}^{N} (x_i - \\overline{x})^3\\) Un coefficient n\u00e9gatif indique un d\u00e9calage \u00e0 droite, un coefficient positif un d\u00e9calage \u00e0 gauche, et un coefficient nul une distribution sym\u00e9trique. NB : Pour la loi normale, on a \\(\\gamma_1 = 0\\) . Le kurtosis (souvent not\u00e9 \\(\\gamma_2\\) ) permet de quantifier l'acuit\u00e9 ou l'applatissement de la r\u00e9partition des valeurs de la variable autour de sa valeur centrale. \\(\\gamma_2 = \\frac{1}{N \\sigma^4} \\sum_{i=1}^{N} (x_i - \\overline{x})^4\\) Un kurtosis positif est un indicateur de valeurs anormales de la variable (aux extr\u00eamit\u00e9s, aussi appel\u00e9es \"outliers\") plus fr\u00e9quentes. Un kurtosis n\u00e9gatif est un indicateur d'une distribution tr\u00e8s applatie des valeurs de la variable. NB : Pour la loi normale, on a \\(\\gamma_2 = 0\\) . Recherche de corr\u00e9lation Une fois que l'on a d\u00e9crit statistiquement les diff\u00e9rentes variables de notre jeu de donn\u00e9es, on va souvent vouloir essayer des tisser des liens entre ces variables. Cette analyse exploratoire des donn\u00e9es a 2 principales utilit\u00e9s : Voir si une ou plusieurs variables pourraient servir \u00e0 en pr\u00e9dire une ou plusieurs autres. Essayer de r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me bas\u00e9 sur ces variables. En effet, comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es sont souvent multidimensionnels. Quand la dimension des donn\u00e9es devient tr\u00e8s grande, la quantit\u00e9 de donn\u00e9es devient peu dense en comparaison, ce qui rend difficile leur interpr\u00e9tation. On appelle commun\u00e9ment ce probl\u00e8me le \"Fl\u00e9au de la dimension\" (\"curse of dimensionality\" en anglais). Nous allons voir dans un 1er temps comment essayer de d\u00e9terminer ce que l'on appelle des \" corr\u00e9lations \" entre variables. Puis nous verrons une m\u00e9thode classique de r\u00e9duction de dimension appel\u00e9e \" Analyse en Composantes Principales \". Matrice de corr\u00e9lation Une 1\u00e8re approche pour essayer de tisser des liens ou \" corr\u00e9lations \" entre les variables et de tracer ce que l'on appelle une matrice de nuages de points, ou \" scatter-matrix \" en anglais. L'id\u00e9e est d'afficher une matrice de graphiques , repr\u00e9sentant chacun une variable en fonction d'une autre , sous la forme d'un nuage de points. La diagonale n'\u00e9tant pas tr\u00e8s utile (une variable en fonction d'elle-m\u00eame), on la remplace en g\u00e9n\u00e9ral par un histogramme de la variable en question. Ce type de repr\u00e9sentation permet de d\u00e9tecter visuellement des relations entre les variables . Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"plotting.scatter_matrix()\", qui permet d'afficher une \"scatter_matrix\". Pour quantifier la corr\u00e9lation entre 2 variables \\(x\\) et \\(y\\) , on va souvent se contenter de mesurer \u00e0 quel point une relation lin\u00e9aire \\(y = a x + b\\) peut \u00eatre tir\u00e9e de ces variables. Pour cela, on va calculer le coefficient de corr\u00e9lation de Pearson : \\(r = \\frac{\\sum_{i=1}^{N} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^{N} (x_i - \\overline{x})^2 \\sum_{i=1}^{N} (y_i - \\overline{y})^2}}\\) La valeur de ce coefficient est toujours compris entre -1 et 1 : Une valeur de 1 signifie une corr\u00e9lation parfaite entre les variables. Une valeur de -1 signifie une anti-corr\u00e9lation parfaite entre les variables. Une valeur de 0 signifie une d\u00e9corr\u00e9lation parfaite entre les variables : elles sont ind\u00e9pendantes . Il y a du sens \u00e0 vouloir pr\u00e9dire une variable \u00e0 partir d'une autre si elles sont corr\u00e9l\u00e9es / anti-corr\u00e9l\u00e9es. On peut aussi imaginer r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me s'il se base sur plusieurs variables qui ne sont pas ind\u00e9pendantes. NB : Attention ! Corr\u00e9lation entre variables n'implique pas causalit\u00e9 entre variables ! On affiche souvent les coefficients de corr\u00e9lation obtenus pour toutes les combinaisons de variables possibles sous la forme d'une matrice : la matrice de corr\u00e9lation de ces variables. La diagonale de la matrice ne contient bien \u00e9videmment que des 1. Voici un exemple o\u00f9 cherche les corr\u00e9lations entre le diam\u00e8tre, le temps de cuisson, la masse de frangipane et le prix d'une galette des rois : Diam\u00e8tre Cuisson Frangipane Prix Diam\u00e8tre 1 0.8 0.7 0.9 Cuisson 0.8 1 0.5 0.2 Frangipane 0.7 0.5 1 0.6 Prix 0.9 0.2 0.6 1 La moiti\u00e9 de l'information contenue dans cette matrice \u00e9tant redondante, on n'affiche parfois que la partie triangulaire sup\u00e9rieure ou inf\u00e9rieure de cette matrice. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"corr()\" associ\u00e9e aux DataFrames. Elle retourne une matrice de corr\u00e9lation du jeu de donn\u00e9es. Analyse en Composantes Principales (ACP) Comme mentionn\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es que l'on rencontre sont souvent multidimensionels. Ceci rend difficile voir impossible un affichage graphique compr\u00e9hensible des individus d'une variable par rapport \u00e0 une autre (il faudrait un graphique 2D pour 2 variables, 3D pour 3 variables, 4D pour 4 variables, etc.). Afin de repr\u00e9senter des donn\u00e9es multidimensionnelles sous la forme d'un affichage graphique de dimension faible (en g\u00e9n\u00e9ral 1, 2 ou 3), on utilise souvent une m\u00e9thode de r\u00e9duction de dimensionnalit\u00e9 connue sous le nom d' Analyse en Composantes Principales (ACP). L'id\u00e9e est la suivante. Soit un jeu de donn\u00e9es contenant \\(p\\) variables et \\(n\\) individus. On va chercher \\(q\\) nouvelles variables par projections lin\u00e9aires des \\(p\\) variables d'origine, avec \\(q < p\\) , de mani\u00e8re \u00e0 perdre le moins d'information possible sur le jeu de donn\u00e9es. Ces \\(q\\) nouvelles variables sont alors nomm\u00e9es composantes principales . Il existe plusieurs algorithmes pour obtenir ce r\u00e9sultat, celui impl\u00e9ment\u00e9 dans la biblioth\u00e8que Python Scikit-Learn se base sur la D\u00e9composition en Valeurs Singuli\u00e8res (SVD) de la matrice de donn\u00e9es : \\(X = \\begin{pmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} &\\cdots & x_{n,p} \\end{pmatrix}\\) o\u00f9 chaque colonne correspond \u00e0 une variable, et chaque ligne correspond \u00e0 un individu. On peut voir l'ACP comme le choix du sous-espace de dimension \\(q\\) tel que le nuage de points projet\u00e9s ait la variance la plus grande possible. Les r\u00e9sultats d'une ACP peuvent \u00eatre affich\u00e9s sous la forme d'un nuage de points 2D ou 3D ( \\(q = 2\\) ou \\(3\\) ) repr\u00e9sentant les diff\u00e9rents individus, avec pour axes les composantes principales : L'id\u00e9e est de voir si on peut s\u00e9parer les individus en diff\u00e9rents groupes \u00e0 partir des composantes principales. Pour juger de la qualit\u00e9 d'une ACP, on utilise un type de graphique appel\u00e9 \" cercle des corr\u00e9lations \". Ce graphique 2D repr\u00e9sente sur chaque axe la corr\u00e9lation des \\(p\\) variables d'origine avec les \\(q\\) composantes principales. Chacune des \\(p\\) variables correspond \u00e0 un vecteur sur ce graphique, et un cercle de rayon 1 est \u00e9galement affich\u00e9 pour comparaison. Voici un exemple : Un cercle des corr\u00e9lations permet donc de juger de la corr\u00e9lation des variables d'origines avec les composantes principales, et de la corr\u00e9lation des variables d'origine entre elles : Plus une variable d'origine est proche du cercle, plus elle est fid\u00e8lement repr\u00e9sent\u00e9e par l'ACP. Dans l'id\u00e9al, on voudrait donc que toutes les variables soient proches du cercle. Pour 2 variables d'origine proches du cercle, si l'angle entre 2 les variables est aigu elles sont corr\u00e9l\u00e9es, s'il est obtu elles sont anti-corr\u00e9l\u00e9es, et s'il est droit elles sont d\u00e9corr\u00e9l\u00e9es. On pourra utiliser la projection des donn\u00e9es renvoy\u00e9e par l'ACP pour entrainer des mod\u00e8les d'apprentissage. Astuce Python La classe \"sklearn.decomposition.PCA\" de la biblioth\u00e8que \"Scikit-Learn\" vous permet de r\u00e9aliser l'ACP d'une matrice de donn\u00e9es. Le nombre de composantes principales \u00e0 trouver est un des attributs de la classe \u00e0 initialiser (\"n_components\"). Pour obtenir les composantes principales d'une matrice de donn\u00e9es, il faut lui appliquer la m\u00e9thode \"fit_transform()\" de la classe. Pour aller plus loin : D'autres m\u00e9thodes de r\u00e9duction de dimensionnalit\u00e9 existent, on peut citer entre autres les \"auto-encodeurs\" et la \"t-SNE\". Pr\u00e9paration des donn\u00e9es Une fois les donn\u00e9es analys\u00e9es, on a normalement une bonne id\u00e9e de ce qu'un outil automatique pourra en \"apprendre\" ou non. Cependant, la plupart de ces outils (dont nous parlerons dans la section suivante), ont besoin que les donn\u00e9es soient \" transform\u00e9es \" d'une certaine mani\u00e8re. C'est pourquoi nous allons voir dans cette section quelques transformations classiques pour pr\u00e9parer nos donn\u00e9es . Tout d'abord, il est possible que le jeu de donn\u00e9es contienne des valeurs erron\u00e9es ou manquantes , souvent marqu\u00e9es par des NaN (\"Not a Number\"). Il convient alors de se d\u00e9barrasser de ces valeurs avant apprentissage, car la plupart des outils ne savent pas g\u00e9rer ce probl\u00e8me. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"dropna\" associ\u00e9e aux objets DataFrames. Cette m\u00e9thode permet de supprimer les NaN d'un DataFrame. Nous allons voir que les donn\u00e9es qualitatives doivent \u00eatre encod\u00e9es avant apprentissage, soit en \"ordinal\", soit en \"one-hot\". Enfin, les outils d'apprentissage sont affect\u00e9s par les diff\u00e9rences d'ordre de grandeur entre les variables . C'est pourquoi une remise \u00e0 l'\u00e9chelle des diff\u00e9rentes variables d'un jeu de donn\u00e9es est n\u00e9cessaire avant apprentissage. On appelle ce processus recalibration , ou \" feature scaling \" en anglais. Nous allons voir en particulier 2 types de transformation pour recalibrer des donn\u00e9es : la transformation min-max et le centrage-r\u00e9duction . Transformation min-max Certains types de mod\u00e8les d'apprentissage n\u00e9cessitent des valeurs d'entr\u00e9e entre 0 et 1. C'est pourquoi la transformation min-max (\"normalization\" en anglais) va recalibrer toutes les variables de mani\u00e8re \u00e0 ce que leurs valeurs restent entre 0 et 1 . Pour ce faire, on va appliquer la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x_i-min_j}{max_j-min_j}\\) avec \\(min_j\\) le minimum et \\(max_j\\) le maximum des individus de la j-\u00e8me variable. (Il est \u00e9galement possible d'adapter cette transformation pour les mod\u00e8les prenant des valeurs entre -1 et 1 en entr\u00e9e). Le probl\u00e8me majeur avec cette transformation est sa sensibilit\u00e9 aux valeurs aberrantes. En effet, il suffit qu'une variable ait une valeur aberrante pour qu'elle devienne le minimum ou le maximum, impactant ainsi la transformation. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"MinMaxScaler\". Transformation centrage-r\u00e9duction La transformation centrage-reduction (\"standardization\" en anglais) applique la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x-\\overline{x_p}}{\\sigma_p}\\) avec \\(\\overline{x_p}\\) la moyenne et \\(\\sigma_p\\) l'\u00e9cart-type des individus de la j-\u00e8me variable. Cette transformation est beaucoup moins sensible aux valeurs aberrantes, mais elle ne garanti pas que les valeurs des diff\u00e9rentes variables seront entre 0 et 1 (ou -1 et 1). Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"StandardScaler\". Autres transformations Nous l'avons pr\u00e9c\u00e9demment, on peut d\u00e9couvrir que les individus d'une variable ont une distribution asym\u00e9trique. Par exemple, la distribution des individus peut avoir une longue tra\u00eene d'un c\u00f4t\u00e9 de la m\u00e9diane. On peut aussi avoir une distribution multimodale (c'est-\u00e0-dire avec plusieurs pics). Ceci peut perturber un apprentissage automatique. Dans ces situations, d'autres types de transformation pourrons alors \u00eatre envisag\u00e9es en addition des 2 pr\u00e9c\u00e9dentes : utiliser la racine carr\u00e9e ou le logarithme de la variable, utiliser les quantiles de la variable, utiliser un encodage de la variable, etc. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" permet de cr\u00e9er sa propre transformation, avec \"FunctionTransformer\". Encodage des donn\u00e9es La plupart des mod\u00e8les d'apprentissage automatique dont nous allons parler dans ce cours ne peuvent manipuler que des valeurs num\u00e9riques. On va donc en g\u00e9n\u00e9ral encoder des donn\u00e9es qualitatives avec des valeurs num\u00e9riques . Par exemple : Cuisson du pain Encodage Blanc 1 Pas trop cuit 2 Bien cuit 3 On appelle cet encodage, encodage par \u00e9tiquette (\"label encoding\"). Cette m\u00e9thode fonctionne toujours pour des donn\u00e9es ordinales comme la cuisson du pain, mais pour des donn\u00e9es nominales le mod\u00e8le risque de croire qu'il y a un ordre hi\u00e9rarchique dans les donn\u00e9es qui n'existe pas. C'est pourquoi on utilise souvent l'encodage one-hot . L'id\u00e9e est de faire comme si chaque nom possible pour une variable qualitative \u00e9tait une variable en soit. On appelle parfois ces variables imaginaires des \"dummy variables\". Par exemple, pour la r\u00e9gion d'origine des p\u00e2tisseries, on passe de : P\u00e2tisserie R\u00e9gion Croissant Paris Merveilleux Nord Kouign-amann Bretagne Cannel\u00e9 Sud-Ouest Kougelhopf Est \u00e0 l'encodage one-hot suivant : P\u00e2tisserie Paris Nord Bretagne Sud-Ouest Est Croissant 1 0 0 0 0 Merveilleux 0 1 0 0 0 Kouign-amann 0 0 1 0 0 Cannel\u00e9 0 0 0 1 0 Kougelhopf 0 0 0 0 1 Pour le Merveilleux, on donnera donc en entr\u00e9e d'un mod\u00e8le le binaire 01000. On remarque ici que plus la variable a de noms possibles, et plus les binaires d'encodage one-hot seront longs, ce qui peut \u00eatre probl\u00e9matique. Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"preprocessing\" des fonctions \"LabelEncoder\" et \"OrdinalEncoder\", permettant d'assigner un entier \u00e0 des variables qualitatives nominales ou ordinales. Dans ce m\u00eame package, vous trouverez \u00e9galement une fonction \"OneHotEncoder\", permettant d'encoder en one-hot des variables qualitatives nominales. Dans les 2 cas, il vous faut cr\u00e9er une instance de \"OrdinalEncoder\" ou de \"OneHotEncoder\", puis utiliser la m\u00e9thode \"fit_transform()\" avec vos donn\u00e9es en entr\u00e9e. Les apprentissages Une fois que l'on a bien cern\u00e9 notre jeu de donn\u00e9es, et qu'on l'a transform\u00e9 de mani\u00e8re ad\u00e9quate, on va en g\u00e9n\u00e9ral vouloir le mod\u00e9liser . L'id\u00e9e du mod\u00e8le sera de prendre une d\u00e9cision sur \u00e0 partir de nouvelles donn\u00e9es, en se basant sur la connaissance des donn\u00e9es de la base d'origine. On parle alors d'\"apprendre\" des donn\u00e9es. L'apprentissage automatique Par \"mod\u00e9liser\", on entend trouver une fonction param\u00e9trique \\(M\\) qui permet de d\u00e9duire une sortie vectorielle \\(y\\) voulue \u00e0 partir d'une entr\u00e9e vectorielle de nouvelles donn\u00e9es \\(x\\) et de nos connaissances sur les donn\u00e9es d'origine : \\(y = M(x,\\theta)\\) avec \\(\\theta\\) les param\u00e8tres du mod\u00e8le, qui correspondent \u00e0 notre connaissance du jeu de donn\u00e9es initial. Il nous faut donc ajuster les param\u00e8tres \\(\\theta\\) pour obtenir la sortie \\(y\\) attendue en fonction de \\(x\\) qui colle le plus aux donn\u00e9es. C'est ce processus d' optimisation de \\(\\theta\\) que l'on appelle \" apprentissage \". Les jeux de donn\u00e9es dont on doit apprendre sont en g\u00e9n\u00e9ral \u00e9normes, ce qui rend souvent un ajustement manuel des param\u00e8tres impossible. C'est pourquoi on va en g\u00e9n\u00e9ral choisir un type de mod\u00e8le , et ajuster automatiquement les param\u00e8tres \u00e0 nos donn\u00e9es. D'o\u00f9 l'expression \" apprentissage automatique \". Suivant les applications, il existe diff\u00e9rents types d'apprentissage, avec pour chacun diff\u00e9rents types de mod\u00e8les possibles. Lors de ce cours, nous verrons 3 grands types d'apprentissage, et nous verrons pour chacun quelques exemples de mod\u00e8les classiques. Les 3 grands types d'apprentissages En apprentissage, on appelle souvent en anglais les entr\u00e9es d'un mod\u00e8le les \" features \", et les sorties des \" labels \". Lors du processus d' apprentissage (ajustement des param\u00e8tres), on va enseigner au mod\u00e8le comment d\u00e9terminer des \"labels\" correspondant \u00e0 des \"features\", en se basant sur ce qu'il a appris d'une base de donn\u00e9es d'\"entrainement\" de \"features\". Par exemple, on peut vouloir entrainer un mod\u00e8le \u00e0 associer \u00e0 une photo de viennoiserie (feature) le nom de la viennoiserie (label) : Ou alors, on peut vouloir entrainer un mod\u00e8le \u00e0 associer \u00e0 la masse de farine, la masse de beurre, le volume de lait et le diam\u00e8tre d'un lot de cr\u00eapes (features) le prix de la cr\u00eape (label) : Apprentissage supervis\u00e9 ou non-supervis\u00e9 Dans le cas o\u00f9 les donn\u00e9es d'apprentissage ont des \"labels\" d\u00e9finis, le mod\u00e8le va apprendre \u00e0 retrouver ces \"labels\" (connus) pour ces \"features\". On esp\u00e8re alors qu'apr\u00e8s apprentissage, le mod\u00e8le pourra retourner les \"labels\" corrects une fois confront\u00e9 \u00e0 des \"features\" issus de nouvelles donn\u00e9es. On parle alors de \" g\u00e9n\u00e9ralisation \". Comme on peut directement v\u00e9rifier les performances du mod\u00e8le \u00e0 pr\u00e9dire les \"labels\" du jeu de donn\u00e9es d'entrainement, on parle d' apprentissage supervis\u00e9 . Le processus est en g\u00e9n\u00e9ral it\u00e9ratif : le mod\u00e8le va se mettre \u00e0 jour au fur et \u00e0 mesure des it\u00e9rations pour r\u00e9duire l'erreur de pr\u00e9diction. On a donc besoin d'une fonction d'\u00e9valuation de l'erreur, aussi appel\u00e9e \" fonction de co\u00fbt \". Voici le concept r\u00e9sum\u00e9 graphiquement, avec notre exemple de l'association d'un nom de viennoiserie \u00e0 une photo : Dans le cas o\u00f9 les donn\u00e9es d'apprentissage n'ont pas de \"labels\", on peut tout de m\u00eame essayer de diviser les individus des \"features\" en diff\u00e9rent groupes, auxquels on assignera des \"labels\" plus tard. On appelle la premi\u00e8re \u00e9tape \" partition \", et la seconde \" lab\u00e9lisation \". On a besoin d'un crit\u00e8re de proximit\u00e9 entre individus afin de d\u00e9finir ces groupes, souvent une \" distance \". Il s'agit aussi souvent d'un processus it\u00e9ratif. Comme nous n'avons pas de \"labels\" d'entrainement comme r\u00e9f\u00e9rence, on parle d' apprentissage non-supervis\u00e9 ou \"clustering\" (\"partition de donn\u00e9es\"). Voici le concept r\u00e9sum\u00e9 graphiquement, toujours avec notre exemple de l'association d'un nom de viennoiserie \u00e0 une photo : Classification et r\u00e9gression On peut aussi diviser les apprentissages suivant la nature des sorties attendue, et donc de mod\u00e8le \u00e0 entrainer. Si la sortie est quantitative discr\u00e8te ou qualitative , on va parler de \" classification \". Si la sortie est quantitative continue , on va parler de \" r\u00e9gression \". Si l'on reprend nos 2 exemples pr\u00e9c\u00e9dents : Le nom d'une viennoiserie \u00e9tant une \u00e9tiquette, soit une variable qualitative, il s'agit d'un probl\u00e8me de classification. Le prix d'une cr\u00eape \u00e9tant une variable quatitative continue, il s'agit d'un probl\u00e8me de r\u00e9gression. On peut entrainer un mod\u00e8le de classification de mani\u00e8re supervis\u00e9e ou non-supervis\u00e9e . On ne peut entrainer un mod\u00e8le de r\u00e9gression que de mani\u00e8re supervis\u00e9e . NB : Seule la classification peut \u00eatre \"non-supervis\u00e9e\", et on parle en g\u00e9n\u00e9ral directement de partitionnement (\"clustering\"). Pour aller plus loin... Il existe un 3\u00e8me type d'apprentissage, que nous ne d\u00e9taillerons pas dans ce cours, qui s'appelle \"apprentissage par renforcement \". L'id\u00e9e est la suivante : Le mod\u00e8le est directement mis en place sur son cas d'application final. Le mod\u00e8le prend des d\u00e9cisions en fonction des situations, et re\u00e7oit un retour (\"feedback\") sur sa d\u00e9cision, positif ou n\u00e9gatif. Le mod\u00e8le se met \u00e0 jour en fonction du retour qu'il a re\u00e7u. Ce processus se r\u00e9p\u00e8te pour chaque nouvelle situation, et ainsi le mod\u00e8le apprend de ses exp\u00e9riences . Voici un petit sch\u00e9ma r\u00e9capitulatif des diff\u00e9rents types d'apprentissages que nous avons vus : Difficult\u00e9s de l'apprentissage Comme expliqu\u00e9 plus haut, l' apprentissage est un processus d' optimisation , qui consiste en l' ajustement des param\u00e8tres d'un mod\u00e8le en se basant sur les donn\u00e9es disponibles, dans le but de prendre des d\u00e9cisions correctes \u00e0 partir de donn\u00e9es futures ( g\u00e9n\u00e9ralisation ). La phase durant laquelle on ajuste les param\u00e8tres est appel\u00e9e entra\u00eenement , et les donn\u00e9es sur lesquelles cet ajustement est fait sont appel\u00e9es \" base de donn\u00e9es d'entra\u00eenement \". Dans la section qui suit, nous aurons un aper\u00e7u des grandes difficult\u00e9es que l'on peut rencontrer lors de l'entrainement d'un mod\u00e8le, tous types de mod\u00e8les confondus. Quantit\u00e9 et qualit\u00e9 des donn\u00e9es S'il n'y a pas de r\u00e8gle pr\u00e9cise pour d\u00e9terminer la quantit\u00e9 de donn\u00e9es n\u00e9cessaire \u00e0 un apprentissage, il y a 2 maximes \u00e0 retenir : Plus on a de donn\u00e9es d'entrainement, meilleur sera l'apprentissage par le mod\u00e8le. Plus le probl\u00e8me complexe, plus il faudra de donn\u00e9es d'entrainement. Par exemple, dans notre exemple de classification des images de viennoiseries : Pour donner un ordre de grandeur, la quantit\u00e9 d'individus n\u00e9cessaires \u00e0 un apprentissage va en g\u00e9n\u00e9ral de quelques milliers \u00e0 des centaines de millions . Cependant, il n'est pas ais\u00e9 de constituer une base de donn\u00e9es aussi large, et de surcroit une base de donn\u00e9e de qualit\u00e9. En effet, comme on peut facilement le deviner, la qualit\u00e9 des donn\u00e9es aura un impact sur l'apprentissage. La qualit\u00e9 des donn\u00e9es peut par exemple \u00eatre d\u00e9grad\u00e9e par : La pr\u00e9sence d' individus ab\u00e9rrants (\"outliers\"), li\u00e9e \u00e0 des erreurs de mesures ou \u00e0 des cas exceptionnels. Des individus manquants , li\u00e9s \u00e0 notre \u00e9chantillonnage ou a des erreurs de mesures. La pr\u00e9sence de bruit dans les donn\u00e9es. Toujours pour notre exemple, on peut avoir des donn\u00e9es manquantes ou ab\u00e9rrantes : D'o\u00f9 la n\u00e9cessit\u00e9 de proc\u00e9der \u00e0 un nettoyage des donn\u00e9es en amont de l'apprentissage : supprimer certaines donn\u00e9es, les combler, ou faire de nouvelles mesures. Malheureusement, nettoyer les donn\u00e9es implique parfois de diminuer la quantit\u00e9 de donn\u00e9es : on est donc souvent confront\u00e9 \u00e0 un compromis entre quantit\u00e9 et qualit\u00e9 des donn\u00e9es. Repr\u00e9sentativit\u00e9 et \u00e9quilibre des donn\u00e9es Comme expliqu\u00e9 pr\u00e9cedemment, notre but est d'obtenir \u00e0 partir de notre base de donn\u00e9es d'entrainement un mod\u00e8le qui soit g\u00e9n\u00e9ralisable \u00e0 toutes nouvelles donn\u00e9es que l'on peut rencontrer. Pour atteindre cet objectif, il faut que la base de donn\u00e9es que l'on utilise pour entrainer le mod\u00e8le soit repr\u00e9sentative de la distribution des diff\u00e9rentes variables de mani\u00e8re g\u00e9n\u00e9rale. Ceci implique de faire attention \u00e0 la repr\u00e9sentativit\u00e9 de notre population au moment de l' \u00e9chantillonnage , sous peine que le mod\u00e8le ait du mal \u00e0 g\u00e9n\u00e9raliser. Parfois, certains types d'individus sont par nature sous ou sur-repr\u00e9sent\u00e9s dans la population g\u00e9n\u00e9rale, et donc le seront toujours si on \u00e9chantillonne de mani\u00e8re repr\u00e9sentative. Comme on peut s'y attendre, ceci va avoir tendance \u00e0 biaser notre mod\u00e8le. Par exemple, mettons que l'on veuille entrainer un mod\u00e8le \u00e0 reconnaitre une photo d'un pain au chocolat d'un pain suisse. Les pains au chocolat \u00e9tant plus courants en boulangerie que les pains suisses, on aura un d\u00e9s\u00e9quilibre dans la base de donn\u00e9es d'entrainement, qui fera que notre mod\u00e8le aura plus tendance \u00e0 pr\u00e9dire qu'une photo montre un pain au chocolat qu'un pain suisse. Il existe plusieurs m\u00e9thodes pour \u00e9viter les biais d'entrainement, en jouant soit sur l'\u00e9chantillonage, soit sur les poids accord\u00e9s aux diff\u00e9rentes donn\u00e9es pendant l'entrainement. Il est \u00e0 noter qu'un mauvais \u00e9chantillonage ou un d\u00e9s\u00e9quilibre d'une base de donn\u00e9es utilis\u00e9e pour la tester notre mod\u00e8le apr\u00e8s entrainement est aussi probl\u00e9matique : si on teste notre mod\u00e8le sur une base de donn\u00e9es ne contenant que des photos de pains au chocolat, il est \u00e9vident que notre mesure des performances du mod\u00e8le ne vaudra pas grand chose. Pertinence des variables Pour que notre mod\u00e8le soit capable de renvoyer les labels (sorties) d\u00e9sir\u00e9s, il faut que les features (entr\u00e9es) le permettent. Le s\u00e9lection de variables pertinentes pour un probl\u00e8me donn\u00e9 est donc capital. C'est ce que l'on appelle en anglais le \" feature engineering \". On peut diviser cette activit\u00e9 en 2 grandes techniques : la s\u00e9lection de variables et l' extraction de variables . La s\u00e9lection de variables Ce n'est pas parce qu'une variable est dans la base de donn\u00e9es qu'il faut l'utiliser pour entrainer un mod\u00e8le. Il faut faire un tri pour choisir les variables pertinentes pour un probl\u00e8me parmi toutes les variables disponibles. Par exemple, si pour estimer le prix d'une cr\u00eape on a acc\u00e8s \u00e0 la masse de farine, le volume de lait et l'\u00e2ge du cr\u00eapier, on va intuitivement choisir comme features la masse de farine et le volume de lait, car on devine que l'\u00e2ge du cr\u00eapier n'aura pas d'impact sur le prix. L'id\u00e9e est donc de s\u00e9lectionner comme features des variables qui soient corr\u00e9l\u00e9es aux labels . Pour ce faire, on utilise g\u00e9n\u00e9ralement des matrices de corr\u00e9lations vues pr\u00e9c\u00e9demment. L'extraction de variables Multiplier le nombre de variables revient \u00e0 augmenter la dimensionnalit\u00e9 d'un probl\u00e8me, ce qui rend l'apprentissage plus compliqu\u00e9. De plus, certaines variables peuvent \u00eatre corr\u00e9l\u00e9es entre elles. C'est pourquoi on peut vouloir cr\u00e9er de nouvelles variables pour un probl\u00e8me, en combinant des variables parmi celles disponibles. Par exemple, plut\u00f4t que d'utiliser la masse de farine et le volume de lait pour pr\u00e9dire le prix d'une cr\u00eape, on peut imaginer utiliser un ratio volume de lait / masse de farine. L'id\u00e9e est donc de trouver des combinaisons de variables pertinentes pour pr\u00e9dire les labels. Pour ce faire, on peut se servir d'une m\u00e9thode de r\u00e9duction de dimensionnalit\u00e9 telle que l' ACP vue pr\u00e9c\u00e9demment. Sur-apprentissage / sous-apprentissage Un des pires cauchemars de tout \"data scientist\" est le sur-apprentissage . Le but de tout apprentissage est d' obtenir un mod\u00e8le capable de g\u00e9n\u00e9raliser \u00e0 de nouvelles donn\u00e9es ce qu'il appris sur la base de donn\u00e9es d'entrainement. Le probl\u00e8me est que lors de l'apprentissage, on va chercher \u00e0 obtenir les meilleures performances possibles sur les donn\u00e9es d'entrainement, ce qui peut pousser le mod\u00e8le \u00e0 apprendre des d\u00e9tails tr\u00e8s sp\u00e9cifiques \u00e0 ces donn\u00e9es : le bruit et les \"outliers\" par exemple. D'o\u00f9 le terme de \"sur-apprentissage\". Un tel mod\u00e8le, trop compliqu\u00e9, aura alors de mauvaises performances sur de nouvelles donn\u00e9es : il sera incapable de g\u00e9n\u00e9raliser . Nous verrons dans la suite de ce chapitre la strat\u00e9gie classique pour d\u00e9tecter et \u00e9viter le sur-apprentissage. On peut n\u00e9anmoins r\u00e9duire les risques en : Ayant des donn\u00e9es nombreuses et repr\u00e9sentatives. Choisissant d'abord un type de mod\u00e8le simple, puis un plus complexe si besoin. Nettoyant les donn\u00e9es pour \u00e9viter le bruit et les \"outliers\". L'inverse du sur-apprentissage existe aussi, et se nomme \" sous-apprentissage \". Le mod\u00e8le est alors trop simple pour capturer la complexit\u00e9 des donn\u00e9es, et il tout aussi incapable de g\u00e9n\u00e9raliser. Le bon compromis que l'on va rechercher se situe donc entre les 2 : On appelle la recherche de ce compromis la \" r\u00e9gularisation \". Strat\u00e9gie pour l'apprentissage Maintenant que nous avons pass\u00e9 en revue les principales difficult\u00e9es auxquelles on doit faire face lors de l'apprentissage d'un mod\u00e8le, nous allons nous int\u00e9resser aux strat\u00e9gies d'apprentissage classiques. Hyperparam\u00e8tres Dans le contexte de l'apprentissage d'un mod\u00e8le, on distingue 2 types de param\u00e8tres : Les param\u00e8tres du mod\u00e8le que nous allons vouloir ajuster au cours de l'apprentissage, afin d'obtenir les labels d\u00e9sir\u00e9s \u00e0 partir de features donn\u00e9es. Les param\u00e8tres propres \u00e0 l' apprentissage , que l'on nomme \" hyperparam\u00e8tres \". Parmis les hyperparam\u00e8tres, on peut citer : la fonction de co\u00fbt utilis\u00e9e pour l'optimisation du mod\u00e8le, l'algorithme d'optimisation utilis\u00e9, la vitesse de convergence choisie, le nombre d'it\u00e9rations de l'entrainement, ou encore l'architecture du mod\u00e8le. Il est \u00e9vident que les hyperparam\u00e8tres vont impacter les performances du mod\u00e8le obtenu apr\u00e8s entrainement. C'est pourquoi lors d'un apprentissage, on va en g\u00e9n\u00e9ral r\u00e9aliser plusieurs entrainements , avec des hyperparam\u00e8tres diff\u00e9rents, afin d' optimiser aussi les hyperparam\u00e8tres . Fonction de co\u00fbt La fonction de co\u00fbt (\"loss function\" en anglais), est une fonction qui prend en entr\u00e9e les labels pr\u00e9dits par le mod\u00e8le et les labels attendus , et retourne un score suivant la proximit\u00e9 de ces labels. On l'utilise comme crit\u00e8re de la qualit\u00e9 des labels pr\u00e9dits par un mod\u00e8le lors de son entrainement . En g\u00e9n\u00e9ral, plus la valeur retourn\u00e9e par la fonction de co\u00fbt est faible, et meilleure est la pr\u00e9diction. Le processus d'optimisation du mod\u00e8le va donc chercher \u00e0 minimiser la fonction de co\u00fbt . On choisit une fonction de co\u00fbt selon diff\u00e9rents crit\u00e8res : Qu'elle soit adapt\u00e9e au type de probl\u00e8me de classification / r\u00e9gression auquel on est confront\u00e9. Qu'elle soit adapt\u00e9e \u00e0 l'algorithme d'optimisation choisi : certains algorithmes n\u00e9cessitent par exemple une fonction continue ou diff\u00e9rentiable. Qu'elle soit adapt\u00e9e aux hypoth\u00e8ses que nous avons sur les donn\u00e9es : certaines fonctions donneront par exemple plus ou moins de poids aux \"outliers\". Dans les chapitres suivants, nous verrons quelques exemples de fonctions de co\u00fbt, adapt\u00e9es \u00e0 diff\u00e9rents types de probl\u00e8mes. Attention ! La fonction de co\u00fbt est utilis\u00e9e pour \u00e9valuer la qualit\u00e9 de la pr\u00e9diction d'un mod\u00e8le sur des donn\u00e9es lab\u00e9lis\u00e9e. Il ne s'agit pas d'une \u00e9valuation de la performance d'un mod\u00e8le en g\u00e9n\u00e9ralisation ! Cette notion, bien diff\u00e9rente, fera l'objet d'une section dans la suite de ce chapitre. Algorithme d'optimisation On appelle \" algorithme d'optimisation \" un algorithme dont l'objectif est de trouver le minimum global de la fonction de co\u00fbt, afin d' optimiser les param\u00e8tres d'un mod\u00e8le. C'est lui qui r\u00e9alise l'entrainement proprement dit. Il s'agit en g\u00e9n\u00e9ral d'un algorithme it\u00e9ratif , qui va faire varier les param\u00e8tres du mod\u00e8le , afin de trouver ceux qui minimisent la fonction de co\u00fbt. On appelle souvent les it\u00e9rations de l'algorithme \u00e9poques (\"epochs\" en anglais). Voici une illustration pour un cas tr\u00e8s simplifi\u00e9 de mod\u00e8le \u00e0 1 param\u00e8tre : L'algorithme d'optimisation de base en apprentissage supervis\u00e9 est celui de la descente de gradient . Son principe est le suivant : Algorithme de descente de gradient Soit \\(f(x)\\) une fonction diff\u00e9rentiable dont on cherche le minimum. On initialise une valeur de \\(x\\) not\u00e9e \\(x_0\\) . A chaque it\u00e9ration \\(n\\) de l'algorithme, on va se d\u00e9placer dans l'espace des valeurs de \\(x\\) , de \\(x_n\\) \u00e0 \\(x_{n+1}\\) , avec la formule : \\(x_{n+1} = x_n - \\gamma \\nabla f(x_n)\\) avec \\(\\nabla f(x_n)\\) le gradient de la fonction en \\(x_n\\) , et \\(\\gamma\\) le \"taux d'apprentissage\". On peut mettre un crit\u00e8re d'arr\u00eat sur \\(\\| \\nabla f(x_n) \\|\\) . L'id\u00e9e est de parcourir l'espace des valeurs de la fonction de co\u00fbt en fonction des param\u00e8tres du mod\u00e8le, en suivant la direction inverse au gradient. On explique souvent le principe de cette m\u00e9thode par l'analogie des \"randonneurs dans le brouillard\" : Des randonneurs de montagne se retrouvent pi\u00e9g\u00e9s par le brouillard alors qu'il cherchent \u00e0 rentrer dans la vall\u00e9e. Comme ils ne peuvent plus voir le chemin, ils d\u00e9cident de se fier \u00e0 leurs pieds : ils se dirigent dans la direction de la pente la plus forte. Sans le savoir, les randonneurs sont en train d'appliquer l'algorithme de descente de gradient pour trouver le point minimisant l'altitude : la vall\u00e9e. Encore aujourd'hui, l'entrainement des mod\u00e8les d'apprentissage supervis\u00e9 se fait avec des algorithmes d'optimisation bas\u00e9s sur la descente de gradient. Vitesse de convergence et nombre d'it\u00e9rations Nous avons vu dans la section pr\u00e9c\u00e9dente que l'algorithme de la descente de gradient avait un param\u00e8tre appel\u00e9 taux d'apprentissage . Il est inversement proportionnel au pas avec lequel l'algorithme d'optimisation va se d\u00e9placer dans l'espace des valeurs de la fonction de co\u00fbt en fonction des param\u00e8tres du mod\u00e8le. Par cons\u00e9quent, le taux d'apprentissage permet de choisir la vitesse de convergence de l'algorithme d'optimisation . On s'attend \u00e0 ce que plus le taux d'apprentissage soit \u00e9lev\u00e9, plus la convergence vers le mod\u00e8le optimal soit rapide. Or, ce n'est pas toujours le cas. En effet, si l'algorithme se d\u00e9place trop rapidement dans l'espace des valeurs de la fonction de co\u00fbt, on a un risque de d\u00e9passement (\"overshooting\" en anglais) : l'algorithme peut passer \"par-dessus\" le minimum sans s'y arr\u00eater, voir m\u00eame diverger compl\u00e8tement. Le choix d'un taux d'apprentissage adapt\u00e9 est donc toujours affaire de compromis entre vitesse de convergence et non-d\u00e9passement. Nous avons \u00e9galement vu pr\u00e9c\u00e9demment que l'on peut placer dans l'algorithme un crit\u00e8re sur le gradient pour arr\u00eater les it\u00e9rations. Cependant, il est possible que ce crit\u00e8re ne soit jamais v\u00e9rifi\u00e9 si l'algorithme diverge. On va donc en g\u00e9n\u00e9ral choisir par s\u00e9curit\u00e9 un nombre d'\u00e9poques maximal comme crit\u00e8re d'arr\u00eat additionnel. Un autre probl\u00e8me courant est le fait que l'espace des valeurs de la fonction de co\u00fbt a souvent des minima locaux , dans lesquels l'algorithme peut se retrouver bloqu\u00e9 . C'est pourquoi en g\u00e9n\u00e9ral, on ne va pas lancer un seul entrainement pour un jeu d'hyperparam\u00e8tres, mais plusieurs avec des initialisations diff\u00e9rentes . Architecture du mod\u00e8le Certains types de mod\u00e8les peuvent avoir des architectures diff\u00e9rentes, plus ou moins complexes suivant le probl\u00e8me \u00e0 r\u00e9soudre. L'exemple le plus \u00e9vident sont les c\u00e9l\u00e8bres r\u00e9seaux de neurones , dont nous reparlerons dans les chapitres suivants. Dans le cas de ce type de mod\u00e8le, on peut jouer sur les hyperparm\u00e8tres suivants: Le nombre de couches de neurones. Le nombre de neurones par couche. La fonction d'activation de chaque neurone. Nous d\u00e9velopperons plus tard dans ce cours \u00e0 quoi correspondent ces diff\u00e9rents param\u00e8tres. Entrainement, validation et test Ce n'est pas parce que notre mod\u00e8le a converg\u00e9 vers le minimum global de l'espace de la fonction de co\u00fbt en fonction de ses param\u00e8tres qu'il aura de bonnes performances en g\u00e9n\u00e9ralisation ! Performances en g\u00e9n\u00e9ralisation En effet, nous avons vu plus t\u00f4t que le mod\u00e8le peut \u00eatre victime de sur-apprentissage : il a appris trop sp\u00e9cifiquement des donn\u00e9es d'entrainement, et n'arrive donc pas \u00e0 g\u00e9n\u00e9raliser \u00e0 de nouvelles donn\u00e9es. Intuitivement, on devine qu'il faudrait mettre de c\u00f4t\u00e9 une partie de notre base de donn\u00e9es d'entrainement, sur laquelle nous n'entrainerons pas notre mod\u00e8le. Ce sous-ensemble de donn\u00e9es servira \u00e0 tester le mod\u00e8le une fois l'entrainement termin\u00e9, pour v\u00e9rifier qu'il n'y a pas eu sur-apprentissage. C'est en effet la strat\u00e9gie classique pour l'apprentissage d'un mod\u00e8le. On dit alors que l'on a s\u00e9par\u00e9 la base de donn\u00e9e en un jeu d'entrainement et un jeu de test . On consid\u00e8rera alors que les performances obtenues par le mod\u00e8le sur le jeu de test seront ses performances en g\u00e9n\u00e9ralisation : il aura des performances \u00e9quivalentes sur tout nouveau jeu de donn\u00e9es. Le probl\u00e8me est alors le suivant : Plus on r\u00e9serve de donn\u00e9es pour les tests, et plus difficile sera l'apprentissage. Plus on r\u00e9serve de donn\u00e9es pour l'entrainement, et moins repr\u00e9sentatif sera le test. Souvent, le compromis retenu est 80% des donn\u00e9es pour l'entrainement et 20% des donn\u00e9es pour le test . Pour \u00e9valuer les performances en g\u00e9n\u00e9ralisation de notre mod\u00e8le, on va utiliser une s\u00e9lection de crit\u00e8res de performance , appel\u00e9s \"metrics\" en anglais. Pour chaque type de mod\u00e8le, il existe diff\u00e9rents crit\u00e8res de performances, qui vont chacun essayer de capturer un aspect diff\u00e9rent de la qualit\u00e9 de ses pr\u00e9dictions. Nous verrons des crit\u00e8res de performance classiques dans chacun des chapitres de ce cours. Validation par exclusion Une fois que nous avons s\u00e9par\u00e9 notre base de donn\u00e9es en un jeu d'entrainement et un jeu de test, l'approche na\u00efve pour \u00e9viter le sur-apprentissage serait d'entrainer plusieurs mod\u00e8les avec diff\u00e9rents hyperparam\u00e8tres, et de retenir celui qui donne les meilleures performances en test. Cependant, si vous faites ceci, il y a peu de chances que votre mod\u00e8le ait de bonnes performances en g\u00e9n\u00e9ralisation . En effet, ce processus revient \u00e0 optimiser les hyperparam\u00e8tres sp\u00e9cifiquement pour notre jeu de donn\u00e9es de test . C'est pourquoi on va en r\u00e9alit\u00e9 diviser extraire un 3\u00e8me sous-ensemble de notre base de jeu d'entrainement : un jeu de validation . L' optimisation des hyperparam\u00e8tres se fera sur ce basant sur les performance en validation , et l'\u00e9valuation des performances en g\u00e9n\u00e9ralisation ce fera sur le jeu de test . Attention ! Les performances en test ne doivent jamais \u00eatre optimis\u00e9es, au risque de ne plus \u00eatre repr\u00e9sentatives des performances en g\u00e9n\u00e9ralisation du mod\u00e8le ! Le processus que nous avons d\u00e9crit ici est connu sous le nom de validation par exclusion . Le voici d\u00e9crit en d\u00e9tails : Validation par exclusion - On s\u00e9pare la base de donn\u00e9es en un jeu d'entrainement, un jeu de validation et un jeu de test. - On entraine plusieurs mod\u00e8les sur le jeu d'entrainement, avec diff\u00e9rents hyperparam\u00e8tres. - On s\u00e9lectionne le mod\u00e8le ayant les meilleurs r\u00e9sultats sur le jeu de validation. - On r\u00e9-entraine ce mod\u00e8le avec les m\u00eames hyperparam\u00e8tres, sur les donn\u00e9es d'entrainement et de validation. - On \u00e9value les performances en g\u00e9n\u00e9ralisation de ce mod\u00e8le sur les donn\u00e9es de test. Souvent, on retiendra 60% des donn\u00e9es pour l'entrainement , 20% des donn\u00e9es pour la validation et 20% des donn\u00e9es pour le test . Voici la validation par exclusion r\u00e9sum\u00e9e sch\u00e9matiquement : Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"model_selection\" une fonction \"train_test_split\" pour s\u00e9parer une base de donn\u00e9es en jeux d'entrainement et de test. Validation crois\u00e9e La validation par exclusion a 2 grands d\u00e9fauts : En d\u00e9coupant en 3 jeux les donn\u00e9es disponibles, nous r\u00e9duisons la taille de la base de donn\u00e9es d'entrainement, et donc nous rendons l'apprentissage plus difficile. Les r\u00e9sultats peuvent d\u00e9pendre de la mani\u00e8re dont nous avons d\u00e9coup\u00e9 notre base de donn\u00e9es en 3. C'est pourquoi on lui pr\u00e9f\u00e8re souvent la validation crois\u00e9e . L'id\u00e9e est la suivante : on met toujours de c\u00f4t\u00e9 un jeu de test, mais on ne r\u00e9alise plus une s\u00e9paration entre donn\u00e9es d'entrainement et donn\u00e9es de validation. A la place, on s\u00e9pare la base d'entrainement en plusieurs petits jeux de donn\u00e9es, et on r\u00e9alise plusieurs entrainements du mod\u00e8le, avec pour chacun jeu un de validation diff\u00e9rent parmi ces petits jeux. Validation crois\u00e9e - On s\u00e9pare la base de donn\u00e9es en un jeu d'entrainement et un jeu de test. - On s\u00e9pare le jeu d'entrainement en k sous-jeux. - On r\u00e9alise k combinaisons, o\u00f9 un des k sous-jeux est utilis\u00e9 pour la validation, pendant que les k-1 autres sont utilis\u00e9es pour l'entrainement. - La qualit\u00e9 des pr\u00e9dictions du mod\u00e8le est \u00e9valu\u00e9e \u00e0 chaque it\u00e9ration comme \u00e9tant la moyenne de celles \u00e9valu\u00e9e pour chaque combinaison. - On s\u00e9lectionne le mod\u00e8le ayant en moyenne les meilleurs r\u00e9sultats en validation. - On r\u00e9-entraine ce mod\u00e8le avec les m\u00eames hyperparam\u00e8tres, sur le jeu d'entrainement complet. - On \u00e9value les performances en g\u00e9n\u00e9ralisation de ce mod\u00e8le sur les donn\u00e9es de test. Voici la validation crois\u00e9e r\u00e9sum\u00e9e sch\u00e9matiquement : Cette strat\u00e9gie d'apprentissage a tout de m\u00eame un d\u00e9savantage compar\u00e9e \u00e0 la validation par exclusion : elle n\u00e9cessite un temps de calcul beaucoup plus long ! Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"model_selection\" une fonction \"cross_validate\". R\u00e9gularisation par arr\u00eat pr\u00e9matur\u00e9 Nous avons vu que la strat\u00e9gie recommand\u00e9e pour \u00e9viter les probl\u00e8mes de g\u00e9n\u00e9ralisation est de comparer les performances du mod\u00e8le obtenues sur les donn\u00e9es d'entrainement \u00e0 celles obtenues sur les donn\u00e9es de validation. Pour \u00e9viter le sur-apprentissage , on peut en plus appliquer lors de l'entrainement la m\u00e9thode de r\u00e9gularisation connue sous le nom d' arr\u00eat pr\u00e9matur\u00e9 (\"early-stopping\" en anglais). Son principe se base sur les id\u00e9es suivantes : Puisque nous optimisons les param\u00e8tres du mod\u00e8le pour minimiser la fonction de co\u00fbt, nous nous attendons \u00e0 ce qu'au cours de l'entrainement, les valeurs de la fonction de co\u00fbt d\u00e9croissent avec les \u00e9poques pour les donn\u00e9es d'entrainement . A mesure que le mod\u00e8le est optimis\u00e9 sur les donn\u00e9es d'entrainement, nous nous attendons d'abord \u00e0 ce que dans un 1er temps , la fonction de co\u00fbt d\u00e9croisse aussi avec avec les \u00e9poques pour les donn\u00e9es de validation . Mais lorsque le mod\u00e8le se met \u00e0 souffrir de sur-apprentissage , nous nous attendons \u00e0 ce que la fonction de co\u00fbt se mette \u00e0 cro\u00eetre avec les \u00e9poques pour les donn\u00e9es de validation . On peut donc d\u00e9tecter le d\u00e9but du sur-apprentissage comme \u00e9tant l'\u00e9poque o\u00f9 la fonction de co\u00fbt se met \u00e0 croitre pour les donn\u00e9es de validation, et arr\u00eater l'apprentissage \u00e0 cette it\u00e9ration. D'o\u00f9 le nom d'\" arr\u00eat pr\u00e9matur\u00e9 \". Le processus exact est le suivant : Arr\u00eat pr\u00e9matur\u00e9 Pour chaque it\u00e9ration du processus d'apprentissage : - On \u00e9value la fonction de co\u00fbt pour les pr\u00e9dictions de la m\u00e9thode sur le jeu d'entrainement. - On \u00e9value la fonction de co\u00fbt pour les pr\u00e9dictions de la m\u00e9thode sur le jeu d'\u00e9valuation. - On v\u00e9rifie qu'on a bien une d\u00e9croissance pour les 2 valeurs, sinon on stoppe l'entrainement. Import de donn\u00e9es et fichiers CSV Pour enregistrer une base de donn\u00e9es sous la forme d'un tableau, on utilise couramment le format CSV : \"Comma Separated Values\". Il s'agit d'un format ouvert, compr\u00e9hensible par un humain, qui peut \u00eatre lu par n'importe quel \u00e9diteur de texte. Comme son nom l'indique, un fichier CSV s'organise de la mani\u00e8re suivante : Chaque ligne correspond \u00e0 une ligne du tableau . Les lignes sont s\u00e9par\u00e9es par un retour \u00e0 la ligne \"\\n\". Au sein d'une ligne, les \u00e9l\u00e9ments de chaque colonnes sont s\u00e9par\u00e9s par des virgules \",\". La premi\u00e8re ligne est souvent consid\u00e9r\u00e9e comme l' en-t\u00eate du tableau (le nom des colonnes). D'o\u00f9 le nom \"Comma Separated Values\". Voici un exemple de tableau de donn\u00e9es : Ville Latitude Longitude Nom Paris 48.866667 2.333333 Pain au chocolat Orl\u00e9ans 47.902734 1.908607 Pain au chocolat Bordeaux 44.841225 -0.580036 Chocolatine Brest 48.390528 -4.486009 Pain au chocolat Toulouse 43.604464 1.444243 Chocolatine Et voici tout simplement sa conversion en format CSV : Ville,Latitude,Longitude,Nom Paris,48.866667,2.333333,Pain au chocolat Orl\u00e9ans,47.902734,1.908607,Pain au chocolat Bordeaux,44.841225,-0.580036,Chocolatine Brest,48.390528,-4.486009,Pain au chocolat Toulouse,43.604464,1.444243,Chocolatine L'extension d'un fichier CSV est \" .csv \". Il peut \u00eatre import\u00e9 par la plupart des logiciels tableurs (Excel, OpenOffice Calc), et sous Python avec la biblioth\u00e8que Pandas, dont nous reparlerons plus loin. Astuce Python Avec la biblioth\u00e8que Python \"Pandas\", on peut importer un fichier CSV sous la forme d'un \"DataFrame\" (tableau de donn\u00e9es propre \u00e0 \"Pandas\") avec la m\u00e9thode \"read_csv\". Bien qu'il soit tr\u00e8s utilis\u00e9, le format CSV a quelques d\u00e9savantages. Notamment : Il n'est pas optimal en terme de m\u00e9moire. Il emp\u00eache d'utiliser le caract\u00e8re \",\" pour autre chose que s\u00e9parer les colonnes du tableau. C'est pourquoi, suivant les applications, on pr\u00e9f\u00e8rera utiliser un fichier binaire. Outils Python pour l'apprentissage Pour chacun des programmes Python pr\u00e9sent\u00e9s dans ce cours, on partira du principe que les biblioth\u00e8ques Numpy et Matplotlib sont import\u00e9es . Pandas Pandas est une biblioth\u00e8que Python tr\u00e8s populaire pour l' analyse de donn\u00e9es . Elle permet : D' importer des donn\u00e9es sous la forme d'un conteneur nomm\u00e9 \" DataFrame \". De manipuler ces donn\u00e9es de mani\u00e8re efficace. D'afficher facilement des graphiques \u00e0 partir de ces donn\u00e9es. D' exporter ces donn\u00e9es en diff\u00e9rents formats. De fournir une entr\u00e9e pour l' apprentissage de mod\u00e8les. Elle s'importe souvent sous le nom \"pd\" avec la commande : import pandas as pd Un DataFrame Pandas contient un jeu de donn\u00e9es sous la forme d'un tableau 2D , pouvant contenir des objets de types diff\u00e9rents. On associe \u00e0 chaque donn\u00e9e dans un DataFrame une \" column \" correspondant au nom de la variable, et un \" index \" correspondant au num\u00e9ro de l'individu. Voici notre exemple de tableau pr\u00e9c\u00e9dent, converti en DataFrame : 'Ville' 'Latitude' 'Longitude' 'Nom' 0 'Paris' 48.866667 2.333333 'Pain au chocolat' 1 'Orl\u00e9ans' 47.902734 1.908607 'Pain au chocolat' 2 'Bordeaux' 44.841225 -0.580036 'Chocolatine' 3 'Brest' 48.390528 -4.486009 'Pain au chocolat' 4 'Toulouse' 43.604464 1.444243 'Chocolatine' On peut acc\u00e9der \u00e0 une ligne d'indice i d'un DataFrame df avec la commande : df.loc[i] Et on peut acc\u00e9der \u00e0 une colonne 'col' d'un DataFrame df avec la commande : df['col'] On peut \u00e9galement s\u00e9lectionner un sous-ensemble de donn\u00e9es \u00e0 partir d'un DataFrame, de mani\u00e8re conditionnelle. Par exemple, pour cr\u00e9er un DataFrame df2 \u00e0 partir de notre DataFrame pr\u00e9c\u00e9dent que nous nommerons df , mais ne contenant que les individus pour lesquels la variable 'Latitude' est inf\u00e9rieure \u00e0 48.5, et la variable 'Nom' est \u00e9gale \u00e0 'Pain au chocolat', on utilisera la commande : df2 = df[(df['Latitude']<48.5)&(df['Nom']=='Pain au chocolat')] On obtient alors le DataFrame df2 suivant : 'Ville' 'Latitude' 'Longitude' 'Nom' 1 'Orl\u00e9ans' 47.902734 1.908607 'Pain au chocolat' 3 'Brest' 48.390528 -4.486009 'Pain au chocolat' On remarque que pour l'op\u00e9rateur bool\u00e9en ET on doit utiliser \"&\", et pour l'op\u00e9rateur bool\u00e9en OU on doit utiliser \"|\". Nous utiliserons la biblioth\u00e8que Pandas afin d'importer, d'analyser et de manipuler des donn\u00e9es dans le cadre de ce cours . Scipy Bien que Scipy ne soit pas \u00e0 proprement parler une biblioth\u00e8que d'apprentissage automatique, on trouve dans son module \"stats\" des outils d'inf\u00e9rence statistique de base pour de la classification et de la r\u00e9gression. On peut importer ce module avec la commande Python suivante : import scipy.stats Dans le cadre de ce cours, nous utiliserons Scipy pour certaines m\u00e9thodes de base , car ses impl\u00e9mentations sont moins des \"boites noires\" que pour d'autres biblioth\u00e8ques. Scikit-Learn Scikit-Learn est une biblioth\u00e8que Python pour l' apprentissage automatique . Elle contient des impl\u00e9mentations de m\u00e9thodes pour : La classification supervis\u00e9e . La r\u00e9gression . Le partitionnement . La r\u00e9duction de dimensionnalit\u00e9. On peut importer cette biblioth\u00e8que Python avec la commande : import sklearn M\u00eame si en g\u00e9n\u00e9ral on n'importe pas toute la biblioth\u00e8que, mais seulement les packages dont on a besoin. Nous utiliserons la biblioth\u00e8que Scikit-Learn pour de l'apprentissage de mod\u00e8les dans le cadre de ce cours . Seaborn Seaborn est une biblioth\u00e8que Python d' affichage graphique de donn\u00e9es statistiques. Ses outils d'affichage peuvent s'av\u00e9rer utiles en compl\u00e9ment de ceux de Pandas et Matplotlib. Elle s'importe souvent sous le nom \"sns\" avec la commande : import seaborn as sns Nous utiliserons quelques outils d'affichage graphique de Seaborn dans le cadre de ce cours . Keras-Tensorflow, Pytorch Pour aller plus loin, lorsque le mod\u00e8le est un r\u00e9seau de neurones, et que ce r\u00e9seau contient de nombreuses couches de neurones, on entre dans un sous-domaine de l'apprentissage appel\u00e9 apprentissage profond (\"deep learning\" en anglais). (Nous reparlerons des r\u00e9seaux de neurones dans les chapitres suivants). Pour l'apprentissage profond, il existe 2 grandes biblioth\u00e8ques concurrentes sur Python : Keras-Tensorflow et Pytorch . L'apprentissage profond ne sera pas au programme de ce cours, qui n'est qu'une introduction aux sciences des donn\u00e9es . Conclusion Ce chapitre vous a donn\u00e9 une introduction tr\u00e8s g\u00e9n\u00e9rale aux sciences des donn\u00e9es . La suite de ce cours sera d\u00e9coup\u00e9e en 3 chapitres, portant chacun sur un type d'apprentissage automatique : La classification supervis\u00e9e . La r\u00e9gression . Le partitionnement (\"clustering\" ou \"classification non-supervis\u00e9e\"). Chacun introduira de mani\u00e8re g\u00e9n\u00e9rale son type d'apprentissage, pr\u00e9sentera un panel des mod\u00e8les de base, et donnera des outils d' \u00e9valuation des performances . Un exemple \"fil rouge\" d'application sera utilis\u00e9 pour illustrer chaque chapitre.","title":"I. Introduction"},{"location":"Chap1_Introduction/#chapitre-i-introduction-aux-sciences-des-donnees","text":"Ce chapitre porte sur les grands concepts et les enjeux des sciences des donn\u00e9es.","title":"Chapitre I : Introduction aux sciences des donn\u00e9es"},{"location":"Chap1_Introduction/#analyse-de-donnees","text":"L'explosion des capacit\u00e9s de stockage de donn\u00e9es est \u00e0 l'origine d'une explosion de la taille des jeux de donn\u00e9es \u00e0 traiter. D'o\u00f9 la n\u00e9cessit\u00e9 de trouver de nouvelles mani\u00e8res de manipuler, traiter, analyser et interpr\u00e9ter nos donn\u00e9es. La 1\u00e8re \u00e9tape lorsque l'on est confront\u00e9 \u00e0 un vaste jeu de donn\u00e9es est toujours de l'analyser, afin d'essayer de le comprendre : Quels types de donn\u00e9es contient-il ? Ces donn\u00e9es sont-elles de qualit\u00e9 ? Comment ces donn\u00e9es sont-elles r\u00e9parties ? Peut-on tisser des liens entre les diff\u00e9rentes variables ? Peut-on regrouper les diff\u00e9rentes r\u00e9alisations de ces variables en groupes ? Cette \u00e9tape est essentielle si l'on veut par la suite entrainer un mod\u00e8le \u00e0 \"apprendre\" de nos donn\u00e9es.","title":"Analyse de donn\u00e9es"},{"location":"Chap1_Introduction/#nature-et-type-des-donnees","text":"Une des difficult\u00e9s rencontr\u00e9es en sciences des donn\u00e9es provient de la grande vari\u00e9t\u00e9s des donn\u00e9es .","title":"Nature et type des donn\u00e9es"},{"location":"Chap1_Introduction/#donnees-de-differentes-natures","text":"Tout d'abord, les variables \u00e9tudi\u00e9es peuvent \u00eatre de nature diff\u00e9rente : Une donn\u00e9e quantitative continue peut prendre n'importe quelle valeur num\u00e9rique : par exemple, le prix d'un kilo de farine. Une donn\u00e9e quantitative discr\u00e8te ne peut prendre qu'un nombre fini de valeurs num\u00e9riques dans un intervalle : par exemple, le nombre de p\u00e9pites de chocolats dans une brioche. Une donn\u00e9e qualitative nominale est descriptive sans ordre hi\u00e9rarchique : par exemple, la r\u00e9gion d'origine d'une p\u00e2tisserie. Une donn\u00e9e qualitative ordinale est descriptive avec un ordre hi\u00e9rarchique : par exemple, le niveau de cuisson d'une baguette de pain (blanche, pas trop cuite, bien cuite).","title":"Donn\u00e9es de diff\u00e9rentes natures"},{"location":"Chap1_Introduction/#donnees-multidimensionnelles","text":"Les donn\u00e9es \u00e9tudi\u00e9es peuvent aussi \u00eatre multidimensionnelles . En effet, dans la pluplart des situations, notre jeu de donn\u00e9es peut se mettre sous la forme d'un tableau, dont Les colonnes correspondront aux \" variables \". Les lignes correspondront aux \" individus \" : les diff\u00e9rentes r\u00e9alisations de ces variables. L'ensemble des individus sera nomm\u00e9 \" population \", une s\u00e9lection des individus un \" \u00e9chantillon \". Voici un exemple de jeu de donn\u00e9es multidimensionnelles : Brioche n\u00b01 Poids (g) Nombre de p\u00e9pites de chocolat Prix (\u20ac) 1 70 13 3.5 2 80 17 3.6 3 85 15 3.7 4 83 16 3.4 5 76 18 3.3 6 78 13 3.5 Nous avons ici 6 individus, les brioches, pour lesquelles nous avons mesur\u00e9 3 variables, le poids, le nombre de p\u00e9pites de chocolat, et le prix. Astuce Python Pour stocker puis manipuler des donn\u00e9es multidimensionnelles, on utilise souvent en Python un type de conteneur de la biblioth\u00e8que Pandas : les \" DataFrames \". Les DataFrames se pr\u00e9sentent comme des tableaux pouvant contenir des variables de types diff\u00e9rents, avec un label associ\u00e9 \u00e0 chaque colonne du tableau (variable). Nous reparlerons de Pandas plus loin dans ce chapitre.","title":"Donn\u00e9es multidimensionnelles"},{"location":"Chap1_Introduction/#donnees-structurees","text":"Enfin, les donn\u00e9es \u00e9tudi\u00e9es peuvent \u00eatre structur\u00e9es . On entend par l\u00e0 que des donn\u00e9es peuvent avoir un coh\u00e9rence chronologique (s\u00e9rie temporelle, un son) ou spatiale (une carte, une image, un texte, une vid\u00e9o). Par exemple, dans le cas d'une image : Chaque pixel de l'image doit \u00eatre compris dans le contexte global de l'image. Il est \u00e9vident que changer la position des pixels les uns par rapport aux autres change le jeu de donn\u00e9es : Dans certains cas, l' ordre des donn\u00e9es est donc en soit une information n\u00e9cessaire \u00e0 leur interpr\u00e9tation. Vous l'aurez compris, la nature des donn\u00e9es, leur dimensionnalit\u00e9, ainsi que leur structure, peuvent rendre leur compr\u00e9hension difficile . Nous allons dans la suite voir comment on peut essayer de tirer des informations pertinentes de nos donn\u00e9es.","title":"Donn\u00e9es structur\u00e9es"},{"location":"Chap1_Introduction/#visualisation-graphique","text":"La 1\u00e8re \u00e9tape lorsque l'on cherche \u00e0 comprendre ses donn\u00e9es, c'est d'essayer de les visualiser de mani\u00e8re pertinente. Nous allons voir les types de repr\u00e9sentations graphiques les plus classiques pour visualiser un jeu de donn\u00e9es.","title":"Visualisation graphique"},{"location":"Chap1_Introduction/#courbes-et-nuages-de-points","text":"Lorsque l'on veut afficher les diff\u00e9rentes r\u00e9alisations de 2 variables \\(X\\) et \\(Y\\) l'une en fonction de l'autre , on va classiquement utiliser une courbe ou un nuage de points . Si les donn\u00e9es ne sont pas structur\u00e9es , on peut utiliser un nuage de points , qui va simplement afficher chaque r\u00e9alisation comme un point sur le graphique. Si les donn\u00e9es sont structur\u00e9es , on peut tracer des lignes entre les diff\u00e9rentes r\u00e9alisations, dans l'ordre, ce qui va donner une courbe .","title":"Courbes et nuages de points :"},{"location":"Chap1_Introduction/#diagrammes-en-barres-et-histogrammes","text":"Lorsque l'on veut rapidement comparer des quantit\u00e9s les unes aux autres, on va classiquement utiliser un diagramme en barres ou un histogramme . Si on a des individus \\(A\\) , \\(B\\) et \\(C\\) et que l'on veut comparer les valeurs d'une variable \\(X\\) pour ces 3 individus, on peut utiliser un diagramme en barres . On peut \u00e9galement utiliser un diagramme en barres pour afficher le nombre d'occurences d'une \u00e9tiquette d'une variable qualitative. Si on veut repr\u00e9senter la distribution des valeurs d'une variable \\(X\\) parmi les diff\u00e9rents individus d'une population, on va utiliser un histogramme . Un histogramme affiche le nombre d'occurences \\(N\\) des valeurs sur un intervalle de \\(X\\) . Ceci implique donc de diviser au pr\u00e9alable les valeurs de \\(X\\) en intervalles.","title":"Diagrammes en barres et histogrammes :"},{"location":"Chap1_Introduction/#boites-a-moustaches","text":"Lorsque l'on veut afficher de mani\u00e8re visuellement compr\u00e9hensible la distribution individus pour diff\u00e9rentes variables quantitatives \\(X\\) , \\(Y\\) et \\(Z\\) , on peut utiliser des bo\u00eetes \u00e0 moustaches . On peut \u00e9galement l'utiliser pour visualiser la distribution d'une m\u00eame variable pour diff\u00e9rentes sous-populations . On appelle \"bo\u00eete \u00e0 moustaches\" une repr\u00e9sentation graphique des principaux indicateurs de distribution d'une population pour une variable donn\u00e9e. En g\u00e9n\u00e9ral : le minimum, le 1er quartile, la m\u00e9diane, le 3\u00e8me quartile et le maximum. Nous reparlerons de ces indicateurs plus loin dans ce chapitre.","title":"Bo\u00eetes \u00e0 moustaches :"},{"location":"Chap1_Introduction/#kernel-density-estimation-kde","text":"La KDE ou \"estimation par noyau\" (Kernel Density Estimation) est une m\u00e9thode non-param\u00e9trique pour estimer la distribution de probabilit\u00e9 d'une variable . L'avantage est que contrairement \u00e0 un histogramme, on a des valeurs continues entre 0 et 1 (on ne divise pas en intervalles). On peut l'utiliser pour afficher sous la forme d'une courbe la densit\u00e9 de probabilit\u00e9 d'une variable \\(X\\) (KDE 1D). On peut aussi afficher la densit\u00e9 de probabilit\u00e9 d'une variable \\(X\\) par rapport \u00e0 une autre variable \\(Y\\) , sous la forme d'une carte 2D avec des isolignes de densit\u00e9 de probabilit\u00e9 (KDE 2D).","title":"Kernel Density Estimation (KDE) :"},{"location":"Chap1_Introduction/#diagramme-circulaire-camembert","text":"Lorsque l'on veut afficher des proportions \u00e0 comparer, on utilise souvent le diagramme circulaire , aussi connu sous le nom de \"diagramme camembert\". Il s'agit simplement de diviser un cercle en sections, dont la largeur repr\u00e9sentera une proportion, et le cercle complet la proportion totale. On peut par exemple l'utiliser pour v\u00e9rifier la r\u00e9partition de variables qualitatives au sein d'une base de donn\u00e9es.","title":"Diagramme circulaire (camembert) :"},{"location":"Chap1_Introduction/#graphique-en-aires","text":"Lorsque l'on veut afficher l'\u00e9volution d'une proportion au cours d'un variable (souvent au cours du temps), on utilise en g\u00e9n\u00e9ral un graphique en aires . On peut par exemple l'utiliser pour v\u00e9rifier la r\u00e9partition de variables qualitatives au sein d'une base de donn\u00e9es, en fonction d'une variable quantitative \\(X\\) , par exemple le temps.","title":"Graphique en aires :"},{"location":"Chap1_Introduction/#en-python","text":"Astuce Python La biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, propose une m\u00e9thode \"plot\" \u00e0 ses objets \"DataFrame\" qui permet des affichages graphiques \u00e0 partir de jeux de donn\u00e9es. Il suffit donner le bon param\u00e8tre \"kind\" en entr\u00e9e pour obtenir le type d'affichage voulu : - \"line\" : une courbe. - \"scatter\" : un nuage de points. - \"bar\" : un diagramme en barres vertical. - \"barh\" : un diagramme en barres horizontal. - \"hist\" : un histogramme. - \"box\" : des bo\u00eetes \u00e0 moustaches. - \"kde\" : une \"kernel density estimation\". - \"pie\" : un diagramme circulaire. - \"area\" : un graphique en aires.","title":"En Python"},{"location":"Chap1_Introduction/#statistiques-descriptives","text":"Toujours dans l'objectif de comprendre notre jeu de donn\u00e9es, on peut essayer de d\u00e9crire chaque variable par des indicateurs statistiques . Nous allons voir ici les indicateurs les plus communs en statistiques descriptives. Il est important de savoir comment ces indicateurs sont d\u00e9finis afin de comprendre les informations qu'ils donnent ou ne donnent pas sur un jeu de donn\u00e9es.","title":"Statistiques descriptives"},{"location":"Chap1_Introduction/#moyenne-mediane-et-mode","text":"Lorsque l'on veut connaitre l'ordre de grandeur des valeurs d'une variable, l\u00e0 o\u00f9 se rassemblent la plupart des valeurs, on va utiliser un indicateur de tendance centrale : moyenne, m\u00e9diane ou mode. Il existe plusieurs fa\u00e7on de d\u00e9finir la moyenne, mais la plus connue est la moyenne arithm\u00e9tique : \\(\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i\\) On note en effet souvent \\(\\overline{x}\\) la moyenne d'une variable \\(x\\) . La m\u00e9diane est la valeur s\u00e9parant les valeurs de la variable en 2 groupes de m\u00eame taille : la moiti\u00e9 des valeurs sont sup\u00e9rieures \u00e0 la m\u00e9diane, l'autre moiti\u00e9 lui sont inf\u00e9rieures. Le mode est la valeur la plus repr\u00e9sent\u00e9e dans l'ensemble des valeurs de la variable. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".mean()\": la moyenne. - \".median()\": la m\u00e9diane. - \".mode()\": le mode.","title":"Moyenne, m\u00e9diane et mode"},{"location":"Chap1_Introduction/#variance-et-ecart-type","text":"Lorsque l'on veut savoir \u00e0 quel point les valeurs d'une variable fluctuent autour de la valeur centrale, on va utiliser des indicateurs de dispersion . Les valeurs extr\u00eames de la variable, le min et le max , pour connaitre l'\u00e9tendue de la variable. La variance est d\u00e9finie par la moyenne des carr\u00e9es des \u00e9carts \u00e0 la moyenne : \\(\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\\) L' \u00e9cart-type (souvent not\u00e9 \\(\\sigma\\) ) est la racine carr\u00e9e de la variance, soit a moyenne quadratique de \u00e9carts \u00e0 la moyenne. Contrairement \u00e0 la variance, il a l'avantage d'\u00eatre homog\u00e8ne \u00e0 la variable \u00e9tudi\u00e9e . Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".min()\" et \".max()\": le minimum et le maximum. - \".var()\": la variance. - \".std()\": l'\u00e9cart-type.","title":"Variance et \u00e9cart-type"},{"location":"Chap1_Introduction/#quantiles","text":"Afin d'avoir plus d'informations sur la r\u00e9partition de valeurs d'une variable, on peut g\u00e9n\u00e9raliser la notion de m\u00e9diane en utilisant ce que l'on appelle les quantiles : La division des valeurs de la variables en groupes de tailles \u00e9gales. Quartiles : 3 indicateurs en divisant les valeurs de la variable en 4 groupes (25%,50% et 75%). D\u00e9ciles : 9 indicateurs en divisant les valeurs de la variable en 10 groupes (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%). Centiles : 99 indicateurs en divisant les valeurs de la variable en 100 groupes (1%, 2%, 3%, ..., 98%, 99%).","title":"Quantiles"},{"location":"Chap1_Introduction/#asymetrie-et-kurtosis","text":"Enfin, si l'on veut une information sur la r\u00e9partition des valeurs d'une variable, sous la forme d'un indicateur unique, on va utiliser un indicateur de forme . Le coefficient d' asym\u00e9trie (\"skewness\" en anglais, souvent not\u00e9 \\(\\gamma_1\\) ) permet de quantifier le d\u00e9sequilibre de la r\u00e9partition des valeurs de la variable de chaque c\u00f4t\u00e9 de sa valeur centrale. \\(\\gamma_1 = \\frac{1}{N \\sigma^3} \\sum_{i=1}^{N} (x_i - \\overline{x})^3\\) Un coefficient n\u00e9gatif indique un d\u00e9calage \u00e0 droite, un coefficient positif un d\u00e9calage \u00e0 gauche, et un coefficient nul une distribution sym\u00e9trique. NB : Pour la loi normale, on a \\(\\gamma_1 = 0\\) . Le kurtosis (souvent not\u00e9 \\(\\gamma_2\\) ) permet de quantifier l'acuit\u00e9 ou l'applatissement de la r\u00e9partition des valeurs de la variable autour de sa valeur centrale. \\(\\gamma_2 = \\frac{1}{N \\sigma^4} \\sum_{i=1}^{N} (x_i - \\overline{x})^4\\) Un kurtosis positif est un indicateur de valeurs anormales de la variable (aux extr\u00eamit\u00e9s, aussi appel\u00e9es \"outliers\") plus fr\u00e9quentes. Un kurtosis n\u00e9gatif est un indicateur d'une distribution tr\u00e8s applatie des valeurs de la variable. NB : Pour la loi normale, on a \\(\\gamma_2 = 0\\) .","title":"Asym\u00e9trie et kurtosis"},{"location":"Chap1_Introduction/#recherche-de-correlation","text":"Une fois que l'on a d\u00e9crit statistiquement les diff\u00e9rentes variables de notre jeu de donn\u00e9es, on va souvent vouloir essayer des tisser des liens entre ces variables. Cette analyse exploratoire des donn\u00e9es a 2 principales utilit\u00e9s : Voir si une ou plusieurs variables pourraient servir \u00e0 en pr\u00e9dire une ou plusieurs autres. Essayer de r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me bas\u00e9 sur ces variables. En effet, comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es sont souvent multidimensionnels. Quand la dimension des donn\u00e9es devient tr\u00e8s grande, la quantit\u00e9 de donn\u00e9es devient peu dense en comparaison, ce qui rend difficile leur interpr\u00e9tation. On appelle commun\u00e9ment ce probl\u00e8me le \"Fl\u00e9au de la dimension\" (\"curse of dimensionality\" en anglais). Nous allons voir dans un 1er temps comment essayer de d\u00e9terminer ce que l'on appelle des \" corr\u00e9lations \" entre variables. Puis nous verrons une m\u00e9thode classique de r\u00e9duction de dimension appel\u00e9e \" Analyse en Composantes Principales \".","title":"Recherche de corr\u00e9lation"},{"location":"Chap1_Introduction/#matrice-de-correlation","text":"Une 1\u00e8re approche pour essayer de tisser des liens ou \" corr\u00e9lations \" entre les variables et de tracer ce que l'on appelle une matrice de nuages de points, ou \" scatter-matrix \" en anglais. L'id\u00e9e est d'afficher une matrice de graphiques , repr\u00e9sentant chacun une variable en fonction d'une autre , sous la forme d'un nuage de points. La diagonale n'\u00e9tant pas tr\u00e8s utile (une variable en fonction d'elle-m\u00eame), on la remplace en g\u00e9n\u00e9ral par un histogramme de la variable en question. Ce type de repr\u00e9sentation permet de d\u00e9tecter visuellement des relations entre les variables . Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"plotting.scatter_matrix()\", qui permet d'afficher une \"scatter_matrix\". Pour quantifier la corr\u00e9lation entre 2 variables \\(x\\) et \\(y\\) , on va souvent se contenter de mesurer \u00e0 quel point une relation lin\u00e9aire \\(y = a x + b\\) peut \u00eatre tir\u00e9e de ces variables. Pour cela, on va calculer le coefficient de corr\u00e9lation de Pearson : \\(r = \\frac{\\sum_{i=1}^{N} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^{N} (x_i - \\overline{x})^2 \\sum_{i=1}^{N} (y_i - \\overline{y})^2}}\\) La valeur de ce coefficient est toujours compris entre -1 et 1 : Une valeur de 1 signifie une corr\u00e9lation parfaite entre les variables. Une valeur de -1 signifie une anti-corr\u00e9lation parfaite entre les variables. Une valeur de 0 signifie une d\u00e9corr\u00e9lation parfaite entre les variables : elles sont ind\u00e9pendantes . Il y a du sens \u00e0 vouloir pr\u00e9dire une variable \u00e0 partir d'une autre si elles sont corr\u00e9l\u00e9es / anti-corr\u00e9l\u00e9es. On peut aussi imaginer r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me s'il se base sur plusieurs variables qui ne sont pas ind\u00e9pendantes. NB : Attention ! Corr\u00e9lation entre variables n'implique pas causalit\u00e9 entre variables ! On affiche souvent les coefficients de corr\u00e9lation obtenus pour toutes les combinaisons de variables possibles sous la forme d'une matrice : la matrice de corr\u00e9lation de ces variables. La diagonale de la matrice ne contient bien \u00e9videmment que des 1. Voici un exemple o\u00f9 cherche les corr\u00e9lations entre le diam\u00e8tre, le temps de cuisson, la masse de frangipane et le prix d'une galette des rois : Diam\u00e8tre Cuisson Frangipane Prix Diam\u00e8tre 1 0.8 0.7 0.9 Cuisson 0.8 1 0.5 0.2 Frangipane 0.7 0.5 1 0.6 Prix 0.9 0.2 0.6 1 La moiti\u00e9 de l'information contenue dans cette matrice \u00e9tant redondante, on n'affiche parfois que la partie triangulaire sup\u00e9rieure ou inf\u00e9rieure de cette matrice. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"corr()\" associ\u00e9e aux DataFrames. Elle retourne une matrice de corr\u00e9lation du jeu de donn\u00e9es.","title":"Matrice de corr\u00e9lation"},{"location":"Chap1_Introduction/#analyse-en-composantes-principales-acp","text":"Comme mentionn\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es que l'on rencontre sont souvent multidimensionels. Ceci rend difficile voir impossible un affichage graphique compr\u00e9hensible des individus d'une variable par rapport \u00e0 une autre (il faudrait un graphique 2D pour 2 variables, 3D pour 3 variables, 4D pour 4 variables, etc.). Afin de repr\u00e9senter des donn\u00e9es multidimensionnelles sous la forme d'un affichage graphique de dimension faible (en g\u00e9n\u00e9ral 1, 2 ou 3), on utilise souvent une m\u00e9thode de r\u00e9duction de dimensionnalit\u00e9 connue sous le nom d' Analyse en Composantes Principales (ACP). L'id\u00e9e est la suivante. Soit un jeu de donn\u00e9es contenant \\(p\\) variables et \\(n\\) individus. On va chercher \\(q\\) nouvelles variables par projections lin\u00e9aires des \\(p\\) variables d'origine, avec \\(q < p\\) , de mani\u00e8re \u00e0 perdre le moins d'information possible sur le jeu de donn\u00e9es. Ces \\(q\\) nouvelles variables sont alors nomm\u00e9es composantes principales . Il existe plusieurs algorithmes pour obtenir ce r\u00e9sultat, celui impl\u00e9ment\u00e9 dans la biblioth\u00e8que Python Scikit-Learn se base sur la D\u00e9composition en Valeurs Singuli\u00e8res (SVD) de la matrice de donn\u00e9es : \\(X = \\begin{pmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} &\\cdots & x_{n,p} \\end{pmatrix}\\) o\u00f9 chaque colonne correspond \u00e0 une variable, et chaque ligne correspond \u00e0 un individu. On peut voir l'ACP comme le choix du sous-espace de dimension \\(q\\) tel que le nuage de points projet\u00e9s ait la variance la plus grande possible. Les r\u00e9sultats d'une ACP peuvent \u00eatre affich\u00e9s sous la forme d'un nuage de points 2D ou 3D ( \\(q = 2\\) ou \\(3\\) ) repr\u00e9sentant les diff\u00e9rents individus, avec pour axes les composantes principales : L'id\u00e9e est de voir si on peut s\u00e9parer les individus en diff\u00e9rents groupes \u00e0 partir des composantes principales. Pour juger de la qualit\u00e9 d'une ACP, on utilise un type de graphique appel\u00e9 \" cercle des corr\u00e9lations \". Ce graphique 2D repr\u00e9sente sur chaque axe la corr\u00e9lation des \\(p\\) variables d'origine avec les \\(q\\) composantes principales. Chacune des \\(p\\) variables correspond \u00e0 un vecteur sur ce graphique, et un cercle de rayon 1 est \u00e9galement affich\u00e9 pour comparaison. Voici un exemple : Un cercle des corr\u00e9lations permet donc de juger de la corr\u00e9lation des variables d'origines avec les composantes principales, et de la corr\u00e9lation des variables d'origine entre elles : Plus une variable d'origine est proche du cercle, plus elle est fid\u00e8lement repr\u00e9sent\u00e9e par l'ACP. Dans l'id\u00e9al, on voudrait donc que toutes les variables soient proches du cercle. Pour 2 variables d'origine proches du cercle, si l'angle entre 2 les variables est aigu elles sont corr\u00e9l\u00e9es, s'il est obtu elles sont anti-corr\u00e9l\u00e9es, et s'il est droit elles sont d\u00e9corr\u00e9l\u00e9es. On pourra utiliser la projection des donn\u00e9es renvoy\u00e9e par l'ACP pour entrainer des mod\u00e8les d'apprentissage. Astuce Python La classe \"sklearn.decomposition.PCA\" de la biblioth\u00e8que \"Scikit-Learn\" vous permet de r\u00e9aliser l'ACP d'une matrice de donn\u00e9es. Le nombre de composantes principales \u00e0 trouver est un des attributs de la classe \u00e0 initialiser (\"n_components\"). Pour obtenir les composantes principales d'une matrice de donn\u00e9es, il faut lui appliquer la m\u00e9thode \"fit_transform()\" de la classe. Pour aller plus loin : D'autres m\u00e9thodes de r\u00e9duction de dimensionnalit\u00e9 existent, on peut citer entre autres les \"auto-encodeurs\" et la \"t-SNE\".","title":"Analyse en Composantes Principales (ACP)"},{"location":"Chap1_Introduction/#preparation-des-donnees","text":"Une fois les donn\u00e9es analys\u00e9es, on a normalement une bonne id\u00e9e de ce qu'un outil automatique pourra en \"apprendre\" ou non. Cependant, la plupart de ces outils (dont nous parlerons dans la section suivante), ont besoin que les donn\u00e9es soient \" transform\u00e9es \" d'une certaine mani\u00e8re. C'est pourquoi nous allons voir dans cette section quelques transformations classiques pour pr\u00e9parer nos donn\u00e9es . Tout d'abord, il est possible que le jeu de donn\u00e9es contienne des valeurs erron\u00e9es ou manquantes , souvent marqu\u00e9es par des NaN (\"Not a Number\"). Il convient alors de se d\u00e9barrasser de ces valeurs avant apprentissage, car la plupart des outils ne savent pas g\u00e9rer ce probl\u00e8me. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"dropna\" associ\u00e9e aux objets DataFrames. Cette m\u00e9thode permet de supprimer les NaN d'un DataFrame. Nous allons voir que les donn\u00e9es qualitatives doivent \u00eatre encod\u00e9es avant apprentissage, soit en \"ordinal\", soit en \"one-hot\". Enfin, les outils d'apprentissage sont affect\u00e9s par les diff\u00e9rences d'ordre de grandeur entre les variables . C'est pourquoi une remise \u00e0 l'\u00e9chelle des diff\u00e9rentes variables d'un jeu de donn\u00e9es est n\u00e9cessaire avant apprentissage. On appelle ce processus recalibration , ou \" feature scaling \" en anglais. Nous allons voir en particulier 2 types de transformation pour recalibrer des donn\u00e9es : la transformation min-max et le centrage-r\u00e9duction .","title":"Pr\u00e9paration des donn\u00e9es"},{"location":"Chap1_Introduction/#transformation-min-max","text":"Certains types de mod\u00e8les d'apprentissage n\u00e9cessitent des valeurs d'entr\u00e9e entre 0 et 1. C'est pourquoi la transformation min-max (\"normalization\" en anglais) va recalibrer toutes les variables de mani\u00e8re \u00e0 ce que leurs valeurs restent entre 0 et 1 . Pour ce faire, on va appliquer la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x_i-min_j}{max_j-min_j}\\) avec \\(min_j\\) le minimum et \\(max_j\\) le maximum des individus de la j-\u00e8me variable. (Il est \u00e9galement possible d'adapter cette transformation pour les mod\u00e8les prenant des valeurs entre -1 et 1 en entr\u00e9e). Le probl\u00e8me majeur avec cette transformation est sa sensibilit\u00e9 aux valeurs aberrantes. En effet, il suffit qu'une variable ait une valeur aberrante pour qu'elle devienne le minimum ou le maximum, impactant ainsi la transformation. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"MinMaxScaler\".","title":"Transformation min-max"},{"location":"Chap1_Introduction/#transformation-centrage-reduction","text":"La transformation centrage-reduction (\"standardization\" en anglais) applique la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x-\\overline{x_p}}{\\sigma_p}\\) avec \\(\\overline{x_p}\\) la moyenne et \\(\\sigma_p\\) l'\u00e9cart-type des individus de la j-\u00e8me variable. Cette transformation est beaucoup moins sensible aux valeurs aberrantes, mais elle ne garanti pas que les valeurs des diff\u00e9rentes variables seront entre 0 et 1 (ou -1 et 1). Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"StandardScaler\".","title":"Transformation centrage-r\u00e9duction"},{"location":"Chap1_Introduction/#autres-transformations","text":"Nous l'avons pr\u00e9c\u00e9demment, on peut d\u00e9couvrir que les individus d'une variable ont une distribution asym\u00e9trique. Par exemple, la distribution des individus peut avoir une longue tra\u00eene d'un c\u00f4t\u00e9 de la m\u00e9diane. On peut aussi avoir une distribution multimodale (c'est-\u00e0-dire avec plusieurs pics). Ceci peut perturber un apprentissage automatique. Dans ces situations, d'autres types de transformation pourrons alors \u00eatre envisag\u00e9es en addition des 2 pr\u00e9c\u00e9dentes : utiliser la racine carr\u00e9e ou le logarithme de la variable, utiliser les quantiles de la variable, utiliser un encodage de la variable, etc. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" permet de cr\u00e9er sa propre transformation, avec \"FunctionTransformer\".","title":"Autres transformations"},{"location":"Chap1_Introduction/#encodage-des-donnees","text":"La plupart des mod\u00e8les d'apprentissage automatique dont nous allons parler dans ce cours ne peuvent manipuler que des valeurs num\u00e9riques. On va donc en g\u00e9n\u00e9ral encoder des donn\u00e9es qualitatives avec des valeurs num\u00e9riques . Par exemple : Cuisson du pain Encodage Blanc 1 Pas trop cuit 2 Bien cuit 3 On appelle cet encodage, encodage par \u00e9tiquette (\"label encoding\"). Cette m\u00e9thode fonctionne toujours pour des donn\u00e9es ordinales comme la cuisson du pain, mais pour des donn\u00e9es nominales le mod\u00e8le risque de croire qu'il y a un ordre hi\u00e9rarchique dans les donn\u00e9es qui n'existe pas. C'est pourquoi on utilise souvent l'encodage one-hot . L'id\u00e9e est de faire comme si chaque nom possible pour une variable qualitative \u00e9tait une variable en soit. On appelle parfois ces variables imaginaires des \"dummy variables\". Par exemple, pour la r\u00e9gion d'origine des p\u00e2tisseries, on passe de : P\u00e2tisserie R\u00e9gion Croissant Paris Merveilleux Nord Kouign-amann Bretagne Cannel\u00e9 Sud-Ouest Kougelhopf Est \u00e0 l'encodage one-hot suivant : P\u00e2tisserie Paris Nord Bretagne Sud-Ouest Est Croissant 1 0 0 0 0 Merveilleux 0 1 0 0 0 Kouign-amann 0 0 1 0 0 Cannel\u00e9 0 0 0 1 0 Kougelhopf 0 0 0 0 1 Pour le Merveilleux, on donnera donc en entr\u00e9e d'un mod\u00e8le le binaire 01000. On remarque ici que plus la variable a de noms possibles, et plus les binaires d'encodage one-hot seront longs, ce qui peut \u00eatre probl\u00e9matique. Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"preprocessing\" des fonctions \"LabelEncoder\" et \"OrdinalEncoder\", permettant d'assigner un entier \u00e0 des variables qualitatives nominales ou ordinales. Dans ce m\u00eame package, vous trouverez \u00e9galement une fonction \"OneHotEncoder\", permettant d'encoder en one-hot des variables qualitatives nominales. Dans les 2 cas, il vous faut cr\u00e9er une instance de \"OrdinalEncoder\" ou de \"OneHotEncoder\", puis utiliser la m\u00e9thode \"fit_transform()\" avec vos donn\u00e9es en entr\u00e9e.","title":"Encodage des donn\u00e9es"},{"location":"Chap1_Introduction/#les-apprentissages","text":"Une fois que l'on a bien cern\u00e9 notre jeu de donn\u00e9es, et qu'on l'a transform\u00e9 de mani\u00e8re ad\u00e9quate, on va en g\u00e9n\u00e9ral vouloir le mod\u00e9liser . L'id\u00e9e du mod\u00e8le sera de prendre une d\u00e9cision sur \u00e0 partir de nouvelles donn\u00e9es, en se basant sur la connaissance des donn\u00e9es de la base d'origine. On parle alors d'\"apprendre\" des donn\u00e9es.","title":"Les apprentissages"},{"location":"Chap1_Introduction/#lapprentissage-automatique","text":"Par \"mod\u00e9liser\", on entend trouver une fonction param\u00e9trique \\(M\\) qui permet de d\u00e9duire une sortie vectorielle \\(y\\) voulue \u00e0 partir d'une entr\u00e9e vectorielle de nouvelles donn\u00e9es \\(x\\) et de nos connaissances sur les donn\u00e9es d'origine : \\(y = M(x,\\theta)\\) avec \\(\\theta\\) les param\u00e8tres du mod\u00e8le, qui correspondent \u00e0 notre connaissance du jeu de donn\u00e9es initial. Il nous faut donc ajuster les param\u00e8tres \\(\\theta\\) pour obtenir la sortie \\(y\\) attendue en fonction de \\(x\\) qui colle le plus aux donn\u00e9es. C'est ce processus d' optimisation de \\(\\theta\\) que l'on appelle \" apprentissage \". Les jeux de donn\u00e9es dont on doit apprendre sont en g\u00e9n\u00e9ral \u00e9normes, ce qui rend souvent un ajustement manuel des param\u00e8tres impossible. C'est pourquoi on va en g\u00e9n\u00e9ral choisir un type de mod\u00e8le , et ajuster automatiquement les param\u00e8tres \u00e0 nos donn\u00e9es. D'o\u00f9 l'expression \" apprentissage automatique \". Suivant les applications, il existe diff\u00e9rents types d'apprentissage, avec pour chacun diff\u00e9rents types de mod\u00e8les possibles. Lors de ce cours, nous verrons 3 grands types d'apprentissage, et nous verrons pour chacun quelques exemples de mod\u00e8les classiques.","title":"L'apprentissage automatique"},{"location":"Chap1_Introduction/#les-3-grands-types-dapprentissages","text":"En apprentissage, on appelle souvent en anglais les entr\u00e9es d'un mod\u00e8le les \" features \", et les sorties des \" labels \". Lors du processus d' apprentissage (ajustement des param\u00e8tres), on va enseigner au mod\u00e8le comment d\u00e9terminer des \"labels\" correspondant \u00e0 des \"features\", en se basant sur ce qu'il a appris d'une base de donn\u00e9es d'\"entrainement\" de \"features\". Par exemple, on peut vouloir entrainer un mod\u00e8le \u00e0 associer \u00e0 une photo de viennoiserie (feature) le nom de la viennoiserie (label) : Ou alors, on peut vouloir entrainer un mod\u00e8le \u00e0 associer \u00e0 la masse de farine, la masse de beurre, le volume de lait et le diam\u00e8tre d'un lot de cr\u00eapes (features) le prix de la cr\u00eape (label) :","title":"Les 3 grands types d'apprentissages"},{"location":"Chap1_Introduction/#apprentissage-supervise-ou-non-supervise","text":"Dans le cas o\u00f9 les donn\u00e9es d'apprentissage ont des \"labels\" d\u00e9finis, le mod\u00e8le va apprendre \u00e0 retrouver ces \"labels\" (connus) pour ces \"features\". On esp\u00e8re alors qu'apr\u00e8s apprentissage, le mod\u00e8le pourra retourner les \"labels\" corrects une fois confront\u00e9 \u00e0 des \"features\" issus de nouvelles donn\u00e9es. On parle alors de \" g\u00e9n\u00e9ralisation \". Comme on peut directement v\u00e9rifier les performances du mod\u00e8le \u00e0 pr\u00e9dire les \"labels\" du jeu de donn\u00e9es d'entrainement, on parle d' apprentissage supervis\u00e9 . Le processus est en g\u00e9n\u00e9ral it\u00e9ratif : le mod\u00e8le va se mettre \u00e0 jour au fur et \u00e0 mesure des it\u00e9rations pour r\u00e9duire l'erreur de pr\u00e9diction. On a donc besoin d'une fonction d'\u00e9valuation de l'erreur, aussi appel\u00e9e \" fonction de co\u00fbt \". Voici le concept r\u00e9sum\u00e9 graphiquement, avec notre exemple de l'association d'un nom de viennoiserie \u00e0 une photo : Dans le cas o\u00f9 les donn\u00e9es d'apprentissage n'ont pas de \"labels\", on peut tout de m\u00eame essayer de diviser les individus des \"features\" en diff\u00e9rent groupes, auxquels on assignera des \"labels\" plus tard. On appelle la premi\u00e8re \u00e9tape \" partition \", et la seconde \" lab\u00e9lisation \". On a besoin d'un crit\u00e8re de proximit\u00e9 entre individus afin de d\u00e9finir ces groupes, souvent une \" distance \". Il s'agit aussi souvent d'un processus it\u00e9ratif. Comme nous n'avons pas de \"labels\" d'entrainement comme r\u00e9f\u00e9rence, on parle d' apprentissage non-supervis\u00e9 ou \"clustering\" (\"partition de donn\u00e9es\"). Voici le concept r\u00e9sum\u00e9 graphiquement, toujours avec notre exemple de l'association d'un nom de viennoiserie \u00e0 une photo :","title":"Apprentissage supervis\u00e9 ou non-supervis\u00e9"},{"location":"Chap1_Introduction/#classification-et-regression","text":"On peut aussi diviser les apprentissages suivant la nature des sorties attendue, et donc de mod\u00e8le \u00e0 entrainer. Si la sortie est quantitative discr\u00e8te ou qualitative , on va parler de \" classification \". Si la sortie est quantitative continue , on va parler de \" r\u00e9gression \". Si l'on reprend nos 2 exemples pr\u00e9c\u00e9dents : Le nom d'une viennoiserie \u00e9tant une \u00e9tiquette, soit une variable qualitative, il s'agit d'un probl\u00e8me de classification. Le prix d'une cr\u00eape \u00e9tant une variable quatitative continue, il s'agit d'un probl\u00e8me de r\u00e9gression. On peut entrainer un mod\u00e8le de classification de mani\u00e8re supervis\u00e9e ou non-supervis\u00e9e . On ne peut entrainer un mod\u00e8le de r\u00e9gression que de mani\u00e8re supervis\u00e9e . NB : Seule la classification peut \u00eatre \"non-supervis\u00e9e\", et on parle en g\u00e9n\u00e9ral directement de partitionnement (\"clustering\").","title":"Classification et r\u00e9gression"},{"location":"Chap1_Introduction/#pour-aller-plus-loin","text":"Il existe un 3\u00e8me type d'apprentissage, que nous ne d\u00e9taillerons pas dans ce cours, qui s'appelle \"apprentissage par renforcement \". L'id\u00e9e est la suivante : Le mod\u00e8le est directement mis en place sur son cas d'application final. Le mod\u00e8le prend des d\u00e9cisions en fonction des situations, et re\u00e7oit un retour (\"feedback\") sur sa d\u00e9cision, positif ou n\u00e9gatif. Le mod\u00e8le se met \u00e0 jour en fonction du retour qu'il a re\u00e7u. Ce processus se r\u00e9p\u00e8te pour chaque nouvelle situation, et ainsi le mod\u00e8le apprend de ses exp\u00e9riences . Voici un petit sch\u00e9ma r\u00e9capitulatif des diff\u00e9rents types d'apprentissages que nous avons vus :","title":"Pour aller plus loin..."},{"location":"Chap1_Introduction/#difficultes-de-lapprentissage","text":"Comme expliqu\u00e9 plus haut, l' apprentissage est un processus d' optimisation , qui consiste en l' ajustement des param\u00e8tres d'un mod\u00e8le en se basant sur les donn\u00e9es disponibles, dans le but de prendre des d\u00e9cisions correctes \u00e0 partir de donn\u00e9es futures ( g\u00e9n\u00e9ralisation ). La phase durant laquelle on ajuste les param\u00e8tres est appel\u00e9e entra\u00eenement , et les donn\u00e9es sur lesquelles cet ajustement est fait sont appel\u00e9es \" base de donn\u00e9es d'entra\u00eenement \". Dans la section qui suit, nous aurons un aper\u00e7u des grandes difficult\u00e9es que l'on peut rencontrer lors de l'entrainement d'un mod\u00e8le, tous types de mod\u00e8les confondus.","title":"Difficult\u00e9s de l'apprentissage"},{"location":"Chap1_Introduction/#quantite-et-qualite-des-donnees","text":"S'il n'y a pas de r\u00e8gle pr\u00e9cise pour d\u00e9terminer la quantit\u00e9 de donn\u00e9es n\u00e9cessaire \u00e0 un apprentissage, il y a 2 maximes \u00e0 retenir : Plus on a de donn\u00e9es d'entrainement, meilleur sera l'apprentissage par le mod\u00e8le. Plus le probl\u00e8me complexe, plus il faudra de donn\u00e9es d'entrainement. Par exemple, dans notre exemple de classification des images de viennoiseries : Pour donner un ordre de grandeur, la quantit\u00e9 d'individus n\u00e9cessaires \u00e0 un apprentissage va en g\u00e9n\u00e9ral de quelques milliers \u00e0 des centaines de millions . Cependant, il n'est pas ais\u00e9 de constituer une base de donn\u00e9es aussi large, et de surcroit une base de donn\u00e9e de qualit\u00e9. En effet, comme on peut facilement le deviner, la qualit\u00e9 des donn\u00e9es aura un impact sur l'apprentissage. La qualit\u00e9 des donn\u00e9es peut par exemple \u00eatre d\u00e9grad\u00e9e par : La pr\u00e9sence d' individus ab\u00e9rrants (\"outliers\"), li\u00e9e \u00e0 des erreurs de mesures ou \u00e0 des cas exceptionnels. Des individus manquants , li\u00e9s \u00e0 notre \u00e9chantillonnage ou a des erreurs de mesures. La pr\u00e9sence de bruit dans les donn\u00e9es. Toujours pour notre exemple, on peut avoir des donn\u00e9es manquantes ou ab\u00e9rrantes : D'o\u00f9 la n\u00e9cessit\u00e9 de proc\u00e9der \u00e0 un nettoyage des donn\u00e9es en amont de l'apprentissage : supprimer certaines donn\u00e9es, les combler, ou faire de nouvelles mesures. Malheureusement, nettoyer les donn\u00e9es implique parfois de diminuer la quantit\u00e9 de donn\u00e9es : on est donc souvent confront\u00e9 \u00e0 un compromis entre quantit\u00e9 et qualit\u00e9 des donn\u00e9es.","title":"Quantit\u00e9 et qualit\u00e9 des donn\u00e9es"},{"location":"Chap1_Introduction/#representativite-et-equilibre-des-donnees","text":"Comme expliqu\u00e9 pr\u00e9cedemment, notre but est d'obtenir \u00e0 partir de notre base de donn\u00e9es d'entrainement un mod\u00e8le qui soit g\u00e9n\u00e9ralisable \u00e0 toutes nouvelles donn\u00e9es que l'on peut rencontrer. Pour atteindre cet objectif, il faut que la base de donn\u00e9es que l'on utilise pour entrainer le mod\u00e8le soit repr\u00e9sentative de la distribution des diff\u00e9rentes variables de mani\u00e8re g\u00e9n\u00e9rale. Ceci implique de faire attention \u00e0 la repr\u00e9sentativit\u00e9 de notre population au moment de l' \u00e9chantillonnage , sous peine que le mod\u00e8le ait du mal \u00e0 g\u00e9n\u00e9raliser. Parfois, certains types d'individus sont par nature sous ou sur-repr\u00e9sent\u00e9s dans la population g\u00e9n\u00e9rale, et donc le seront toujours si on \u00e9chantillonne de mani\u00e8re repr\u00e9sentative. Comme on peut s'y attendre, ceci va avoir tendance \u00e0 biaser notre mod\u00e8le. Par exemple, mettons que l'on veuille entrainer un mod\u00e8le \u00e0 reconnaitre une photo d'un pain au chocolat d'un pain suisse. Les pains au chocolat \u00e9tant plus courants en boulangerie que les pains suisses, on aura un d\u00e9s\u00e9quilibre dans la base de donn\u00e9es d'entrainement, qui fera que notre mod\u00e8le aura plus tendance \u00e0 pr\u00e9dire qu'une photo montre un pain au chocolat qu'un pain suisse. Il existe plusieurs m\u00e9thodes pour \u00e9viter les biais d'entrainement, en jouant soit sur l'\u00e9chantillonage, soit sur les poids accord\u00e9s aux diff\u00e9rentes donn\u00e9es pendant l'entrainement. Il est \u00e0 noter qu'un mauvais \u00e9chantillonage ou un d\u00e9s\u00e9quilibre d'une base de donn\u00e9es utilis\u00e9e pour la tester notre mod\u00e8le apr\u00e8s entrainement est aussi probl\u00e9matique : si on teste notre mod\u00e8le sur une base de donn\u00e9es ne contenant que des photos de pains au chocolat, il est \u00e9vident que notre mesure des performances du mod\u00e8le ne vaudra pas grand chose.","title":"Repr\u00e9sentativit\u00e9 et \u00e9quilibre des donn\u00e9es"},{"location":"Chap1_Introduction/#pertinence-des-variables","text":"Pour que notre mod\u00e8le soit capable de renvoyer les labels (sorties) d\u00e9sir\u00e9s, il faut que les features (entr\u00e9es) le permettent. Le s\u00e9lection de variables pertinentes pour un probl\u00e8me donn\u00e9 est donc capital. C'est ce que l'on appelle en anglais le \" feature engineering \". On peut diviser cette activit\u00e9 en 2 grandes techniques : la s\u00e9lection de variables et l' extraction de variables .","title":"Pertinence des variables"},{"location":"Chap1_Introduction/#la-selection-de-variables","text":"Ce n'est pas parce qu'une variable est dans la base de donn\u00e9es qu'il faut l'utiliser pour entrainer un mod\u00e8le. Il faut faire un tri pour choisir les variables pertinentes pour un probl\u00e8me parmi toutes les variables disponibles. Par exemple, si pour estimer le prix d'une cr\u00eape on a acc\u00e8s \u00e0 la masse de farine, le volume de lait et l'\u00e2ge du cr\u00eapier, on va intuitivement choisir comme features la masse de farine et le volume de lait, car on devine que l'\u00e2ge du cr\u00eapier n'aura pas d'impact sur le prix. L'id\u00e9e est donc de s\u00e9lectionner comme features des variables qui soient corr\u00e9l\u00e9es aux labels . Pour ce faire, on utilise g\u00e9n\u00e9ralement des matrices de corr\u00e9lations vues pr\u00e9c\u00e9demment.","title":"La s\u00e9lection de variables"},{"location":"Chap1_Introduction/#lextraction-de-variables","text":"Multiplier le nombre de variables revient \u00e0 augmenter la dimensionnalit\u00e9 d'un probl\u00e8me, ce qui rend l'apprentissage plus compliqu\u00e9. De plus, certaines variables peuvent \u00eatre corr\u00e9l\u00e9es entre elles. C'est pourquoi on peut vouloir cr\u00e9er de nouvelles variables pour un probl\u00e8me, en combinant des variables parmi celles disponibles. Par exemple, plut\u00f4t que d'utiliser la masse de farine et le volume de lait pour pr\u00e9dire le prix d'une cr\u00eape, on peut imaginer utiliser un ratio volume de lait / masse de farine. L'id\u00e9e est donc de trouver des combinaisons de variables pertinentes pour pr\u00e9dire les labels. Pour ce faire, on peut se servir d'une m\u00e9thode de r\u00e9duction de dimensionnalit\u00e9 telle que l' ACP vue pr\u00e9c\u00e9demment.","title":"L'extraction de variables"},{"location":"Chap1_Introduction/#sur-apprentissage-sous-apprentissage","text":"Un des pires cauchemars de tout \"data scientist\" est le sur-apprentissage . Le but de tout apprentissage est d' obtenir un mod\u00e8le capable de g\u00e9n\u00e9raliser \u00e0 de nouvelles donn\u00e9es ce qu'il appris sur la base de donn\u00e9es d'entrainement. Le probl\u00e8me est que lors de l'apprentissage, on va chercher \u00e0 obtenir les meilleures performances possibles sur les donn\u00e9es d'entrainement, ce qui peut pousser le mod\u00e8le \u00e0 apprendre des d\u00e9tails tr\u00e8s sp\u00e9cifiques \u00e0 ces donn\u00e9es : le bruit et les \"outliers\" par exemple. D'o\u00f9 le terme de \"sur-apprentissage\". Un tel mod\u00e8le, trop compliqu\u00e9, aura alors de mauvaises performances sur de nouvelles donn\u00e9es : il sera incapable de g\u00e9n\u00e9raliser . Nous verrons dans la suite de ce chapitre la strat\u00e9gie classique pour d\u00e9tecter et \u00e9viter le sur-apprentissage. On peut n\u00e9anmoins r\u00e9duire les risques en : Ayant des donn\u00e9es nombreuses et repr\u00e9sentatives. Choisissant d'abord un type de mod\u00e8le simple, puis un plus complexe si besoin. Nettoyant les donn\u00e9es pour \u00e9viter le bruit et les \"outliers\". L'inverse du sur-apprentissage existe aussi, et se nomme \" sous-apprentissage \". Le mod\u00e8le est alors trop simple pour capturer la complexit\u00e9 des donn\u00e9es, et il tout aussi incapable de g\u00e9n\u00e9raliser. Le bon compromis que l'on va rechercher se situe donc entre les 2 : On appelle la recherche de ce compromis la \" r\u00e9gularisation \".","title":"Sur-apprentissage / sous-apprentissage"},{"location":"Chap1_Introduction/#strategie-pour-lapprentissage","text":"Maintenant que nous avons pass\u00e9 en revue les principales difficult\u00e9es auxquelles on doit faire face lors de l'apprentissage d'un mod\u00e8le, nous allons nous int\u00e9resser aux strat\u00e9gies d'apprentissage classiques.","title":"Strat\u00e9gie pour l'apprentissage"},{"location":"Chap1_Introduction/#hyperparametres","text":"Dans le contexte de l'apprentissage d'un mod\u00e8le, on distingue 2 types de param\u00e8tres : Les param\u00e8tres du mod\u00e8le que nous allons vouloir ajuster au cours de l'apprentissage, afin d'obtenir les labels d\u00e9sir\u00e9s \u00e0 partir de features donn\u00e9es. Les param\u00e8tres propres \u00e0 l' apprentissage , que l'on nomme \" hyperparam\u00e8tres \". Parmis les hyperparam\u00e8tres, on peut citer : la fonction de co\u00fbt utilis\u00e9e pour l'optimisation du mod\u00e8le, l'algorithme d'optimisation utilis\u00e9, la vitesse de convergence choisie, le nombre d'it\u00e9rations de l'entrainement, ou encore l'architecture du mod\u00e8le. Il est \u00e9vident que les hyperparam\u00e8tres vont impacter les performances du mod\u00e8le obtenu apr\u00e8s entrainement. C'est pourquoi lors d'un apprentissage, on va en g\u00e9n\u00e9ral r\u00e9aliser plusieurs entrainements , avec des hyperparam\u00e8tres diff\u00e9rents, afin d' optimiser aussi les hyperparam\u00e8tres .","title":"Hyperparam\u00e8tres"},{"location":"Chap1_Introduction/#fonction-de-cout","text":"La fonction de co\u00fbt (\"loss function\" en anglais), est une fonction qui prend en entr\u00e9e les labels pr\u00e9dits par le mod\u00e8le et les labels attendus , et retourne un score suivant la proximit\u00e9 de ces labels. On l'utilise comme crit\u00e8re de la qualit\u00e9 des labels pr\u00e9dits par un mod\u00e8le lors de son entrainement . En g\u00e9n\u00e9ral, plus la valeur retourn\u00e9e par la fonction de co\u00fbt est faible, et meilleure est la pr\u00e9diction. Le processus d'optimisation du mod\u00e8le va donc chercher \u00e0 minimiser la fonction de co\u00fbt . On choisit une fonction de co\u00fbt selon diff\u00e9rents crit\u00e8res : Qu'elle soit adapt\u00e9e au type de probl\u00e8me de classification / r\u00e9gression auquel on est confront\u00e9. Qu'elle soit adapt\u00e9e \u00e0 l'algorithme d'optimisation choisi : certains algorithmes n\u00e9cessitent par exemple une fonction continue ou diff\u00e9rentiable. Qu'elle soit adapt\u00e9e aux hypoth\u00e8ses que nous avons sur les donn\u00e9es : certaines fonctions donneront par exemple plus ou moins de poids aux \"outliers\". Dans les chapitres suivants, nous verrons quelques exemples de fonctions de co\u00fbt, adapt\u00e9es \u00e0 diff\u00e9rents types de probl\u00e8mes. Attention ! La fonction de co\u00fbt est utilis\u00e9e pour \u00e9valuer la qualit\u00e9 de la pr\u00e9diction d'un mod\u00e8le sur des donn\u00e9es lab\u00e9lis\u00e9e. Il ne s'agit pas d'une \u00e9valuation de la performance d'un mod\u00e8le en g\u00e9n\u00e9ralisation ! Cette notion, bien diff\u00e9rente, fera l'objet d'une section dans la suite de ce chapitre.","title":"Fonction de co\u00fbt"},{"location":"Chap1_Introduction/#algorithme-doptimisation","text":"On appelle \" algorithme d'optimisation \" un algorithme dont l'objectif est de trouver le minimum global de la fonction de co\u00fbt, afin d' optimiser les param\u00e8tres d'un mod\u00e8le. C'est lui qui r\u00e9alise l'entrainement proprement dit. Il s'agit en g\u00e9n\u00e9ral d'un algorithme it\u00e9ratif , qui va faire varier les param\u00e8tres du mod\u00e8le , afin de trouver ceux qui minimisent la fonction de co\u00fbt. On appelle souvent les it\u00e9rations de l'algorithme \u00e9poques (\"epochs\" en anglais). Voici une illustration pour un cas tr\u00e8s simplifi\u00e9 de mod\u00e8le \u00e0 1 param\u00e8tre : L'algorithme d'optimisation de base en apprentissage supervis\u00e9 est celui de la descente de gradient . Son principe est le suivant : Algorithme de descente de gradient Soit \\(f(x)\\) une fonction diff\u00e9rentiable dont on cherche le minimum. On initialise une valeur de \\(x\\) not\u00e9e \\(x_0\\) . A chaque it\u00e9ration \\(n\\) de l'algorithme, on va se d\u00e9placer dans l'espace des valeurs de \\(x\\) , de \\(x_n\\) \u00e0 \\(x_{n+1}\\) , avec la formule : \\(x_{n+1} = x_n - \\gamma \\nabla f(x_n)\\) avec \\(\\nabla f(x_n)\\) le gradient de la fonction en \\(x_n\\) , et \\(\\gamma\\) le \"taux d'apprentissage\". On peut mettre un crit\u00e8re d'arr\u00eat sur \\(\\| \\nabla f(x_n) \\|\\) . L'id\u00e9e est de parcourir l'espace des valeurs de la fonction de co\u00fbt en fonction des param\u00e8tres du mod\u00e8le, en suivant la direction inverse au gradient. On explique souvent le principe de cette m\u00e9thode par l'analogie des \"randonneurs dans le brouillard\" : Des randonneurs de montagne se retrouvent pi\u00e9g\u00e9s par le brouillard alors qu'il cherchent \u00e0 rentrer dans la vall\u00e9e. Comme ils ne peuvent plus voir le chemin, ils d\u00e9cident de se fier \u00e0 leurs pieds : ils se dirigent dans la direction de la pente la plus forte. Sans le savoir, les randonneurs sont en train d'appliquer l'algorithme de descente de gradient pour trouver le point minimisant l'altitude : la vall\u00e9e. Encore aujourd'hui, l'entrainement des mod\u00e8les d'apprentissage supervis\u00e9 se fait avec des algorithmes d'optimisation bas\u00e9s sur la descente de gradient.","title":"Algorithme d'optimisation"},{"location":"Chap1_Introduction/#vitesse-de-convergence-et-nombre-diterations","text":"Nous avons vu dans la section pr\u00e9c\u00e9dente que l'algorithme de la descente de gradient avait un param\u00e8tre appel\u00e9 taux d'apprentissage . Il est inversement proportionnel au pas avec lequel l'algorithme d'optimisation va se d\u00e9placer dans l'espace des valeurs de la fonction de co\u00fbt en fonction des param\u00e8tres du mod\u00e8le. Par cons\u00e9quent, le taux d'apprentissage permet de choisir la vitesse de convergence de l'algorithme d'optimisation . On s'attend \u00e0 ce que plus le taux d'apprentissage soit \u00e9lev\u00e9, plus la convergence vers le mod\u00e8le optimal soit rapide. Or, ce n'est pas toujours le cas. En effet, si l'algorithme se d\u00e9place trop rapidement dans l'espace des valeurs de la fonction de co\u00fbt, on a un risque de d\u00e9passement (\"overshooting\" en anglais) : l'algorithme peut passer \"par-dessus\" le minimum sans s'y arr\u00eater, voir m\u00eame diverger compl\u00e8tement. Le choix d'un taux d'apprentissage adapt\u00e9 est donc toujours affaire de compromis entre vitesse de convergence et non-d\u00e9passement. Nous avons \u00e9galement vu pr\u00e9c\u00e9demment que l'on peut placer dans l'algorithme un crit\u00e8re sur le gradient pour arr\u00eater les it\u00e9rations. Cependant, il est possible que ce crit\u00e8re ne soit jamais v\u00e9rifi\u00e9 si l'algorithme diverge. On va donc en g\u00e9n\u00e9ral choisir par s\u00e9curit\u00e9 un nombre d'\u00e9poques maximal comme crit\u00e8re d'arr\u00eat additionnel. Un autre probl\u00e8me courant est le fait que l'espace des valeurs de la fonction de co\u00fbt a souvent des minima locaux , dans lesquels l'algorithme peut se retrouver bloqu\u00e9 . C'est pourquoi en g\u00e9n\u00e9ral, on ne va pas lancer un seul entrainement pour un jeu d'hyperparam\u00e8tres, mais plusieurs avec des initialisations diff\u00e9rentes .","title":"Vitesse de convergence et nombre d'it\u00e9rations"},{"location":"Chap1_Introduction/#architecture-du-modele","text":"Certains types de mod\u00e8les peuvent avoir des architectures diff\u00e9rentes, plus ou moins complexes suivant le probl\u00e8me \u00e0 r\u00e9soudre. L'exemple le plus \u00e9vident sont les c\u00e9l\u00e8bres r\u00e9seaux de neurones , dont nous reparlerons dans les chapitres suivants. Dans le cas de ce type de mod\u00e8le, on peut jouer sur les hyperparm\u00e8tres suivants: Le nombre de couches de neurones. Le nombre de neurones par couche. La fonction d'activation de chaque neurone. Nous d\u00e9velopperons plus tard dans ce cours \u00e0 quoi correspondent ces diff\u00e9rents param\u00e8tres.","title":"Architecture du mod\u00e8le"},{"location":"Chap1_Introduction/#entrainement-validation-et-test","text":"Ce n'est pas parce que notre mod\u00e8le a converg\u00e9 vers le minimum global de l'espace de la fonction de co\u00fbt en fonction de ses param\u00e8tres qu'il aura de bonnes performances en g\u00e9n\u00e9ralisation !","title":"Entrainement, validation et test"},{"location":"Chap1_Introduction/#performances-en-generalisation","text":"En effet, nous avons vu plus t\u00f4t que le mod\u00e8le peut \u00eatre victime de sur-apprentissage : il a appris trop sp\u00e9cifiquement des donn\u00e9es d'entrainement, et n'arrive donc pas \u00e0 g\u00e9n\u00e9raliser \u00e0 de nouvelles donn\u00e9es. Intuitivement, on devine qu'il faudrait mettre de c\u00f4t\u00e9 une partie de notre base de donn\u00e9es d'entrainement, sur laquelle nous n'entrainerons pas notre mod\u00e8le. Ce sous-ensemble de donn\u00e9es servira \u00e0 tester le mod\u00e8le une fois l'entrainement termin\u00e9, pour v\u00e9rifier qu'il n'y a pas eu sur-apprentissage. C'est en effet la strat\u00e9gie classique pour l'apprentissage d'un mod\u00e8le. On dit alors que l'on a s\u00e9par\u00e9 la base de donn\u00e9e en un jeu d'entrainement et un jeu de test . On consid\u00e8rera alors que les performances obtenues par le mod\u00e8le sur le jeu de test seront ses performances en g\u00e9n\u00e9ralisation : il aura des performances \u00e9quivalentes sur tout nouveau jeu de donn\u00e9es. Le probl\u00e8me est alors le suivant : Plus on r\u00e9serve de donn\u00e9es pour les tests, et plus difficile sera l'apprentissage. Plus on r\u00e9serve de donn\u00e9es pour l'entrainement, et moins repr\u00e9sentatif sera le test. Souvent, le compromis retenu est 80% des donn\u00e9es pour l'entrainement et 20% des donn\u00e9es pour le test . Pour \u00e9valuer les performances en g\u00e9n\u00e9ralisation de notre mod\u00e8le, on va utiliser une s\u00e9lection de crit\u00e8res de performance , appel\u00e9s \"metrics\" en anglais. Pour chaque type de mod\u00e8le, il existe diff\u00e9rents crit\u00e8res de performances, qui vont chacun essayer de capturer un aspect diff\u00e9rent de la qualit\u00e9 de ses pr\u00e9dictions. Nous verrons des crit\u00e8res de performance classiques dans chacun des chapitres de ce cours.","title":"Performances en g\u00e9n\u00e9ralisation"},{"location":"Chap1_Introduction/#validation-par-exclusion","text":"Une fois que nous avons s\u00e9par\u00e9 notre base de donn\u00e9es en un jeu d'entrainement et un jeu de test, l'approche na\u00efve pour \u00e9viter le sur-apprentissage serait d'entrainer plusieurs mod\u00e8les avec diff\u00e9rents hyperparam\u00e8tres, et de retenir celui qui donne les meilleures performances en test. Cependant, si vous faites ceci, il y a peu de chances que votre mod\u00e8le ait de bonnes performances en g\u00e9n\u00e9ralisation . En effet, ce processus revient \u00e0 optimiser les hyperparam\u00e8tres sp\u00e9cifiquement pour notre jeu de donn\u00e9es de test . C'est pourquoi on va en r\u00e9alit\u00e9 diviser extraire un 3\u00e8me sous-ensemble de notre base de jeu d'entrainement : un jeu de validation . L' optimisation des hyperparam\u00e8tres se fera sur ce basant sur les performance en validation , et l'\u00e9valuation des performances en g\u00e9n\u00e9ralisation ce fera sur le jeu de test . Attention ! Les performances en test ne doivent jamais \u00eatre optimis\u00e9es, au risque de ne plus \u00eatre repr\u00e9sentatives des performances en g\u00e9n\u00e9ralisation du mod\u00e8le ! Le processus que nous avons d\u00e9crit ici est connu sous le nom de validation par exclusion . Le voici d\u00e9crit en d\u00e9tails : Validation par exclusion - On s\u00e9pare la base de donn\u00e9es en un jeu d'entrainement, un jeu de validation et un jeu de test. - On entraine plusieurs mod\u00e8les sur le jeu d'entrainement, avec diff\u00e9rents hyperparam\u00e8tres. - On s\u00e9lectionne le mod\u00e8le ayant les meilleurs r\u00e9sultats sur le jeu de validation. - On r\u00e9-entraine ce mod\u00e8le avec les m\u00eames hyperparam\u00e8tres, sur les donn\u00e9es d'entrainement et de validation. - On \u00e9value les performances en g\u00e9n\u00e9ralisation de ce mod\u00e8le sur les donn\u00e9es de test. Souvent, on retiendra 60% des donn\u00e9es pour l'entrainement , 20% des donn\u00e9es pour la validation et 20% des donn\u00e9es pour le test . Voici la validation par exclusion r\u00e9sum\u00e9e sch\u00e9matiquement : Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"model_selection\" une fonction \"train_test_split\" pour s\u00e9parer une base de donn\u00e9es en jeux d'entrainement et de test.","title":"Validation par exclusion"},{"location":"Chap1_Introduction/#validation-croisee","text":"La validation par exclusion a 2 grands d\u00e9fauts : En d\u00e9coupant en 3 jeux les donn\u00e9es disponibles, nous r\u00e9duisons la taille de la base de donn\u00e9es d'entrainement, et donc nous rendons l'apprentissage plus difficile. Les r\u00e9sultats peuvent d\u00e9pendre de la mani\u00e8re dont nous avons d\u00e9coup\u00e9 notre base de donn\u00e9es en 3. C'est pourquoi on lui pr\u00e9f\u00e8re souvent la validation crois\u00e9e . L'id\u00e9e est la suivante : on met toujours de c\u00f4t\u00e9 un jeu de test, mais on ne r\u00e9alise plus une s\u00e9paration entre donn\u00e9es d'entrainement et donn\u00e9es de validation. A la place, on s\u00e9pare la base d'entrainement en plusieurs petits jeux de donn\u00e9es, et on r\u00e9alise plusieurs entrainements du mod\u00e8le, avec pour chacun jeu un de validation diff\u00e9rent parmi ces petits jeux. Validation crois\u00e9e - On s\u00e9pare la base de donn\u00e9es en un jeu d'entrainement et un jeu de test. - On s\u00e9pare le jeu d'entrainement en k sous-jeux. - On r\u00e9alise k combinaisons, o\u00f9 un des k sous-jeux est utilis\u00e9 pour la validation, pendant que les k-1 autres sont utilis\u00e9es pour l'entrainement. - La qualit\u00e9 des pr\u00e9dictions du mod\u00e8le est \u00e9valu\u00e9e \u00e0 chaque it\u00e9ration comme \u00e9tant la moyenne de celles \u00e9valu\u00e9e pour chaque combinaison. - On s\u00e9lectionne le mod\u00e8le ayant en moyenne les meilleurs r\u00e9sultats en validation. - On r\u00e9-entraine ce mod\u00e8le avec les m\u00eames hyperparam\u00e8tres, sur le jeu d'entrainement complet. - On \u00e9value les performances en g\u00e9n\u00e9ralisation de ce mod\u00e8le sur les donn\u00e9es de test. Voici la validation crois\u00e9e r\u00e9sum\u00e9e sch\u00e9matiquement : Cette strat\u00e9gie d'apprentissage a tout de m\u00eame un d\u00e9savantage compar\u00e9e \u00e0 la validation par exclusion : elle n\u00e9cessite un temps de calcul beaucoup plus long ! Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package \"model_selection\" une fonction \"cross_validate\".","title":"Validation crois\u00e9e"},{"location":"Chap1_Introduction/#regularisation-par-arret-premature","text":"Nous avons vu que la strat\u00e9gie recommand\u00e9e pour \u00e9viter les probl\u00e8mes de g\u00e9n\u00e9ralisation est de comparer les performances du mod\u00e8le obtenues sur les donn\u00e9es d'entrainement \u00e0 celles obtenues sur les donn\u00e9es de validation. Pour \u00e9viter le sur-apprentissage , on peut en plus appliquer lors de l'entrainement la m\u00e9thode de r\u00e9gularisation connue sous le nom d' arr\u00eat pr\u00e9matur\u00e9 (\"early-stopping\" en anglais). Son principe se base sur les id\u00e9es suivantes : Puisque nous optimisons les param\u00e8tres du mod\u00e8le pour minimiser la fonction de co\u00fbt, nous nous attendons \u00e0 ce qu'au cours de l'entrainement, les valeurs de la fonction de co\u00fbt d\u00e9croissent avec les \u00e9poques pour les donn\u00e9es d'entrainement . A mesure que le mod\u00e8le est optimis\u00e9 sur les donn\u00e9es d'entrainement, nous nous attendons d'abord \u00e0 ce que dans un 1er temps , la fonction de co\u00fbt d\u00e9croisse aussi avec avec les \u00e9poques pour les donn\u00e9es de validation . Mais lorsque le mod\u00e8le se met \u00e0 souffrir de sur-apprentissage , nous nous attendons \u00e0 ce que la fonction de co\u00fbt se mette \u00e0 cro\u00eetre avec les \u00e9poques pour les donn\u00e9es de validation . On peut donc d\u00e9tecter le d\u00e9but du sur-apprentissage comme \u00e9tant l'\u00e9poque o\u00f9 la fonction de co\u00fbt se met \u00e0 croitre pour les donn\u00e9es de validation, et arr\u00eater l'apprentissage \u00e0 cette it\u00e9ration. D'o\u00f9 le nom d'\" arr\u00eat pr\u00e9matur\u00e9 \". Le processus exact est le suivant : Arr\u00eat pr\u00e9matur\u00e9 Pour chaque it\u00e9ration du processus d'apprentissage : - On \u00e9value la fonction de co\u00fbt pour les pr\u00e9dictions de la m\u00e9thode sur le jeu d'entrainement. - On \u00e9value la fonction de co\u00fbt pour les pr\u00e9dictions de la m\u00e9thode sur le jeu d'\u00e9valuation. - On v\u00e9rifie qu'on a bien une d\u00e9croissance pour les 2 valeurs, sinon on stoppe l'entrainement.","title":"R\u00e9gularisation par arr\u00eat pr\u00e9matur\u00e9"},{"location":"Chap1_Introduction/#import-de-donnees-et-fichiers-csv","text":"Pour enregistrer une base de donn\u00e9es sous la forme d'un tableau, on utilise couramment le format CSV : \"Comma Separated Values\". Il s'agit d'un format ouvert, compr\u00e9hensible par un humain, qui peut \u00eatre lu par n'importe quel \u00e9diteur de texte. Comme son nom l'indique, un fichier CSV s'organise de la mani\u00e8re suivante : Chaque ligne correspond \u00e0 une ligne du tableau . Les lignes sont s\u00e9par\u00e9es par un retour \u00e0 la ligne \"\\n\". Au sein d'une ligne, les \u00e9l\u00e9ments de chaque colonnes sont s\u00e9par\u00e9s par des virgules \",\". La premi\u00e8re ligne est souvent consid\u00e9r\u00e9e comme l' en-t\u00eate du tableau (le nom des colonnes). D'o\u00f9 le nom \"Comma Separated Values\". Voici un exemple de tableau de donn\u00e9es : Ville Latitude Longitude Nom Paris 48.866667 2.333333 Pain au chocolat Orl\u00e9ans 47.902734 1.908607 Pain au chocolat Bordeaux 44.841225 -0.580036 Chocolatine Brest 48.390528 -4.486009 Pain au chocolat Toulouse 43.604464 1.444243 Chocolatine Et voici tout simplement sa conversion en format CSV : Ville,Latitude,Longitude,Nom Paris,48.866667,2.333333,Pain au chocolat Orl\u00e9ans,47.902734,1.908607,Pain au chocolat Bordeaux,44.841225,-0.580036,Chocolatine Brest,48.390528,-4.486009,Pain au chocolat Toulouse,43.604464,1.444243,Chocolatine L'extension d'un fichier CSV est \" .csv \". Il peut \u00eatre import\u00e9 par la plupart des logiciels tableurs (Excel, OpenOffice Calc), et sous Python avec la biblioth\u00e8que Pandas, dont nous reparlerons plus loin. Astuce Python Avec la biblioth\u00e8que Python \"Pandas\", on peut importer un fichier CSV sous la forme d'un \"DataFrame\" (tableau de donn\u00e9es propre \u00e0 \"Pandas\") avec la m\u00e9thode \"read_csv\". Bien qu'il soit tr\u00e8s utilis\u00e9, le format CSV a quelques d\u00e9savantages. Notamment : Il n'est pas optimal en terme de m\u00e9moire. Il emp\u00eache d'utiliser le caract\u00e8re \",\" pour autre chose que s\u00e9parer les colonnes du tableau. C'est pourquoi, suivant les applications, on pr\u00e9f\u00e8rera utiliser un fichier binaire.","title":"Import de donn\u00e9es et fichiers CSV"},{"location":"Chap1_Introduction/#outils-python-pour-lapprentissage","text":"Pour chacun des programmes Python pr\u00e9sent\u00e9s dans ce cours, on partira du principe que les biblioth\u00e8ques Numpy et Matplotlib sont import\u00e9es .","title":"Outils Python pour l'apprentissage"},{"location":"Chap1_Introduction/#pandas","text":"Pandas est une biblioth\u00e8que Python tr\u00e8s populaire pour l' analyse de donn\u00e9es . Elle permet : D' importer des donn\u00e9es sous la forme d'un conteneur nomm\u00e9 \" DataFrame \". De manipuler ces donn\u00e9es de mani\u00e8re efficace. D'afficher facilement des graphiques \u00e0 partir de ces donn\u00e9es. D' exporter ces donn\u00e9es en diff\u00e9rents formats. De fournir une entr\u00e9e pour l' apprentissage de mod\u00e8les. Elle s'importe souvent sous le nom \"pd\" avec la commande : import pandas as pd Un DataFrame Pandas contient un jeu de donn\u00e9es sous la forme d'un tableau 2D , pouvant contenir des objets de types diff\u00e9rents. On associe \u00e0 chaque donn\u00e9e dans un DataFrame une \" column \" correspondant au nom de la variable, et un \" index \" correspondant au num\u00e9ro de l'individu. Voici notre exemple de tableau pr\u00e9c\u00e9dent, converti en DataFrame : 'Ville' 'Latitude' 'Longitude' 'Nom' 0 'Paris' 48.866667 2.333333 'Pain au chocolat' 1 'Orl\u00e9ans' 47.902734 1.908607 'Pain au chocolat' 2 'Bordeaux' 44.841225 -0.580036 'Chocolatine' 3 'Brest' 48.390528 -4.486009 'Pain au chocolat' 4 'Toulouse' 43.604464 1.444243 'Chocolatine' On peut acc\u00e9der \u00e0 une ligne d'indice i d'un DataFrame df avec la commande : df.loc[i] Et on peut acc\u00e9der \u00e0 une colonne 'col' d'un DataFrame df avec la commande : df['col'] On peut \u00e9galement s\u00e9lectionner un sous-ensemble de donn\u00e9es \u00e0 partir d'un DataFrame, de mani\u00e8re conditionnelle. Par exemple, pour cr\u00e9er un DataFrame df2 \u00e0 partir de notre DataFrame pr\u00e9c\u00e9dent que nous nommerons df , mais ne contenant que les individus pour lesquels la variable 'Latitude' est inf\u00e9rieure \u00e0 48.5, et la variable 'Nom' est \u00e9gale \u00e0 'Pain au chocolat', on utilisera la commande : df2 = df[(df['Latitude']<48.5)&(df['Nom']=='Pain au chocolat')] On obtient alors le DataFrame df2 suivant : 'Ville' 'Latitude' 'Longitude' 'Nom' 1 'Orl\u00e9ans' 47.902734 1.908607 'Pain au chocolat' 3 'Brest' 48.390528 -4.486009 'Pain au chocolat' On remarque que pour l'op\u00e9rateur bool\u00e9en ET on doit utiliser \"&\", et pour l'op\u00e9rateur bool\u00e9en OU on doit utiliser \"|\". Nous utiliserons la biblioth\u00e8que Pandas afin d'importer, d'analyser et de manipuler des donn\u00e9es dans le cadre de ce cours .","title":"Pandas"},{"location":"Chap1_Introduction/#scipy","text":"Bien que Scipy ne soit pas \u00e0 proprement parler une biblioth\u00e8que d'apprentissage automatique, on trouve dans son module \"stats\" des outils d'inf\u00e9rence statistique de base pour de la classification et de la r\u00e9gression. On peut importer ce module avec la commande Python suivante : import scipy.stats Dans le cadre de ce cours, nous utiliserons Scipy pour certaines m\u00e9thodes de base , car ses impl\u00e9mentations sont moins des \"boites noires\" que pour d'autres biblioth\u00e8ques.","title":"Scipy"},{"location":"Chap1_Introduction/#scikit-learn","text":"Scikit-Learn est une biblioth\u00e8que Python pour l' apprentissage automatique . Elle contient des impl\u00e9mentations de m\u00e9thodes pour : La classification supervis\u00e9e . La r\u00e9gression . Le partitionnement . La r\u00e9duction de dimensionnalit\u00e9. On peut importer cette biblioth\u00e8que Python avec la commande : import sklearn M\u00eame si en g\u00e9n\u00e9ral on n'importe pas toute la biblioth\u00e8que, mais seulement les packages dont on a besoin. Nous utiliserons la biblioth\u00e8que Scikit-Learn pour de l'apprentissage de mod\u00e8les dans le cadre de ce cours .","title":"Scikit-Learn"},{"location":"Chap1_Introduction/#seaborn","text":"Seaborn est une biblioth\u00e8que Python d' affichage graphique de donn\u00e9es statistiques. Ses outils d'affichage peuvent s'av\u00e9rer utiles en compl\u00e9ment de ceux de Pandas et Matplotlib. Elle s'importe souvent sous le nom \"sns\" avec la commande : import seaborn as sns Nous utiliserons quelques outils d'affichage graphique de Seaborn dans le cadre de ce cours .","title":"Seaborn"},{"location":"Chap1_Introduction/#keras-tensorflow-pytorch","text":"Pour aller plus loin, lorsque le mod\u00e8le est un r\u00e9seau de neurones, et que ce r\u00e9seau contient de nombreuses couches de neurones, on entre dans un sous-domaine de l'apprentissage appel\u00e9 apprentissage profond (\"deep learning\" en anglais). (Nous reparlerons des r\u00e9seaux de neurones dans les chapitres suivants). Pour l'apprentissage profond, il existe 2 grandes biblioth\u00e8ques concurrentes sur Python : Keras-Tensorflow et Pytorch . L'apprentissage profond ne sera pas au programme de ce cours, qui n'est qu'une introduction aux sciences des donn\u00e9es .","title":"Keras-Tensorflow, Pytorch"},{"location":"Chap1_Introduction/#conclusion","text":"Ce chapitre vous a donn\u00e9 une introduction tr\u00e8s g\u00e9n\u00e9rale aux sciences des donn\u00e9es . La suite de ce cours sera d\u00e9coup\u00e9e en 3 chapitres, portant chacun sur un type d'apprentissage automatique : La classification supervis\u00e9e . La r\u00e9gression . Le partitionnement (\"clustering\" ou \"classification non-supervis\u00e9e\"). Chacun introduira de mani\u00e8re g\u00e9n\u00e9rale son type d'apprentissage, pr\u00e9sentera un panel des mod\u00e8les de base, et donnera des outils d' \u00e9valuation des performances . Un exemple \"fil rouge\" d'application sera utilis\u00e9 pour illustrer chaque chapitre.","title":"Conclusion"},{"location":"Chap2_Classification_supervisee/","text":"Chapitre II : Classification supervis\u00e9e Ce chapitre est une introduction \u00e0 la classification supervis\u00e9e : principe, mesures de performances et m\u00e9thodes de base. Probl\u00e8me de classification supervis\u00e9e Comme mentionn\u00e9 lors du Chapitre I, par \" classifier \" on entend associer une r\u00e9alisation d'une variable quantitative discr\u00e8te ou qualitative \u00e0 un individu (labels), \u00e0 partir des r\u00e9alisations d'autres variables (features). On appelle ces labels des \" classes \". On parlera ici de \"classification supervis\u00e9e\" car on va entrainer un mod\u00e8le (aussi appel\u00e9 \"classifieur\") \u00e0 associer une classe \u00e0 des individus, en se basant sur des donn\u00e9es d\u00e9j\u00e0 lab\u00e9lis\u00e9es. Il s'agit donc bien d'un apprentissage supervis\u00e9 . L'id\u00e9e est que le classifieur soit ensuite capable de g\u00e9n\u00e9raliser : pr\u00e9dire la \"classe\" d'un nouvel individu. Les diff\u00e9rents types de classification Plut\u00f4t que de parler de \"la\" classification, on devrait par \"des\" classifications, car il existe plusieurs types de probl\u00e8mes de classification. Nous allons donc commencer par parler des diff\u00e9rents types de classification, en illustrant avec un exemple : reconnaitre sur une photo un instrument de musique breton. Binaire Le type de classification le plus basique, et pour lequel tous les mod\u00e8les de classification peuvent \u00eatre entrain\u00e9s, est la classification binaire . Comme son nom l'indique, l'id\u00e9e est simplement de r\u00e9soudre un probl\u00e8me o\u00f9 l'on veut s\u00e9parer les individus en 2 classes . Il peut s'agir de pr\u00e9dire l'appartenance \u00e0 2 classes exclusives dans un cas o\u00f9 il n'y a que 2 labels possibles, par exemple : \"L'instrument sur la photo est-il une bombarde ou un biniou ?\". Ou alors il peut s'agir de pr\u00e9dire l'appartenance ou la non appartenance \u00e0 une classe parmi d'autres, par exemple : \"L'instrument sur la photo est-il une bombarde ou un autre instrument ?\". Beaucoup des m\u00e9thodes et des crit\u00e8res de performances qui sont pr\u00e9sent\u00e9es dans ce cours ont d'abord \u00e9t\u00e9 d\u00e9finis pour des probl\u00e8mes binaires, avant d'\u00eatre g\u00e9n\u00e9ralis\u00e9s. Nota Bene : En g\u00e9n\u00e9ral, un classifieur binaire ne retourne pas directement une pr\u00e9diction de la classe de l'individu, mais une probabilit\u00e9 d'appartenance \u00e0 la classe : un score entre 0 et 1. Il faut alors placer un seuil sur cette probabilit\u00e9 pour choisir si l'individu appartient \u00e0 la classe ou non (souvent 0.5 par d\u00e9faut). On appelle ce seuil fronti\u00e8re de d\u00e9cision . Les impl\u00e9mentations Scikit-Learn des m\u00e9thodes de classification peuvent souvent retourner soit directement la classe pr\u00e9dite, soit la probabilit\u00e9 d'appartenance \u00e0 la classe. Multi-classe Si on veut classer des individus dans plus de 2 classes , on va parler de classification multi-classe . Par exemple, \"L'instrument sur la photo est-il une bombarde ou un biniou\" est un probl\u00e8me de classification binaire, alors que \"L'instrument sur la photo est-il une bombarde, un biniou ou un tambour ?\" est un probl\u00e8me de classification multi-classe. Or, si toutes les m\u00e9thodes sont capables de r\u00e9aliser une classification binaires, toutes ne sont pas capables de r\u00e9aliser une classification multi-classe. Pour contourner ce probl\u00e8me, on va ramener ce probl\u00e8me \u00e0 de multiples classifications binaires , avec une strat\u00e9gie pour choisir la pr\u00e9diction \u00e0 retourner : One-versus-All : on entraine un classifieur binaire par classe, et la classe pr\u00e9dite pour un individu donn\u00e9 sera celle dont le classifieur aura retourn\u00e9 la probabilit\u00e9 la plus \u00e9lev\u00e9e. One-versus-One : on entraine un classifieur pour chaque couple de classes possible, et la classe pr\u00e9dite est celle qui aura gagn\u00e9 le plus de \"duels\" parmi les sorties des diff\u00e9rents classifieurs. Pour \\(N\\) classes, la strat\u00e9gie \"One-versus-One\" implique d'entrainer \\(N(N-1)/2\\) classifieurs, l\u00e0 o\u00f9 la strat\u00e9gie \"One-versus-All\" n'a besoin d'en entrainer que \\(N\\) . Mais chaque mod\u00e8le est entrain\u00e9 sur un plus petit jeu de donn\u00e9es pour la m\u00e9thode \"one-versus-one\" que pour la m\u00e9thode \"one-versus-all\" Le choix de strat\u00e9gie d\u00e9pendra donc de l'application . Les m\u00e9thodes disponibles sous Scikit-Learn choisissent une strat\u00e9gie par d\u00e9faut, mais il est possible de la modifier. Multi-\u00e9tiquettes Dans les types de classification pr\u00e9c\u00e9dents, on ne pouvait associer qu'une seule classe \u00e0 un individu. Cependant, pour certains probl\u00e8mes il est possible qu' un individu puisse faire partie de plusieurs classes \u00e0 la fois . Par exemple, si le probl\u00e8me est \"Quel instrument est sur cette photo ?\", et que la photo contient une bombarde et un biniou, alors le morceau appartient \u00e0 la fois \u00e0 la classe \"bombarde\" et \u00e0 la classe \"biniou\". Certaines m\u00e9thodes impl\u00e9ment\u00e9es dans Scikit-Learn acc\u00e8ptent une matrices de labels en entrainement au lieu d'un vecteur, et d'autres non. Il faut donc v\u00e9rifier si la m\u00e9thode que vous voulez utiliser supporte bien la classification multi-\u00e9tiquettes. Si un classifieur est multi-\u00e9tiquettes, et que chaque \u00e9tiquette est multi-classe, on dira le classifieur \" multi-sorties \". Exemple de probl\u00e8me Pourquoi est-on capables de reconnaitre le son d'un instrument de musique d'un autre ? Lorsqu'un instrument joue une note, le son \u00e9mit ne contient jamais qu'une seule fr\u00e9quence. Il est en r\u00e9alit\u00e9 constitu\u00e9 d'une \"fr\u00e9quence fondamentale\" (la note que l'on veut jouer), et des \"harmoniques\" (des fr\u00e9quences multiples de la fondamentale). Pour une m\u00eame note jou\u00e9e, suivant l'instrument, les harmoniques n'auront pas la m\u00eame amplitude compar\u00e9e \u00e0 la fondamentale. C'est ce que l'on appelle le \"timbre\" de l'instrument. Lorsque nous \u00e9coutons de la musique, et que nous reconnaissons le son d'un instrument, c'est gr\u00e2ce \u00e0 son timbre. Voici 3 exemples de spectres issus d'enregistrements d'une flute, d'un hautbois et d'une trompette jouant un La (440 Hz) : On voit nettement la diff\u00e9rence de timbre entre les 3 instruments. D'o\u00f9 l'id\u00e9e suivante : peut-on entrainer un mod\u00e8le \u00e0 reconnaitre un instrument \u00e0 partir d'un enregistrement ? Voici un jeu de donn\u00e9es au format CSV, collect\u00e9es \u00e0 partir de milliers d'enregistrements d'une flute, d'un hautbois et d'une trompette jouant un La (440 Hz) : Chap2_instruments_dataset Le tableau de donn\u00e9es qu'il contient est de la forme suivante : instrument harmo1 harmo2 harmo3 oboe 11.842 11.58 10.28 flute -17.083 -17.384 -21.496 trumpet -8.152 -24.089 -23.813 oboe 9.381 12.434 11.905 oboe -1.217 2.082 16.275 trumpet -3.294 -13.812 -17.934 trumpet -4.118 -13.485 -18.985 ... ... ... ... trumpet -7.762 -5.934 -23.308 flute -17.96 -19.406 -22.409 oboe 7.764 6.618 13.361 Il contient pour chacun des 5612 enregistrements le nom de l'instrument, et l'amplitude en dB des 3 premi\u00e8res harmoniques relativement \u00e0 la fondamentale. Notre probl\u00e8me de classification sera le suivant : pr\u00e9dire l'instrument ayant jou\u00e9 un La \u00e0 partir des amplitudes des 3 premi\u00e8res harmoniques . Voyons d'abord si une telle classification est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) Il est possible avec Seaborn d'afficher ces donn\u00e9es sous la forme d'une matrice de nuages de points , avec chaque classe d'une couleur diff\u00e9rente. Ce type de repr\u00e9sentation permet de v\u00e9rifier la s\u00e9parabilit\u00e9 des diff\u00e9rentes classes \u00e0 partir des features s\u00e9lectionn\u00e9s. Voici la commande Seaborn : import seaborn as sns sns.pairplot(df_dataset,hue='instrument') On obtient alors le graphique suivant : On observe que les classes \"flute\", \"oboe\" et \"trumpet\" sont plut\u00f4t bien s\u00e9parables \u00e0 partir des amplitudes des 3 premi\u00e8res harmoniques. Vouloir entrainer un mod\u00e8le \u00e0 reconnaitre un de ces instruments \u00e0 partir de ces donn\u00e9es \u00e0 donc du sens. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP. Mesures de performance Nous allons passer en revue dans cette section les principaux indicateurs de performances applicables \u00e0 tous les types de classification. Matrice de confusion Pour chaque classe \\(C\\) possible, lorsqu'un classifieur r\u00e9alise une pr\u00e9diction sur un individu, il y a 4 possibilit\u00e9s : Le classifieur a pr\u00e9dit \\(C\\) , et l'individu appartient bien \u00e0 \\(C\\) : c'est un vrai positif (not\u00e9 TP). Le classifieur a pr\u00e9dit \\(C\\) , et l'individu n'appartient pas \u00e0 \\(C\\) : c'est un faux positif (not\u00e9 FP). Le classifieur n'a pas pr\u00e9dit \\(C\\) , et l'individu n'appartient pas \u00e0 \\(C\\) : c'est un vrai n\u00e9gatif (not\u00e9 TN). Le classifieur n'a pas pr\u00e9dit \\(C\\) , et l'individu appartient bien \u00e0 \\(C\\) : c'est un faux n\u00e9gatif (not\u00e9 FN). Tous les scores de performance pour la classification que nous allons voir se basent sur le nombre de TP, FP, TN et FN obtenus par le mod\u00e8le sur un jeu d'individus lab\u00e9lis\u00e9. Les indicateurs brutes que sont le nombre de TP, FP, TN et FN sont en g\u00e9n\u00e9ral mis sous la forme d'un tableau, que l'on appelle matrice de confusion . Voici \u00e0 quoi ressemble ce tableau pour une seule classe d'un probl\u00e8me multi-classe, ou pour un probl\u00e8me de classification binaire : On peut \u00e9galement repr\u00e9senter les r\u00e9sultats d'une classification multi-classe pour toutes les classes sous la forme d'une matrice de confusion. Voici un exemple pour 5 classes \\(C_1\\) , \\(C_2\\) , \\(C_3\\) , \\(C_4\\) et \\(C_5\\) : On peut alors lire ce tableau d'un point de vue g\u00e9n\u00e9ral : la diagonale correspond aux vrais positifs \u00e0 maximiser. Mais on peut aussi le lire du point de vue d'une classe ( \\(C_3\\) dans notre illustration), et calculer les nombres de TP, FP, TN et FN correspondant. La matrice de confusion est la repr\u00e9sentation la plus exhaustive possible des performances d'un classifieur, mais elle est d'autant plus difficile \u00e0 lire que le nombre de classes est grand . Ceci peut rendre complexe la comparaison entre 2 classifieurs. Pour cette raison, on va souvent utilis\u00e9 des scores d\u00e9riv\u00e9s du tableau de confusion. Exactitude Le score d' exactitude (\"accuracy\" en anglais) est le plus classique pour \u00e9valuer les performances d'un classifieur. D\u00e9finition L' exactitude : est la taux d'individus class\u00e9s correctement parmi tous les individus class\u00e9s. Dans le cas d'une classification binaire, il s'agit donc de : \\(\\frac{TP+TN}{TP+FP+TN+FN}\\) Dans le cas d'une classification multi-classe, il s'agira de la trace de la matrice de confusion divis\u00e9e par le nombre total d'individus class\u00e9s. Si cet indicateur est intuitif et permet de condenser l'information en un score unique, il aura tendance \u00e0 \u00eatre biais\u00e9 s'il y a un fort d\u00e9s\u00e9quilibre entre classes. En effet, comme on somme TP et TN, l'exactitude aura tendance \u00e0 favoriser la classe majoritaire . C'est pourquoi dans un cas d\u00e9s\u00e9quilibr\u00e9, on pr\u00e9f\u00e8rera utiliser un duo de scores de performances : pr\u00e9cision-rappel ou rappel-fausse alarme. Pr\u00e9cision-rappel et score F1 Si la classe consid\u00e9r\u00e9e est majoritaire , ou si pour notre application nous pr\u00e9f\u00e9rons r\u00e9duire les faux positifs quitte \u00e0 augmenter les faux n\u00e9gatifs, on utilisera plut\u00f4t les indicateurs de pr\u00e9cision et de rappel . D\u00e9finitions - La pr\u00e9cision : est le taux d'individus attribu\u00e9s correctement \u00e0 une classe parmi toutes les pr\u00e9dictions de cette classe. - Le rappel (\"sensibilit\u00e9\" ou \"recall\" en anglais) : est le taux d'individus attribu\u00e9s correctement \u00e0 une classe tous les individus appartenant r\u00e9ellement \u00e0 cette classe. Dans un cas binaire, il s'agit donc de : Pr\u00e9cision : \\(\\frac{TP}{TP+FP}\\) Rappel : \\(\\frac{TP}{TP+FN}\\) Ces 2 scores sont antagonistes : on doit donc choisir un compromis entre les 2 suivant notre application. Si on veut obtenir un compromis donnant une pr\u00e9cision et un rappel similaires, on peut utiliser la moyenne harmonique de ces 2 scores : \\(F_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{rappel}}\\) C'est ce que l'on appelle le score F1 . Mais si on veut trouver un compromis donnant une pr\u00e9cision et un rappel en particulier, il faut utiliser une courbe pr\u00e9cision-rappel . L'id\u00e9e est de faire varier le seuil de d\u00e9cision pour chaque classe, et d'afficher les compromis entre pr\u00e9cision et rappel obtenus pour chaque seuil : La ligne diagonale correspond \u00e0 la performance th\u00e9orique d'un classifieur al\u00e9atoire. Courbe ROC Si la classe consid\u00e9r\u00e9e est minoritaire , ou si pour notre application nous pr\u00e9f\u00e9rons r\u00e9duire les faux n\u00e9gatifs quitte \u00e0 augmenter les faux positifs, on utilisera plut\u00f4t les indicateurs de rappel et de fausse alarme . D\u00e9finitions - Le taux de fausse alarme : est taux d'individus attribu\u00e9s incorrectement \u00e0 une classe parmi tous les individus n'appartenant pas \u00e0 cette classe. Pour des raisons historiques, on appelle souvent dans ce contexte le rappel \" taux de vrais positifs \" (TPR) et le taux de fausse alarme \" taux de faux positifs \" (FPR). Dans un cas binaire, il s'agit donc de : TPR : \\(\\frac{TP}{TP+FN}\\) FPR : \\(\\frac{FP}{FP+TN}\\) Ces 2 scores sont \u00e9galement antagonistes : on doit donc aussi choisir un compromis entre les 2 suivant notre application. Une fois encore, pour trouver un compromis donnant un TPR et un FPR en particulier, on peut tracer les compromis obtenus pour diff\u00e9rents seuils de d\u00e9cision. On appelle ce type de courbe \"Reicever Operating Caracteristic\" (ROC) : Le nom \u00e9trange de cette courbe a une origine historique : elle aurait \u00e9t\u00e9 invent\u00e9e durant la 2nde guerre mondiale, dans le cadre de la classification binaire de signaux radar entre \"avion ennemi\" et \"bruit\". M\u00e9thodes de base D\u00e9cision Bayesienne La d\u00e9cision Bayesienne , aussi connue sous le nom de \"classification Bayesienne na\u00efve\" est une m\u00e9thode de classification se basant sur un mod\u00e8le probabiliste des features, consid\u00e9r\u00e9es ind\u00e9pendantes , et du th\u00e9or\u00e8me de Bayes . Principe Imaginons que nous avons un probl\u00e8me de classification avec \\(q\\) classes \\(C_1\\) , \\(C_2\\) , ..., \\(C_q\\) . Nous voulons pr\u00e9dire la classe \u00e0 laquelle appartient un individu. La probabilit\u00e9 de chaque classe \\(i\\) not\u00e9e \\(p(C_i)\\) , aussi appel\u00e9e \" probabilit\u00e9 a priori \". On a \\(\\sum_{i=1}^{q} p(C_i) = 1\\) et on peut facilement estimer les diff\u00e9rents \\(p(C_i)\\) \u00e0 partir du nombre d'occurences de \\(C_i\\) dans les donn\u00e9es divis\u00e9e par la taille de la base de donn\u00e9es. En ne connaissant que les probabilit\u00e9s a priori de chaque classe, nous serions oblig\u00e9s de classer n'importe quel individu comme appartenant \u00e0 la classe \\(C_i\\) ayant le \\(p(C_i)\\) le plus \u00e9lev\u00e9. Nous aurions alors un classifieur retournant toujours la m\u00eame classe. Pas tr\u00e8s utile... Or, nous avons en r\u00e9alit\u00e9 acc\u00e8s \u00e0 plus d'informations : nos fameuses \"features\", dont nous voulons nous servir pour pr\u00e9dire la classe d'un individu. Mettons que nous avons acc\u00e8s \u00e0 une feature d'int\u00e9r\u00eat pour cette classification. On notera \\(X\\) l'espace des observations associ\u00e9. Pour d\u00e9terminer la classe d'un individu, on peut alors partir du principe suivant : choisir le \\(C_i\\) tel que \\(p(C_i \\mid x)\\) soit maximal . Nous expliquerons pourquoi dans la suite. On nomme \\(p(C_i \\mid x)\\) \" probabilit\u00e9s a posteriori \". D'apr\u00e8s le th\u00e9or\u00e8me de Bayes : \\(p(C_i \\mid x) = \\frac{p(x \\mid C_i)p(C_i)}{p(x)}\\) avec \\(p(x) = \\sum_{i=1}^{q} p(x \\mid C_i)p(C_i)\\) On nomme \\(p(x)\\) la densit\u00e9 de \" probabilit\u00e9 d'observation \", et \\(p(x \\mid C_i)\\) la densit\u00e9 de \" probabilit\u00e9 conditionnelle d'observation \". Toute la difficult\u00e9 de la m\u00e9thode est d' estimer \\(p(x \\mid C_i)\\) . On va en g\u00e9n\u00e9ral chercher \u00e0 mod\u00e9liser ces densit\u00e9s de probabilit\u00e9 conditionnelle. NB : Il est \u00e0 noter que rechercher la classe \\(C_i\\) maximisant \\(p(C_i \\mid x)\\) est \u00e9quivalent \u00e0 rechercher \\(C_i\\) maximisant \\(p(x \\mid C_i)p(C_i)\\) . Il n'est donc en th\u00e9orie pas utile de calculer \\(p(x)\\) pour obtenir le classifieur. Mais il est n\u00e9cessaire d'avoir \\(p(x)\\) pour obtenir des probabilit\u00e9s d'appartenance \u00e0 une classe. Attention ! En g\u00e9n\u00e9ral, il y a des recouvrements entre les diff\u00e9rentes densit\u00e9s de probabilit\u00e9 conditionnelle. On ne peut alors pas obtenir classifieur parfait. On cherchera juste le mod\u00e8le permettant de minimiser les erreurs de classification. Cas particulier : Si tous les \\(p(x \\mid C_i)\\) sont \u00e9gaux, alors la feature s\u00e9lectionn\u00e9e n'est pas pertinente pour la classification. Ce principe est g\u00e9n\u00e9ralisable aux cas de classifications avec \\(m\\) features d'espaces de probabilit\u00e9 \\(X_1\\) , \\(X_2\\) , ... \\(X_m\\) . On cherchera la classe \\(C_i\\) qui maximise \\(p(C_i) \\prod_{j=1}^{m}p(x_j \\mid C_i)\\) . Fronti\u00e8re de d\u00e9cision et erreur Comme nous l'avons expliqu\u00e9 pr\u00e9c\u00e9demment, sauf cas particulier, on ne peut pas obtenir un classifieur parfait. On va donc essayer d'\u00e9tablir des fronti\u00e8res de d\u00e9cision entre les classes : des intervalles de \\(x\\) pour lesquels on attribura une classe. Et nous recherchons m\u00eame les fronti\u00e8res de d\u00e9cision optimales : celles qui minimisent le risque d'erreurs. Prenons un cas simple de classification binaire entre 2 classes \\(C_1\\) et \\(C_2\\) . Nous noterons \\(x = D\\) la fronti\u00e8re de d\u00e9cision choisie. On a alors 2 types d'erreurs de classification possibles : Classifier l'individu en \\(C_1\\) alors qu'il appartient \u00e0 \\(C_2\\) . Classifier l'individu en \\(C_2\\) alors qu'il appartient \u00e0 \\(C_1\\) . Les probabilit\u00e9s d'erreurs associ\u00e9es sont \\(\\int_{-\\infty}^{D} p(x \\mid C_2)p(C_2) dx\\) et \\(\\int_{D}^{+\\infty} p(x \\mid C_1)p(C_1) dx\\) . Elles correspondent aux aires repr\u00e9sent\u00e9es sur ce sch\u00e9ma : Pour obtenir la fronti\u00e8re de d\u00e9cision optimale \\(x = O\\) , on va chercher \u00e0 minimiser la somme de ces erreurs. L'aire entour\u00e9e en vert correspond \u00e0 ce que l'on appelle \"l' erreur r\u00e9ductible \" : c'est la portion de l'erreur totale que l'on peut r\u00e9duire pour obtenir la fronti\u00e8re optimale. On retrouve bien que : Si \\(p(C_1 \\mid x) > p(C_2 \\mid x)\\) alors on classifie l'individu comme appartenant \u00e0 \\(C_1\\) . Si \\(p(C_1 \\mid x) < p(C_2 \\mid x)\\) alors on classifie l'individu comme appartenant \u00e0 \\(C_2\\) . On peut g\u00e9n\u00e9raliser \u00e0 \\(q\\) classes : comme dit pr\u00e9c\u00e9demment, pour obtenir les fronti\u00e8res de d\u00e9cision optimales, on choisi le \\(C_i\\) tel que \\(p(C_i \\mid x)\\) soit maximal . Choix du mod\u00e8le Comme nous l'avons expliqu\u00e9, la d\u00e9cision Bayesienne n\u00e9cessite un mod\u00e8le des probabilit\u00e9s conditionnelles d'observation \\(p(x \\mid C_i)\\) pour chaque classe \\(C_i\\) . Pour ce faire, on ajuste une fonction de densit\u00e9 de probabilit\u00e9 pour chaque \\(p(C_i \\mid x)\\) \u00e0 notre jeu d'entrainement. Ceci implique donc 2 choix : Une fonction de densit\u00e9 de probabilit\u00e9 , ce qui implique de faire une hypoth\u00e8se forte sur la distribution des observations pour chaque classe. Une m\u00e9thode d'ajustement de loi de probabilit\u00e9 . La fonction de densit\u00e9 de probabilit\u00e9 la plus classique est celle de la loi normale : \\(f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\) avec 2 param\u00e8tres \u00e0 ajuster \\(\\mu\\) (la moyenne) et \\(\\sigma\\) (l'\u00e9cart-type). La m\u00e9thode d'ajustement la plus classique pour la d\u00e9cision Bayesienne est celle du maximum de vraisemblance . C'est elle que nous allons d\u00e9tailler. Maximum de vraisemblance D\u00e9finition Soit une loi de probabilit\u00e9 \\(f(x,\\theta)\\) , d\u00e9finie par des param\u00e8tres \\(\\theta\\) . Pour un \u00e9chantillon observ\u00e9 \\((x_1,x_2,...,x_n)\\) , on nomme vraisemblance (\"likelihood\" en anglais) la probabilit\u00e9 que cet \u00e9chantillon provienne d'un tirage de \\(f(x,\\theta)\\) . Si les tirages sont ind\u00e9pendants, on peut exprimer la vraisemblance de la mani\u00e8re suivantes : \\(L(x_1,x_2,...,x_n,\\theta) = \\prod_{k=1}^{n} f(x_k,\\theta)\\) La m\u00e9thode du maximum de vraisemblance d\u00e9coule du fait que le mod\u00e8le \\(f\\) de param\u00e8tres \\(\\theta\\) repr\u00e9sentant le mieux les observations est celui qui maximise la vraisemblance, c'est-\u00e0-dire la probabilit\u00e9 que l'\u00e9chantillon provienne de cette loi . L'id\u00e9e est donc de rechercher les \\(\\theta\\) maximisant \\(L(x_1,x_2,...,x_n,\\theta)\\) . Souvent, pour simplifier les calculs, on ne va pas rechercher le maximum de la vraisemblance, mais de la log-vraisemblance : \\(log(L(x_1,x_2,...,x_n,\\theta)) = \\sum_{k=1}^{n} log(f(x_k,\\theta))\\) En effet, rechercher les param\u00e8tres \\(\\theta\\) maximisant \\(L\\) ou \\(logL\\) est \u00e9quivalent, et rechercher un maximum implique un calcul de d\u00e9riv\u00e9e, ce qui est plus simple pour des sommes que pour des produits. On va donc pour chaque param\u00e8tre \\(\\theta_j\\) de \\(\\theta\\) la valeur qui v\u00e9rifie \\(\\frac{\\partial}{\\partial{\\theta_j}} \\sum_{k=1}^{n} log(f(x_k,\\theta)) = 0\\) . Prenons l'exemple de la loi normale : \\(f(x,\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\) On cherchera alors les param\u00e8tres \\(\\theta = (\\mu,\\sigma)\\) v\u00e9rifiant : \\(\\frac{\\partial}{\\partial{\\theta}} \\sum_{k=1}^{n} (-log(\\sigma) - log(\\sqrt{2 \\pi}) - \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) soit \\(\\frac{\\partial}{\\partial{\\theta}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) soit pour chaque param\u00e8tre : \\(\\frac{\\partial}{\\partial{\\mu}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) \\(\\frac{\\partial}{\\partial{\\sigma}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) On montre alors que les param\u00e8tres v\u00e9rifiant ces \u00e9quations sont : \\(\\mu = \\frac{1}{n} \\sum_{k=1}^{n} x_k\\) et \\(\\sigma^2 = \\frac{1}{n} \\sum_{k=1}^{n} (x_k - \\mu)^2\\) Ce qui \u00e9tait attendu. Impl\u00e9mentation Scipy Afin de r\u00e9aliser un ajustement de loi de probabilit\u00e9, on peut utiliser la biblioth\u00e8que de calculs scientifiques Scipy, et en particulier son module de statistiques \"scipy.stat\". Par exemple, pour un ajustement avec une loi normale, on pourra importer l'objet \"norm\" avec : from scipy.stats import norm On peut alors ajuster une loi normale \u00e0 un ensemble d'observations contenu dans un conteneur x_obs , et r\u00e9cup\u00e9rer la moyenne mu et l'\u00e9cart-type sigma avec : mu,sigma = norm.fit(x_obs) Par d\u00e9faut, la m\u00e9thode du maximum de vraisemblance est utilis\u00e9e. Mais on peut \u00e9galement utiliser la \"m\u00e9thode des moments\" (que nous ne pr\u00e9senterons pas dans ce cours), en ajoutant un param\u00e8tre method = 'MM' en entr\u00e9e. Une fois la loi normale ajust\u00e9e, on a acc\u00e8s \u00e0 la densit\u00e9 de probabilit\u00e9 dp associ\u00e9e \u00e0 une r\u00e9alisation x avec : dp = norm.pdf(x,mu,sigma) Bien d'autres lois de probabilit\u00e9 sont disponibles dans le module \"scipy.stats\", et fonctionnent sur le m\u00eame principe que \"norm\". Impl\u00e9mentation Scikit-Learn Il est \u00e0 noter qu'il existe aussi une impl\u00e9mentation de la classification Bayesienne dans Scikit-Learn, dans l'hypoth\u00e8se de distributions des features suivant des lois normales. Elle peut \u00eatre import\u00e9e avec : from sklearn.naive_bayes import GaussianNB On doit alors cr\u00e9er un objet \"GaussianNB\" qui contiendra notre mod\u00e8le, ici bayes_classifier : bayes_classifier = GaussianNB() Il faut ensuite l'entrainer avec nos features et labels d'entrainement, nomm\u00e9s ici feature_train et label_train : bayes_classifier.fit(feature_train,label_train) Et enfin, on peut r\u00e9aliser une pr\u00e9diction \u00e0 partir de features de test, nomm\u00e9s feature_test : label_test = bayes_classifier.predict(feature_test) Cette impl\u00e9mentation peut \u00eatre pratique dans certains cas, mais elle ne permet pas de jouer sur les hyperparam\u00e8tres suivants : la loi de probabilit\u00e9 et la m\u00e9thode d'ajustement. Une optimisation de ces hyperparam\u00e8tres n'est donc pas possible avec Scikit-Learn. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer la classification Bayesienne \u00e0 notre probl\u00e8me exemple. Afin de rendre la visualisation plus facile, nous allons simplifier le probl\u00e8me : Mettons que nous voulons juste effectuer une classification binaire de nos enregistrements, entre les classes \"flute\" ou \"trompette\", en utilisant comme unique feature l'amplitude relative de la 1\u00e8re harmonique. Pour ce faire, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, et nous s\u00e9lectionnons les variables et les individus qui nous int\u00e9ressent : df_dataset = pd.read_csv(input_path) df_dataset = df_dataset[['instrument','harmo1']] df_dataset = df_dataset[(df_dataset['instrument']=='flute')|(df_dataset['instrument']=='trumpet')] Nous diviserons ici nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 2 DataFrames : df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) On peut alors tracer un histogramme de notre feature pour les donn\u00e9es d'entrainement, en s\u00e9pararant les 2 classes : On peut noter qu'il y a peu de recouvrement entre les 2 distributions, ce qui laisse entrevoir qu'il est possible d'entrainer un mod\u00e8le \u00e0 classifier ces donn\u00e9es. En 1\u00e8re approche, nous choisissons d'ajuster \u00e0 ces 2 distributions des mod\u00e8les de lois normales. Il faudra se poser la question de la pertinence de ce choix. Tout d'abord, nous allons s\u00e9parer le jeu d'entrainement en 2 Series (DataFrame Pandas ne contenant qu'une colonnes) suivant si l'instrument est une flute ou une trompette : sr_harmo1_flute_train = df_train[df_train['instrument']=='flute']['harmo1'] sr_harmo1_trumpet_train = df_train[df_train['instrument']=='trumpet']['harmo1'] On peut alors r\u00e9aliser nos 2 ajustements, et r\u00e9cup\u00e9rer les param\u00e8tres \\(mu\\) et \\(\\sigma\\) correspondants : from scipy.stats import norm mu_harmo1_flute,sig_harmo1_flute = norm.fit(sr_harmo1_flute_train) mu_harmo1_trumpet,sig_harmo1_trumpet = norm.fit(sr_harmo1_trumpet_train) Maintenant que nous avons les param\u00e8tres de nos mod\u00e8les, nous pouvons \u00e9valuer la densit\u00e9 des probabilit\u00e9s conditionnelles pour la classe \"flute\" et la classe \"trompette\". Voici par exemple 301 \u00e9valuations pour des valeurs d'amplitude de la 1\u00e8re harmonique entre -30 et 0 dB : x_axis = np.linspace(-30,0,301) proba_norm_flute = norm.pdf(x_axis,mean_harmo1_flute,sig_harmo1_flute) proba_norm_trumpet = norm.pdf(x_axis,mean_harmo1_trumpet,sig_harmo1_trumpet) Nous pouvons alors tracer les courbes correspondantes par-dessus notre histogramme (affich\u00e9 en densit\u00e9 de probabilit\u00e9) : Si nos mod\u00e8les ne paraissent pas compl\u00e9ment inadapt\u00e9s, on peut noter qu'ils ne capturent pas la l\u00e9g\u00e8re asym\u00e9trie de nos distributions. On pourrait donc se poser la question d'essayer d'autres lois de probabilit\u00e9s, asym\u00e9triques. Continuons avec nos mod\u00e8les pour les probabilit\u00e9s conditionnelles. La prochaine \u00e9tape est d'estimer la probabilit\u00e9 de chaque classe, \u00e0 partir de leurs densit\u00e9s relatives : proba_flute = len(sr_harmo1_flute_train)/len(df_train) proba_trumpet = len(sr_harmo1_trumpet_train)/len(df_train) On peut alors utiliser estimer la densit\u00e9 de probabilit\u00e9 d'observation : proba_obs = proba_norm_flute*proba_flute + proba_norm_trumpet*proba_trumpet Si nous l'affichons avec les histogrammes, nous pouvons v\u00e9rifier qu'elle est bien coh\u00e9rente avec la distribution des observations. Enfin, nous pouvons calculer les probabilit\u00e9s a posteriori, en se basant sur la formule de Bayes : proba_bayes_flute = proba_norm_flute*proba_flute/proba_obs proba_bayes_trumpet = proba_norm_trumpet*proba_trumpet/proba_obs On peut alors afficher ces probabilit\u00e9s, et tracer la fronti\u00e8re de d\u00e9cision : La fronti\u00e8re de d\u00e9cision se trouve \u00e0 environ -13.16 dB : Si on mesure une 1\u00e8re harmonique ayant une amplitude inf\u00e9rieure, on classifiera l'instrument comme \u00e9tant une flute. Si on mesure une 1\u00e8re harmonique ayant une amplitude sup\u00e9rieure, on classifiera l'instrument comme \u00e9tant une trompette. Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9demment, si la probabilit\u00e9 d'appartenance aux classes ne nous int\u00e9resse pas, nous pourrions juste comparer \\(p(x \\mid C = 'flute')p(C='flute')\\) et \\(p(x \\mid C = 'trumpet')p(C='trumpet')\\) pour classifier les observations. Maintenant que nous avons vu le principe, on voudrait pouvoir r\u00e9-entrainer notre mod\u00e8le afin d'optimiser les hyperparam\u00e8tres, et r\u00e9aliser des pr\u00e9dictions sur les jeux d'entrainement et de test, le tout de mani\u00e8re efficace. Dans ce but, nous pouvons mettre notre classification binaire Bayesienne sous la forme d'une classe binary_bayes avec 2 m\u00e9thodes train et predict pour l'entrainement et la pr\u00e9diction. Voici un exemple d'impl\u00e9mentation : class binary_bayes: def __init__(self,stat_model): self.stat_model = stat_model self.true_params = None self.false_params = None self.proba_true = None def train(self,x_true,x_false): self.true_params = self.stat_model.fit(x_true) self.false_params = self.stat_model.fit(x_false) len_true = len(x_true) len_false = len(x_false) self.proba_true = len_true/(len_true+len_false) def predict(self,x): proba_norm_true = self.stat_model.pdf(x,*self.true_params) proba_norm_false = self.stat_model.pdf(x,*self.false_params) proba_obs = proba_norm_true*self.proba_true + proba_norm_false*(1-self.proba_true) proba_bayes_true = proba_norm_true*self.proba_true/proba_obs return proba_bayes_true On peut alors facilement d\u00e9finir un classifieur binaire \"est-ce une flute ?\" utilisant la loi normale telle qu'impl\u00e9ment\u00e9e par Scipy : from scipy.stats import norm is_a_flute = binary_bayes(norm) Entrainer ce classifieur sur notre jeu d'entrainement : is_a_flute.train(sr_harmo1_flute_train,sr_harmo1_trumpet_train) Et r\u00e9aliser des pr\u00e9dictions sur nos donn\u00e9es d'entrainement et de test : prediction_train = (is_a_flute.predict(df_train['harmo1'])) prediction_test = (is_a_flute.predict(df_test['harmo1'])) En partant du principe que nous positionnons la fronti\u00e8re de d\u00e9cision \u00e0 une probabilit\u00e9 d'appartenance \u00e0 classe \"flute\" de 0.5, nous pouvons obtenir les matrices de confusion en entrainement et en test avec les commandes suivantes : from sklearn.metrics import confusion_matrix #Label encoding: ground_truth_train = (df_train['instrument']=='flute').astype(int) ground_truth_test = (df_test['instrument']=='flute').astype(int) cm_train = confusion_matrix(ground_truth_train, prediction_train>0.5) cm_test = confusion_matrix(ground_truth_test, prediction_test>0.5) Voici les r\u00e9sultats en entrainement obtenus pour notre exemple : On observe que les performances du mod\u00e8le sont tr\u00e8s similaires entre les donn\u00e9es d'entrainement et de test. Ceci tend \u00e0 montrer que l'on a pas de probl\u00e8me de sur-ajustement important, ce qui laisse pr\u00e9sager des performances similaires en g\u00e9n\u00e9ralisation. Il n'y a aucun faux positif, mais on a quelques faux n\u00e9gatifs : parfois notre mod\u00e8le classifie des enregistrements de flutes comme n'\u00e9tant pas des flutes. Suivant les applications, on peut vouloir choisir une fronti\u00e8re de d\u00e9cision diff\u00e9rente, pour diminuer le nombre de faux n\u00e9gatifs, au prix d'une augmentation du nombre de faux positifs. Afin de voir les effets d'un tel choix, on tracer une courbe ROC \u00e0 partir des probabilit\u00e9s pr\u00e9dites par notre mod\u00e8le : from sklearn.metrics import roc_curve fp_rate_train, tp_rate_train, thresholds_train = roc_curve(ground_truth_train, prediction_train) fp_rate_test, tp_rate_test, thresholds_test = roc_curve(ground_truth_test, prediction_test) Remarques La m\u00e9thode de la classification de Bayes a les avantages suivants : Elle fonctionne pour tous les types de classification et variables . Elle est relativement simple \u00e0 mettre en place, avec peu de param\u00e8tres . Les d\u00e9cisions qu'elle prend sont compl\u00e8tement expliqu\u00e9es et interpr\u00e9tables : un humain peut les comprendre. Mais cette m\u00e9thode a aussi les limites suivantes : Elle fait l'hypoth\u00e8se de l' ind\u00e9pendance des variables entre elles, ce qui dans la pratique limite son application aux probl\u00e8mes avec peu de features. Elle fait une hypoth\u00e8se forte sur la distribution des observations pour chaque variable. Il s'agit souvent d'une hypoth\u00e8se de normalit\u00e9 . K Plus Proches Voisins Principe La m\u00e9thode de la classification Bayesienne que nous venons de voir avait pour d\u00e9savantage de n\u00e9cessiter une hypoth\u00e8se sur la distribution des observations. Dans cette section, nous allons pr\u00e9senter une m\u00e9thode ne n\u00e9cessitant aucun a priori sur les donn\u00e9es : les K Plus Proches Voisin , aussi connue sous l'acronyme KPPV. Les KPPV est une m\u00e9thode dite de \"lazy learning\" : il n'y a pas de r\u00e9el apprentissage pr\u00e9alable \u00e0 la pr\u00e9diction. Le jeu de donn\u00e9es d'apprentissage est stock\u00e9 en m\u00e9moire , et utilis\u00e9 au moment de la pr\u00e9diction. L'id\u00e9e est la suivante : pour classer un nouvel individu, on va calculer sa distance aux \\(k\\) individus les plus proches dans les donn\u00e9es d'entrainement. On attibura alors \u00e0 l'individu la classe la plus repr\u00e9sent\u00e9e parmi ses \\(k\\) \"plus proches voisins\". Pr\u00e9dire la classe d'un individu avec cette m\u00e9thode implique : (1) De mesurer la distance entre l'individu \u00e0 classifier et tous les individus du jeu d'entrainement . C'est ce que l'on appelle \"l'approche brute\". Et plus le jeu d'entrainement est grand, plus le temps de calcul sera long. Pour cette raison, on choisi de stocker le jeu d'entrainement dans une structure de donn\u00e9e la plus efficace \u00e0 parcourir possible (exemple : KD-Tree). (2) De choisir une mesure de distance adapt\u00e9e au probl\u00e8me. (3) De choisir le nombre de \"plus proches voisins\" \u00e0 l'individu \u00e0 consid\u00e9rer. (4) De choisir de quelle mani\u00e8re on va affecter une classe \u00e0 l'individu \u00e0 partir de la classe de ses voisins : Un vote majoritaire ? S'il y a un gros d\u00e9s\u00e9quilibre entre classes, ce type de vote risque d'\u00eatre biais\u00e9. On pr\u00e9f\u00e9rera alors un vote avec des poids diff\u00e9rents suivant les classes. Choix de la distance Suivant le probl\u00e8me de classification auquel on est confront\u00e9, la \"distance\" entre 2 individus n'a pas le m\u00eame sens. En effet, on comprend bien qu'on utilisera pas les m\u00eames crit\u00e8res pour mesurer la distance entre 2 valeurs r\u00e9elles, entre 2 images, ou entre 2 mots du dictionnaire. D'o\u00f9 l'importance lorsqu'on utilise les KPPV de choisir une mesure de distance pertinente pour notre probl\u00e8me. Parmi les mesures de distances classiques, on peut citer : Distance Euclidienne : Si on veut mesurer la distance Euclidienne entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\) Il s'agit de la mesure de distance la plus connue et la plus utilis\u00e9e. Elle fonctionne bien lorsque l'on est confront\u00e9s \u00e0 des valeurs r\u00e9elles continues, normalis\u00e9es, et avec une dimensionnalit\u00e9 faible. Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme 2. Distance de Manhattan : Si on veut mesurer la distance de Manhattan entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = \\sum_{i=1}^{n} \\mid x_i - y_i \\mid\\) Suivant l'espace des features de notre probl\u00e8me, tracer une \"ligne droite\" entre individus peut ne pas avoir de sens. La distance de Manhattan est alors une alternative \u00e0 la distance Euclidienne. Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme 1. Distance de Chebychev : Si on veut mesurer la distance de Chebychev entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = max(\\mid x_i - y_i \\mid)\\) La distance de Chebychev est assez peu utilis\u00e9e, car elle a des cas d'applications tr\u00e8s sp\u00e9cifiques. (Par exemple, les d\u00e9placements d'un roi sur un jeu d'\u00e9chec ou les automates cellulaires). Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme infinie. Minkowski : Si on veut mesurer la distance de Minkowski entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = (\\sum_{i=1}^{n} \\mid x_i - y_i \\mid^p)^{1/p}\\) La distance de Minkowski est une g\u00e9n\u00e9ralisation des 3 distances pr\u00e9c\u00e9dentes. En effet, on remarque que si \\(p=1\\) elle revient \u00e0 la distance de Manhattan, si \\(p=2\\) elle revient \u00e0 la distance Euclidienne, et si \\(p\\) tend vers l'infini elle revient \u00e0 la distance de Chebychev. Elle permet donc de chercher un compromis entre ces diff\u00e9rentes distances. Hamming : Soit 2 chaines de caract\u00e8res de m\u00eame taille. La distance de Hamming entre ces 2 chaines est alors \u00e9gale au nombre de positions pour lesquelles les caract\u00e8res sont diff\u00e9rents. Cette mesure de distance est couramment utilis\u00e9e lorsque l'on veut comparer des morceaux de textes caract\u00e8re par caract\u00e8re, ou de mani\u00e8re g\u00e9n\u00e9rale pour des donn\u00e9es qualitatives. Similarit\u00e9 cosinus : Si on veut mesurer la \"similarit\u00e9 cosinus\" entre 2 vecteurs \\(x\\) et \\(y\\) : \\(D(x,y) = cos(\\theta) = \\frac{x.y}{\\|x\\| \\|y\\|}\\) Il s'agit du cosinus de l'angle entre les 2 vecteurs. Cette mesure de distance est couramment utilis\u00e9e lorsque l'on doit comparer des vecteurs de haute dimensionnalit\u00e9, et o\u00f9 la norme du vecteur a peu d'importance. Par exemple, c'est la mesure de distance privil\u00e9gi\u00e9e pour de la \"fouille de texte\" (comparaison mot \u00e0 mot de chaines de caract\u00e8re). La distance est donc un hyperparam\u00e8tre \u00e0 optimiser lorsque l'on utilise les KPPV. Choix du param\u00e8tre K Il est \u00e9vident que le choix de \\(k\\) va avoir un impact sur les pr\u00e9dictions obtenues \u00e0 partir des donn\u00e9es d'entrainement. C'est donc \u00e9galement un hyperparam\u00e8tre \u00e0 optimiser . Pour choisir des valeurs de \\(k\\) \u00e0 tester, on peut partir des grands principes suivants : S'il y a un fort d\u00e9s\u00e9quilibre entre classes, il vaut mieux ne pas choisir un \\(k\\) faible . S'il y a beaucoup de recouvrement entre les classes, il vaut mieux choisir un \\(k\\) \u00e9lev\u00e9 . Avec un \\(k\\) trop faible on risque le sur-apprentissage . Avec un \\(k\\) trop grand on risque le sous-apprentissage . Pour \u00e9viter les cas d'\u00e9galit\u00e9, on va en g\u00e9n\u00e9ral choisir une valeur de \\(k\\) impaire. Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode des KPPV. Elle peut \u00eatre import\u00e9e avec : from sklearn.neighbors import KNeighborsClassifier On peut ensuite initialiser un classifieur KPPV knn avec un objet \"KNeighborsClassifier\" de param\u00e8tre k correspondant au nombre de plus proches voisins : knn = KNeighborsClassifier(n_neighbors=k) Pour donner le jeu d'entrainement (features avec feature_train et labels avec label_train ) \u00e0 ce classifieur, on utilise la m\u00e9thode : knn.fit(feature_train,label_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions label_test \u00e0 partir de features de test feature_test : label_test = knn.predict(feature_test) Si on veut effectuer un test de notre classifieur sur un jeu de donn\u00e9es lab\u00e9liser, on peut obtenir un score d'exactitude avec la commande : knn.score(feature_test,label_test) Outils de visualisation MLxtend Pour afficher les fronti\u00e8res de d\u00e9cision donn\u00e9es par un classifieur dans un cas 1D ou 2D, il existe une fonction de la biblioth\u00e8que \"MLxtend\". Une fois la biblioth\u00e8que install\u00e9e, vous pouvez importer la fonction avec : from mlxtend.plotting import plot_decision_regions Pour afficher les fronti\u00e8res de d\u00e9cision d'une classifieur model , avec les donn\u00e9es d'entrainement feature_train et label_train , on utilisera la m\u00e9thode : plot_decision_regions(feature_train, label_train, clf=model) Pour un probl\u00e8me de dimensionnalit\u00e9 plus \u00e9lev\u00e9e que 2, la visualisation des fronti\u00e8res de d\u00e9cision est toujours difficile. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer les KPPV \u00e0 notre probl\u00e8me exemple. Afin de rendre la visualisation plus facile, nous allons simplifier le probl\u00e8me : Mettons que nous voulons effectuer une classification de nos enregistrements entre les classes \"flute\", \"hautbois\" ou \"trompette\", en utilisant en features l'amplitude relative de la 1\u00e8re harmonique et de la 2\u00e8me harmonique. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) M\u00eame si en th\u00e9orie les KPPV n'ont pas besoin de labels num\u00e9riques pour fonctionner, certaines des fonctions que nous utiliserons dans la suite ne fonctionnent qu'avec des valeurs num\u00e9riques. Nous allons donc encoder les labels \"par \u00e9tiquette\" : from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() df_dataset['instrument'] = encoder.fit_transform(df_dataset['instrument']) Comme les KPPV se contentent de calculer des distances sur les features, on peut utiliser un encodage par \u00e9tiquette malgr\u00e9 le fait que nos labels ne sont pas ordinaux. Attention : pour les m\u00e9thodes utilisant une fonction de co\u00fbt sur les labels, un encodage \"one-hot\" devra \u00eatre utilis\u00e9 ! Nous r\u00e9cup\u00e9rons ensuite les features et les labels que nous allons utiliser dans 2 DataFrames : df_features = df_dataset[['harmo1','harmo2']] df_labels = df_dataset['instrument'] Nous s\u00e9parons ensuite nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 4 DataFrames (2 pour les features, 2 pour les labels) : from sklearn.model_selection import train_test_split df_features_train, df_features_test, df_labels_train, df_labels_test = train_test_split(df_features,df_labels,test_size=0.2,random_state=0) Enfin, nous r\u00e9alisons une transformation \"centrage-r\u00e9duction\" (voir Chapitre 1), pour s'assurer que les 2 features \u00e9voluent sur des intervalles comparables. Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_features_train) df_features_train[['harmo1','harmo2']] = scaler.transform(df_features_train) df_features_test[['harmo1','harmo2']] = scaler.transform(df_features_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre classifieur. Voici comment initialiser un classifieur KPPV avec \\(k=3\\) : from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=3) Pour lui fournir les donn\u00e9es d'entrainement, comme vu pr\u00e9c\u00e9demment, il nous suffit d'utiliser la commande suivante : knn.fit(df_features_train,df_labels_train) Nous pouvons \u00e0 pr\u00e9sent utiliser notre mod\u00e8le pour classifier des donn\u00e9es. Tout d'abord, nous allons \u00e9valuer les performances de notre mod\u00e8le en entrainement et en test. On peut d\u00e9j\u00e0 mesurer l'exactitude de notre classifieur sur ces 2 jeux de donn\u00e9es : print(knn.score(df_features_train,df_labels_train)) print(knn.score(df_features_test,df_labels_test)) Pour \\(k=3\\) , on obtient plus de 99.4% d'exactitude en entrainement, et environ 98.8% d'exactitude en test. Ces scores laissent \u00e0 penser que notre mod\u00e8le aura de plut\u00f4t bonnes performances en g\u00e9n\u00e9ralisation. Mais n'oublions pas que l'exactitude peut \u00eatre biais\u00e9e en cas de d\u00e9s\u00e9quilibre entre classes. Dans de tels cas, d'autres indicateurs doivent \u00eatre utilis\u00e9s. Voici une matrice de confusion compl\u00e8te pour nous aider \u00e0 conclure : On observe qu'\u00e0 l'entrainement comme en test, le hautbois est tr\u00e8s bien s\u00e9par\u00e9 des autres instruments, alors que la trompette et la flute sont parfois confondus. Ce r\u00e9sultat \u00e9tait pr\u00e9visible au vu de la matrice de nuages de points que nous avions obtenue lors de notre \u00e9tude pr\u00e9liminaire. Les proportions d'erreurs restent cependant relativement faibles compar\u00e9es aux nombres d'observations. Nous n'avons pour l'instant test\u00e9 qu'une valeur de \\(k\\) . Pour visualiser l'effet de cet hyperparam\u00e8tre, nous pouvons utiliser les affichages graphiques de la biblioth\u00e8que MLxtend. Voici les graphiques obtenus pour \\(k=3\\) (volontairement faible) et \\(k=31\\) (volontairement \u00e9lev\u00e9) : On peut noter que comme attendu, le choix de \\(k\\) a le plus d'effet \u00e0 la fronti\u00e8re entre \"flute\" et \"trompette\". En effet, comme il y a du recouvrement entre ces 2 classes, on sait qu'il vaut mieux choisir un \\(k\\) \u00e9lev\u00e9 pour \u00e9viter le sur-apprentissage. Ceci est confirm\u00e9 par le \"lissage\" de la fronti\u00e8re de d\u00e9cision lorsque l'on utilise \\(k=31\\) . Le choix d'un \\(k\\) \u00e9lev\u00e9 a donc l'air plus appropri\u00e9 ici, mais il faudrait r\u00e9aliser une r\u00e9elle optimisation de cet hyperparam\u00e8tre. Par d\u00e9faut, la distance utilis\u00e9e par l'impl\u00e9mentation Scikit-Learn des KPPV est la distance Euclidienne. Nous pouvons \u00e9galement r\u00e9aliser des affichages pour visualiser l'impact de diff\u00e9rentes distances pour une m\u00eame valeur de \\(k\\) . Voici le r\u00e9sultat pour la distance Euclidienne et la distance de Manhattan, avec \\(k=31\\) . On observe en effet que le choix de la distance impacte significativement les fronti\u00e8res de d\u00e9cision obtenues, m\u00eame s'il est difficile ici de juger de la pertinence d'une des 2 distances essay\u00e9es. Tout comme pour \\(k\\) , il faudrait r\u00e9aliser une v\u00e9ritable optimisation de cet hyperparam\u00e8tre. Remarques La m\u00e9thode des KPPV a les avantages suivants : Il s'agit d'une m\u00e9thode non-param\u00e9trique, qui ne fait aucune hypoth\u00e8se sur la structure des donn\u00e9es. Elle est relativement simple, et n'a que 2 hyperparam\u00e8tres ( \\(k\\) et la distance), ce qui est peu compar\u00e9 \u00e0 certaines m\u00e9thodes. Si de nouvelles observations doivent \u00eatre ajout\u00e9es au jeu d'entrainement, la mise \u00e0 jour du mod\u00e8le est directe . Mais cette m\u00e9thode a aussi les limites suivantes : Le mod\u00e8le ayant besoin de stocker les donn\u00e9es d'entrainement, il peut vite devenir tr\u00e8s lourd . Elle fonctionne mal avec des donn\u00e9es de grande dimension . Elle est tr\u00e8s sujette au sur-apprentissage . Perceptron multicouche La m\u00e9thode de classification que nous allons voir \u00e0 pr\u00e9sent est une ouverture vers les r\u00e9seaux de neurones artificiels , et la discipline qui leur est associ\u00e9e : l' apprentissag profond (ou \"Deep Learning\" en anglais). Perceptron : un neurone artificiel Lorsque l'on parle d'apprentissage pour un humain, on pense tout de suite \u00e0 son cerveau, et plus particuli\u00e8rement \u00e0 ses neurones . Un neurone est en effet une machine \u00e0 apprendre : Il prend plusieurs signaux \u00e9lectriques en entr\u00e9e, donne plus ou moins d'importance \u00e0 chacun, et transmet ou non un signal \u00e9lectrique en sortie en fonction de ces entr\u00e9es pond\u00e9r\u00e9es avec un seuil. Le neurone apprendra les poids \u00e0 donner \u00e0 chaque entr\u00e9e pour fournir une sortie pertinente pour une application. Un neurone se comporte donc comme un classifieur binaire . D'o\u00f9 l'id\u00e9e s\u00e9duisante de s'inspirer des neurones pour l'apprentissage de ce type de mod\u00e8le. Le 1er mod\u00e8le math\u00e9matique d'un neurone, appel\u00e9 \"neurone formel\" ou \"neurone artificiel\", est propos\u00e9 par McCulloch et Pitts en 1943. Dans le cadre de l'apprentissage automatique, il est plus connu sous le nom de \" perceptron \", concept invent\u00e9 par Rosenblatt en 1957. Voici le principe du perceptron \"historique\" de 1957 : Les \\(p\\) r\u00e9alisations de nos \\(p\\) features correspondant \u00e0 un individu sont fournies comme \\(p\\) entr\u00e9es \u00e0 notre neurone. Une entr\u00e9e suppl\u00e9mentaire toujours fix\u00e9e \u00e0 1 sera \u00e9galement fournie, on la nommera \" biais \". On applique \u00e0 chaque entr\u00e9e \\(x_i\\) un coefficient \\(w_i\\) . C'est ce que l'on appellera les param\u00e8tres du mod\u00e8le. Toutes les entr\u00e9es \\(x_i\\) pond\u00e9r\u00e9es par \\(w_i\\) sont somm\u00e9es , donnant la combinaison lin\u00e9aire \\(w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p\\) . Un seuil est finalement appliqu\u00e9 \u00e0 cette somme : suivant si elle d\u00e9passe ou non une certaine valeur, la sortie sera soit 0 ou 1, soit -1 ou 1 suivant la fonction de seuil choisie. L'apprentissage de ce mod\u00e8le consistera en l' optimisation des param\u00e8tres , afin qu'\u00e0 partir des features il soit capable d'associer ou non l'individu \u00e0 une classe (0 ou 1 en sortie). On utilisera le processus d'entrainement suivant : On initialise les param\u00e8tres al\u00e9atoirement. On fait passer un \u00e0 un les individus du jeu d'entrainement \u00e0 travers le mod\u00e8le. Pour chaque individu on compare la sortie du mod\u00e8le \u00e0 celle attendue, et on met \u00e0 jour les param\u00e8tres en cons\u00e9quence. On rep\u00e8te les 2 \u00e9tapes pr\u00e9c\u00e9dentes jusqu'\u00e0 convergence. On appelle la fonction apprise \\(f(x_1,x_2,...,x_p) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p\\) la fonction discriminante . Comme cette fonction est lin\u00e9aire, le mod\u00e8le ne pourra \u00e9tablir que des fronti\u00e8res de d\u00e9cision lin\u00e9aires (un point en 1D, une droite en 2D, un plan en 3D, un hyperplan dans le cas g\u00e9n\u00e9ral). Nous verrons que ceci est assez limitant en pratique : tous les probl\u00e8mes ne sont pas lin\u00e9airement s\u00e9parables ! Voici une repr\u00e9sentation sch\u00e9matique d'une fronti\u00e8re de d\u00e9cision 2D : Comme nous l'avons expliqu\u00e9 pr\u00e9c\u00e9demment, on peut r\u00e9aliser de la classification multi-classe \u00e0 partir de plusieurs classifieurs binaires, avec une strat\u00e9gie One-versus-All ou One-Versus-One. Pour un perceptron, il suffira donc d'utiliser plusieurs neurones en parall\u00e8le avec les m\u00eames entr\u00e9es. Voici un exemple de \\(n\\) perceptrons \\(N\\) en parall\u00e8le pour \\(n\\) classes : Pour pouvoir entrainer un perceptron, il reste \u00e0 choisir une m\u00e9thode pour mettre \u00e0 jour les param\u00e8tres du mod\u00e8le \u00e0 partir des erreurs de pr\u00e9diction. La \"r\u00e8gle d'apprentissage du perceptron\" propos\u00e9e par Rosenblatt est la suivante. A l'it\u00e9ration \\(k\\) , pour le \\(i\\) -\u00e8me param\u00e8tre, on applique : \\(w_i^{(k+1)} = w_i^{(k)} - \\gamma (y^{(k)}-\\hat{y}^{(k)})\\) avec \\(y\\) la sortie attendue, et \\(\\hat{y}\\) la pr\u00e9diction. On reconnait une m\u00e9thode type descente de gradient (voir Chapitre 1), mais tr\u00e8s simple. Ici, on ne calcule pas le gradient de la fonction de co\u00fbt \u00e0 partir de la totalit\u00e9 du jeu d'entrainement, mais \u00e0 partir d'un seul individu. Ce qui rend cet algorithme d'optimisation rapide, mais aussi sensible aux minima locaux. N\u00e9anmoins, on peut montrer que si les classes sont lin\u00e9airement s\u00e9parables alors la m\u00e9thode convergence forc\u00e9ment . C'est le \"th\u00e9or\u00e8me de convergence du perceptron\". Par contre, dans les cas non-lin\u00e9airement s\u00e9parables , cette m\u00e9thode ne donnera pas de r\u00e9sultats satisfaisants . Perceptron multicouche : un r\u00e9seau de neurones artificiels Le perceptron est une 1\u00e8re approche simple \u00e0 mettre en place et \u00e0 entrainer. Mais il n'est applicable en pratique que pour les probl\u00e8mes lin\u00e9airement s\u00e9parables. C'est pourquoi dans les ann\u00e9es 1960, a \u00e9merg\u00e9 l'id\u00e9e de relier plusieurs perceptron en sens direct. On appellera ce type de r\u00e9seau de neurones un perceptron multicouche (PMC). L'id\u00e9e est la suivante : si une combinaison lin\u00e9aire avec un seuil ne peut produire que des fronti\u00e8res de d\u00e9cisions lin\u00e9aires, une combinaison de s\u00e9parateurs lin\u00e9aires avec chacun un seuil peut donner un s\u00e9parateur non-lin\u00e9aire . Le PMC le plus basique poss\u00e8de 3 couches totalement connect\u00e9es : La couche d'entr\u00e9e : il s'agit simplement des diff\u00e9rentes features d'entr\u00e9e du mod\u00e8le. La couche cach\u00e9e : une couche de neurones (perceptrons), chacun ayant ses param\u00e8tres, son biais, son seuil. Elle est \"cach\u00e9e\" car ses sorties sont invisibles pour l'utilisateur. La couche de sortie : une couche de neurone en fin de r\u00e9seau, chacun ayant ses param\u00e8tres, son biais, son seuil, qui va renvoyer la sortie du mod\u00e8le pour chaque classe. On peut d\u00e9montrer que ce mod\u00e8le est capable de tracer n'importe quelle fronti\u00e8re de d\u00e9cision , \u00e0 condition d'avoir assez de neurones dans la couche cach\u00e9e. C'est ce que l'on appelle le th\u00e9or\u00e8me de l'approximation universelle . Le probl\u00e8me est que plus la fronti\u00e8re de d\u00e9cision \u00e0 tracer est complexe, et plus il faut de neurones dans la couche limite. Pour r\u00e9soudre ce probl\u00e8me, on peut ajouter plusieurs couches cach\u00e9es entre les couches d'entr\u00e9e et de sortie : pour un nombre de neurones par couche donn\u00e9, plus on aura de couches, et plus complexes les fronti\u00e8res de d\u00e9cisions pourront \u00eatre. D\u00e8s que l'on a plus d'une couche cach\u00e9e, on parle d' apprentissage profond (\"Deep Learning\"). Nous comprenons bien le potentiel du PMC... \u00e0 condition de pouvoir l'entrainer ! En effet, comment r\u00e9ussir \u00e0 optimiser efficacement les param\u00e8tres de nos diff\u00e9rentes couches, sachant que les couches sont totalement connect\u00e9es ? Ce probl\u00e8me est rest\u00e9 un point de blocage jusqu'en 1985, gr\u00e2ce aux travaux de Rumlhart et son \u00e9quipe. La m\u00e9thode qu'ils ont propos\u00e9e pour entrainer un PMC, encore utilis\u00e9e aujourd'hui, est connue sous le nom de r\u00e9tropropagation du gradient . Retropropagation du gradient Voici le principe sur lequel fonctionne l'algorithme de la propagation du gradient : Algorithme de retropropagation du gradient Les param\u00e8tres du mod\u00e8le sont initialis\u00e9s al\u00e9atoirement. A chaque it\u00e9ration (\u00e9poque) de l'algorithme : - Un \u00e9chantillon d'individus est s\u00e9lectionn\u00e9. - Passage direct : cet \u00e9chantillon est fourni en entr\u00e9e du mod\u00e8le, et la sortie est r\u00e9cup\u00e9r\u00e9e. - Une fonction de co\u00fbt est utilis\u00e9e pour \u00e9valuer l'erreur entre la sortie du mod\u00e8le et ce qui \u00e9tait attendu. - Passage inverse : le gradient de l'erreur associ\u00e9e \u00e0 chaque param\u00e8tre est calcul\u00e9 en \u00e9valuant la contribution de chaque param\u00e8tre \u00e0 l'erreur, en allant de la sortie vers l'entr\u00e9e. - Ces gradients sont fournis \u00e0 l'algorithme de descente de gradient (voir Chapitre 1), qui va mettre \u00e0 jour les param\u00e8tres du mod\u00e8le. On en d\u00e9duit que plus le nombre de couches cach\u00e9e et de neurones par couche cach\u00e9e sera \u00e9lev\u00e9, plus l'apprentissage sera long. Aussi, un nombre de couches trop \u00e9lev\u00e9 peut rendre difficile l'apprentissage des couches les plus en amont : un ph\u00e9nom\u00e8ne connu sous le nom de \"probl\u00e8me de la disparition du grandient\". Pour que cet algorithme fonctionne avec le PMC, il a fallu modifier la fonction de sortie des neurones. En effet, la fonction \"seuil\" ne permet pas de calculer un gradient : elle a un probl\u00e8me de diff\u00e9rentiabilit\u00e9, et une pente nulle partout sauf en un point. C'est pourquoi d'autres \" fonctions d'activation \" sont utilis\u00e9es en sortie des neurones d'un PMC. On peut citer les 3 plus utilis\u00e9es : Sigmo\u00efde : \\(g(u) = \\frac{1}{1+exp(-u)}\\) Il s'agit de la fonction historique, directement inspir\u00e9e de la fonction d'activation d'un neurone biologique. Elle a l'avantage d'\u00eatre continue et \u00e0 d\u00e9riv\u00e9e non nulle partout. Elle retourne un score entre 0 et 1. Tangente hyperbolique : \\(g(u) = tanh(u)\\) Tout comme la sigmo\u00efde, elle a une forme proche de la fonction d'activation d'un neurone biologique. Elle est \u00e9galement continue et \u00e0 d\u00e9riv\u00e9e non nulle partout. Elle retourne un score entre -1 et 1. ReLU : \\(g(u) = max(0,u)\\) Tr\u00e8s utilis\u00e9e depuis les ann\u00e9es 2010, cette fonction n'a aucune inspiration biologique. Elle a un probl\u00e8me de d\u00e9rivabilit\u00e9 en 0, et elle ne retourne pas un score born\u00e9. Par contre, elle permet de lutter contre les probl\u00e8me de \"disparition du gradient\". Lorsque l'on veut r\u00e9soudre un probl\u00e8me de classification multi-classe (mais pas multi-sorties) avec un PMC, au lieu d'une approche One-versus-All ou One-versus-One, on peut utiliser la fonction d'activation suivante : Softmax : pour chaque neurone de sortie \\(i\\) on calcule \\(g(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{n} e^{y_j}}\\) et on retient la classe correspondant \u00e0 la sortie maximisant cette fonction. L'id\u00e9e est que cette fonction prend en entr\u00e9e les scores renvoy\u00e9s par chaque neurone de sortie, et les transforme en probabilit\u00e9 d'appartenance \u00e0 chaque classe (la somme donnant 1). Nota Bene Pour que l'apprentissage d'un perceptron ou d'un PMC se d\u00e9roule correctement, il est recommand\u00e9 d'effectuer une transformation des donn\u00e9es d'entr\u00e9e (voir Chapitre 1), afin d'\u00e9viter que le mod\u00e8le donne artificiellement plus de poids \u00e0 une feature juste parce qu'elle varie sur une plus grande plage de valeurs. Choix des hyperparam\u00e8tres Une des difficult\u00e9 de l'apprentissage d'un PMC est le nombre \u00e9lev\u00e9 de param\u00e8tres et d'hyperparam\u00e8tres \u00e0 optimiser. Ceci rend le PMC particuli\u00e8rement sensible au sur-apprentissage . Une strat\u00e9gie de validation par exclusion ou de validation crois\u00e9e est donc plus que recommand\u00e9e ! Parmi les hyperparam\u00e8tres \u00e0 optimiser, on peut citer : Le nombre de couches cach\u00e9es. Le nombre de neurones par couche cach\u00e9e. La fen\u00eatre d'activation pour les couches cach\u00e9es. La fen\u00eatre d'activation pour la couche de sortie. Le taux d'apprentissage pour la descente de gradient. Le nombre maximum d'\u00e9poques d'apprentissage, et pratiquer ou non de l'arr\u00eat pr\u00e9matur\u00e9. La fonction de co\u00fbt utilis\u00e9e pour l'apprentissage. De plus, l'initialisation des param\u00e8tre du mod\u00e8le se faisant de mani\u00e8re al\u00e9atoire, 2 apprentissages ne donneront pas le m\u00eame mod\u00e8le. Il convient donc de tester plusieurs initialisations. Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn du PMC pour la classification. Elle peut \u00eatre import\u00e9e avec : from sklearn.neural_network import MLPClassifier On peut ensuite initialiser un classifieur PMC mlp avec les hyperparam\u00e8tres par d\u00e9faut en utilisant la commande : mlp = MLPClassifier() Nous verrons plus loin quels sont ces hyperparam\u00e8tres. Pour donner le jeu d'entrainement (features avec feature_train et labels avec label_train ) \u00e0 ce classifieur, on utilise la m\u00e9thode : mlp.fit(feature_train,label_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions label_test \u00e0 partir de features de test feature_test : label_test = mlp.predict(feature_test) Si on veut effectuer un test de notre classifieur sur un jeu de donn\u00e9es lab\u00e9liser, on peut obtenir un score d'exactitude avec la commande : mlp.score(feature_test,label_test) Voici les hyperparam\u00e8tres par d\u00e9faut de l'impl\u00e9mentation Scikit-Learn du PMC : Nombre de couches cach\u00e9es : 1 Nombre de neurones par couche cach\u00e9e : 100 La fen\u00eatre d'activation pour les couches cach\u00e9es : ReLU La fen\u00eatre d'activation pour la couche de sortie : Sigmo\u00efde si binaire, Softmax si multi-classe (non modifiable) Le taux d'apprentissage pour la descente de gradient : 0.001 Le nombre maximum d'\u00e9poques d'apprentissage : 200 (par d\u00e9faut sans arr\u00eat pr\u00e9matur\u00e9, mais on peut l'activer) La fonction de co\u00fbt : Log-loss (non modifiable) Ces hyperparam\u00e8tres sont presque tous modifiables par l'utilisateur. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer le PMC par d\u00e9faut de Scikit-Learn \u00e0 notre probl\u00e8me exemple. Cette fois-ci, nous n'allons pas simplifier notre exemple : nous traiterons directement le probl\u00e8me 3D. Comme pour les exemples pr\u00e9c\u00e9dents, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Nous allons ensuite encoder les labels \"par \u00e9tiquette\" : from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() df_dataset['instrument'] = encoder.fit_transform(df_dataset['instrument']) Ce choix peut paraitre \u00e9tonnant, puisque nos labels ne sont pas ordinaux. On aurait plut\u00f4t tendance \u00e0 utiliser de l'encodage \"one-hot\". L'astuce est que l'impl\u00e9mentation \"MLPClassifier\" de Scikit-Learn r\u00e9alise implicitement un encodage \"one-hot\" \u00e0 partir de labels \"par \u00e9tiquette\". Nous r\u00e9cup\u00e9rons ensuite les features et les labels que nous allons utiliser dans 2 DataFrames : df_features = df_dataset[['harmo1','harmo2','harmo3']] df_labels = df_dataset['instrument'] Nous s\u00e9parons ensuite nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 4 DataFrames (2 pour les features, 2 pour les labels) : from sklearn.model_selection import train_test_split df_features_train, df_features_test, df_labels_train, df_labels_test = train_test_split(df_features,df_labels,test_size=0.2,random_state=0) Afin d'aider le PMC \u00e0 converger, nous allons effectuer une transformation centrage-r\u00e9duction (voir Chapitre 1) afin de s'assurer que les 3 features \u00e9voluent sur des intervalles comparables. Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_features_train) df_features_train[['harmo1','harmo2','harmo3']] = scaler.transform(df_features_train) df_features_test[['harmo1','harmo2','harmo3']] = scaler.transform(df_features_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre classifieur. Voici comment initialiser un classifieur PMC avec les param\u00e8tres par d\u00e9faut : from sklearn.neural_network import MLPClassifier mlp = MLPClassifier() Pour l'entrainer, il nous suffit d'utiliser la commande suivante : mlp.fit(df_features_train,df_labels_train) Nous pouvons \u00e0 pr\u00e9sent utiliser notre mod\u00e8le pour classifier des donn\u00e9es. Tout d'abord, nous allons \u00e9valuer les performances de notre mod\u00e8le en entrainement et en test. On peut d\u00e9j\u00e0 mesurer l'exactitude de notre classifieur sur ces 2 jeux de donn\u00e9es : print(mlp.score(df_features_train,df_labels_train)) print(mlp.score(df_features_test,df_labels_test)) On obtient facilement plus de 99% d'exactitude en entrainement, et plus de 98.5% d'exactitude en test. Ces r\u00e9sultats laissent \u00e0 pense que les performances en g\u00e9n\u00e9ralisation de notre mod\u00e8le seront plut\u00f4t bonnes. Mais comme nous l'avons fait remarquer plus t\u00f4t, le PMC est sensible au sur-apprentissage. Pour cette raison, on peut vouloir appliquer la m\u00e9thode de r\u00e9gularisation par \"arr\u00eat pr\u00e9matur\u00e9\" (voir Chapitre 1). Si on active le param\u00e8tre early_stopping du classifieur PMC de Scikit-Learn, au moment de l'apprentissage il va automatiquement mettre de c\u00f4t\u00e9 une partie du jeu d'entrainement pour faire un jeu de validation. On peut m\u00eame choisir la fraction du jeu d'entrainement \u00e0 utiliser pour la validation, avec le param\u00e8tre validation_fraction . Voici un exemple de d\u00e9finition d'un classifieur, avec de l'arr\u00eat pr\u00e9matur\u00e9 et 20% des donn\u00e9es d'entrainement utilis\u00e9es pour la validation : mlp = MLPClassifier(early_stopping=True,validation_fraction=0.2) Il y a aussi une astuce pour afficher l'\u00e9valuation de la fonction de co\u00fbt au cours des \u00e9poques, pour l'ensemble d'entrainement et de validation. Elle se base sur : Extraire le jeu de validation avec la m\u00e9thode train_test_split de Scikit-Learn. Utiliser la m\u00e9thode partial_fit de notre classifieur PMC, qui permet de r\u00e9aliser une it\u00e9ration \u00e0 la fois. R\u00e9cup\u00e9rer la valeur de la fonction de co\u00fbt sur le jeu d'entrainement, avec l'attribut loss_ de notre classifieur. Evaluer la valeur de la fonction de co\u00fbt sur le jeu de validation, en utilisant la fonction log_loss de Scikit-Learn. Voici l'affichage des 2 courbes Matplotlib obtenues sur notre base de donn\u00e9es, pour 50 \u00e9poques : import matplotlib.pyplot as plt from sklearn.metrics import log_loss df_features_train, df_features_validation, df_labels_train, df_labels_validation = train_test_split(df_features_train,df_labels_train,test_size=0.2,random_state=0) mlp = MLPClassifier() loss_train = [] loss_validation = [] for idx in range(50): mlp.partial_fit(df_features_train,df_labels_train,classes=[0,1,2]) loss_train.append(mlp.loss_) loss_validation.append(log_loss(df_labels_validation,mlp.predict_proba(df_features_validation))) plt.plot(loss_train, label=\"train loss\",c='r') plt.plot(loss_validation, label=\"validation loss\",c='g') plt.xlabel('Epoques') plt.ylabel('Fonction de co\u00fbt') plt.legend() Les performances obtenues avec ce mod\u00e8le sont d\u00e9j\u00e0 excellentes, mais il faudrait \u00e0 pr\u00e9sent se baser sur ces codes pour effectuer une optimisation des hyperparam\u00e8tres. Remarques La m\u00e9thode du Perceptron Multi-Couche a les avantages suivants : Elle permet de dessiner des fronti\u00e8res de d\u00e9cision complexes entre les classes sans faire de grosses hypoth\u00e8ses statistiques au pr\u00e9alable. Une fois le mod\u00e8le entrain\u00e9, il prend moins de m\u00e9moire et est plus rapide pour faire des pr\u00e9diction que les KPPV. On peut entrainer ce type de mod\u00e8le sur de tr\u00e8s grandes bases de donn\u00e9es . Mais cette m\u00e9thode a aussi les limites suivantes : Elle a de nombreux param\u00e8tres et hyperparam\u00e8tres \u00e0 optimiser. Elle est sensible au sur-apprentissage . Les d\u00e9cisions qu'elle prend sont difficilement expliqu\u00e9es et interpr\u00e9tables : il est difficile voir impossible pour un humain de les comprendre. On parle de \"bo\u00eete noire\". L'initialisation de la m\u00e9thode se faisant de mani\u00e8re al\u00e9atoire , 2 apprentissages ne donneront pas exactement le m\u00eame mod\u00e8le. Les r\u00e9seaux de neurones sont \u00e0 la base des mod\u00e8les d'apprentissage modernes, fondant ainsi une nouvelle sous-discipline : l' apprentissage profond . Nous verrons dans le chapitre suivant que le PMC peut aussi \u00eatre utilis\u00e9 pour r\u00e9soudre des probl\u00e8mes de r\u00e9gression...","title":"II. Classification supervis\u00e9e"},{"location":"Chap2_Classification_supervisee/#chapitre-ii-classification-supervisee","text":"Ce chapitre est une introduction \u00e0 la classification supervis\u00e9e : principe, mesures de performances et m\u00e9thodes de base.","title":"Chapitre II : Classification supervis\u00e9e"},{"location":"Chap2_Classification_supervisee/#probleme-de-classification-supervisee","text":"Comme mentionn\u00e9 lors du Chapitre I, par \" classifier \" on entend associer une r\u00e9alisation d'une variable quantitative discr\u00e8te ou qualitative \u00e0 un individu (labels), \u00e0 partir des r\u00e9alisations d'autres variables (features). On appelle ces labels des \" classes \". On parlera ici de \"classification supervis\u00e9e\" car on va entrainer un mod\u00e8le (aussi appel\u00e9 \"classifieur\") \u00e0 associer une classe \u00e0 des individus, en se basant sur des donn\u00e9es d\u00e9j\u00e0 lab\u00e9lis\u00e9es. Il s'agit donc bien d'un apprentissage supervis\u00e9 . L'id\u00e9e est que le classifieur soit ensuite capable de g\u00e9n\u00e9raliser : pr\u00e9dire la \"classe\" d'un nouvel individu.","title":"Probl\u00e8me de classification supervis\u00e9e"},{"location":"Chap2_Classification_supervisee/#les-differents-types-de-classification","text":"Plut\u00f4t que de parler de \"la\" classification, on devrait par \"des\" classifications, car il existe plusieurs types de probl\u00e8mes de classification. Nous allons donc commencer par parler des diff\u00e9rents types de classification, en illustrant avec un exemple : reconnaitre sur une photo un instrument de musique breton.","title":"Les diff\u00e9rents types de classification"},{"location":"Chap2_Classification_supervisee/#binaire","text":"Le type de classification le plus basique, et pour lequel tous les mod\u00e8les de classification peuvent \u00eatre entrain\u00e9s, est la classification binaire . Comme son nom l'indique, l'id\u00e9e est simplement de r\u00e9soudre un probl\u00e8me o\u00f9 l'on veut s\u00e9parer les individus en 2 classes . Il peut s'agir de pr\u00e9dire l'appartenance \u00e0 2 classes exclusives dans un cas o\u00f9 il n'y a que 2 labels possibles, par exemple : \"L'instrument sur la photo est-il une bombarde ou un biniou ?\". Ou alors il peut s'agir de pr\u00e9dire l'appartenance ou la non appartenance \u00e0 une classe parmi d'autres, par exemple : \"L'instrument sur la photo est-il une bombarde ou un autre instrument ?\". Beaucoup des m\u00e9thodes et des crit\u00e8res de performances qui sont pr\u00e9sent\u00e9es dans ce cours ont d'abord \u00e9t\u00e9 d\u00e9finis pour des probl\u00e8mes binaires, avant d'\u00eatre g\u00e9n\u00e9ralis\u00e9s. Nota Bene : En g\u00e9n\u00e9ral, un classifieur binaire ne retourne pas directement une pr\u00e9diction de la classe de l'individu, mais une probabilit\u00e9 d'appartenance \u00e0 la classe : un score entre 0 et 1. Il faut alors placer un seuil sur cette probabilit\u00e9 pour choisir si l'individu appartient \u00e0 la classe ou non (souvent 0.5 par d\u00e9faut). On appelle ce seuil fronti\u00e8re de d\u00e9cision . Les impl\u00e9mentations Scikit-Learn des m\u00e9thodes de classification peuvent souvent retourner soit directement la classe pr\u00e9dite, soit la probabilit\u00e9 d'appartenance \u00e0 la classe.","title":"Binaire"},{"location":"Chap2_Classification_supervisee/#multi-classe","text":"Si on veut classer des individus dans plus de 2 classes , on va parler de classification multi-classe . Par exemple, \"L'instrument sur la photo est-il une bombarde ou un biniou\" est un probl\u00e8me de classification binaire, alors que \"L'instrument sur la photo est-il une bombarde, un biniou ou un tambour ?\" est un probl\u00e8me de classification multi-classe. Or, si toutes les m\u00e9thodes sont capables de r\u00e9aliser une classification binaires, toutes ne sont pas capables de r\u00e9aliser une classification multi-classe. Pour contourner ce probl\u00e8me, on va ramener ce probl\u00e8me \u00e0 de multiples classifications binaires , avec une strat\u00e9gie pour choisir la pr\u00e9diction \u00e0 retourner : One-versus-All : on entraine un classifieur binaire par classe, et la classe pr\u00e9dite pour un individu donn\u00e9 sera celle dont le classifieur aura retourn\u00e9 la probabilit\u00e9 la plus \u00e9lev\u00e9e. One-versus-One : on entraine un classifieur pour chaque couple de classes possible, et la classe pr\u00e9dite est celle qui aura gagn\u00e9 le plus de \"duels\" parmi les sorties des diff\u00e9rents classifieurs. Pour \\(N\\) classes, la strat\u00e9gie \"One-versus-One\" implique d'entrainer \\(N(N-1)/2\\) classifieurs, l\u00e0 o\u00f9 la strat\u00e9gie \"One-versus-All\" n'a besoin d'en entrainer que \\(N\\) . Mais chaque mod\u00e8le est entrain\u00e9 sur un plus petit jeu de donn\u00e9es pour la m\u00e9thode \"one-versus-one\" que pour la m\u00e9thode \"one-versus-all\" Le choix de strat\u00e9gie d\u00e9pendra donc de l'application . Les m\u00e9thodes disponibles sous Scikit-Learn choisissent une strat\u00e9gie par d\u00e9faut, mais il est possible de la modifier.","title":"Multi-classe"},{"location":"Chap2_Classification_supervisee/#multi-etiquettes","text":"Dans les types de classification pr\u00e9c\u00e9dents, on ne pouvait associer qu'une seule classe \u00e0 un individu. Cependant, pour certains probl\u00e8mes il est possible qu' un individu puisse faire partie de plusieurs classes \u00e0 la fois . Par exemple, si le probl\u00e8me est \"Quel instrument est sur cette photo ?\", et que la photo contient une bombarde et un biniou, alors le morceau appartient \u00e0 la fois \u00e0 la classe \"bombarde\" et \u00e0 la classe \"biniou\". Certaines m\u00e9thodes impl\u00e9ment\u00e9es dans Scikit-Learn acc\u00e8ptent une matrices de labels en entrainement au lieu d'un vecteur, et d'autres non. Il faut donc v\u00e9rifier si la m\u00e9thode que vous voulez utiliser supporte bien la classification multi-\u00e9tiquettes. Si un classifieur est multi-\u00e9tiquettes, et que chaque \u00e9tiquette est multi-classe, on dira le classifieur \" multi-sorties \".","title":"Multi-\u00e9tiquettes"},{"location":"Chap2_Classification_supervisee/#exemple-de-probleme","text":"Pourquoi est-on capables de reconnaitre le son d'un instrument de musique d'un autre ? Lorsqu'un instrument joue une note, le son \u00e9mit ne contient jamais qu'une seule fr\u00e9quence. Il est en r\u00e9alit\u00e9 constitu\u00e9 d'une \"fr\u00e9quence fondamentale\" (la note que l'on veut jouer), et des \"harmoniques\" (des fr\u00e9quences multiples de la fondamentale). Pour une m\u00eame note jou\u00e9e, suivant l'instrument, les harmoniques n'auront pas la m\u00eame amplitude compar\u00e9e \u00e0 la fondamentale. C'est ce que l'on appelle le \"timbre\" de l'instrument. Lorsque nous \u00e9coutons de la musique, et que nous reconnaissons le son d'un instrument, c'est gr\u00e2ce \u00e0 son timbre. Voici 3 exemples de spectres issus d'enregistrements d'une flute, d'un hautbois et d'une trompette jouant un La (440 Hz) : On voit nettement la diff\u00e9rence de timbre entre les 3 instruments. D'o\u00f9 l'id\u00e9e suivante : peut-on entrainer un mod\u00e8le \u00e0 reconnaitre un instrument \u00e0 partir d'un enregistrement ? Voici un jeu de donn\u00e9es au format CSV, collect\u00e9es \u00e0 partir de milliers d'enregistrements d'une flute, d'un hautbois et d'une trompette jouant un La (440 Hz) : Chap2_instruments_dataset Le tableau de donn\u00e9es qu'il contient est de la forme suivante : instrument harmo1 harmo2 harmo3 oboe 11.842 11.58 10.28 flute -17.083 -17.384 -21.496 trumpet -8.152 -24.089 -23.813 oboe 9.381 12.434 11.905 oboe -1.217 2.082 16.275 trumpet -3.294 -13.812 -17.934 trumpet -4.118 -13.485 -18.985 ... ... ... ... trumpet -7.762 -5.934 -23.308 flute -17.96 -19.406 -22.409 oboe 7.764 6.618 13.361 Il contient pour chacun des 5612 enregistrements le nom de l'instrument, et l'amplitude en dB des 3 premi\u00e8res harmoniques relativement \u00e0 la fondamentale. Notre probl\u00e8me de classification sera le suivant : pr\u00e9dire l'instrument ayant jou\u00e9 un La \u00e0 partir des amplitudes des 3 premi\u00e8res harmoniques . Voyons d'abord si une telle classification est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) Il est possible avec Seaborn d'afficher ces donn\u00e9es sous la forme d'une matrice de nuages de points , avec chaque classe d'une couleur diff\u00e9rente. Ce type de repr\u00e9sentation permet de v\u00e9rifier la s\u00e9parabilit\u00e9 des diff\u00e9rentes classes \u00e0 partir des features s\u00e9lectionn\u00e9s. Voici la commande Seaborn : import seaborn as sns sns.pairplot(df_dataset,hue='instrument') On obtient alors le graphique suivant : On observe que les classes \"flute\", \"oboe\" et \"trumpet\" sont plut\u00f4t bien s\u00e9parables \u00e0 partir des amplitudes des 3 premi\u00e8res harmoniques. Vouloir entrainer un mod\u00e8le \u00e0 reconnaitre un de ces instruments \u00e0 partir de ces donn\u00e9es \u00e0 donc du sens. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP.","title":"Exemple de probl\u00e8me"},{"location":"Chap2_Classification_supervisee/#mesures-de-performance","text":"Nous allons passer en revue dans cette section les principaux indicateurs de performances applicables \u00e0 tous les types de classification.","title":"Mesures de performance"},{"location":"Chap2_Classification_supervisee/#matrice-de-confusion","text":"Pour chaque classe \\(C\\) possible, lorsqu'un classifieur r\u00e9alise une pr\u00e9diction sur un individu, il y a 4 possibilit\u00e9s : Le classifieur a pr\u00e9dit \\(C\\) , et l'individu appartient bien \u00e0 \\(C\\) : c'est un vrai positif (not\u00e9 TP). Le classifieur a pr\u00e9dit \\(C\\) , et l'individu n'appartient pas \u00e0 \\(C\\) : c'est un faux positif (not\u00e9 FP). Le classifieur n'a pas pr\u00e9dit \\(C\\) , et l'individu n'appartient pas \u00e0 \\(C\\) : c'est un vrai n\u00e9gatif (not\u00e9 TN). Le classifieur n'a pas pr\u00e9dit \\(C\\) , et l'individu appartient bien \u00e0 \\(C\\) : c'est un faux n\u00e9gatif (not\u00e9 FN). Tous les scores de performance pour la classification que nous allons voir se basent sur le nombre de TP, FP, TN et FN obtenus par le mod\u00e8le sur un jeu d'individus lab\u00e9lis\u00e9. Les indicateurs brutes que sont le nombre de TP, FP, TN et FN sont en g\u00e9n\u00e9ral mis sous la forme d'un tableau, que l'on appelle matrice de confusion . Voici \u00e0 quoi ressemble ce tableau pour une seule classe d'un probl\u00e8me multi-classe, ou pour un probl\u00e8me de classification binaire : On peut \u00e9galement repr\u00e9senter les r\u00e9sultats d'une classification multi-classe pour toutes les classes sous la forme d'une matrice de confusion. Voici un exemple pour 5 classes \\(C_1\\) , \\(C_2\\) , \\(C_3\\) , \\(C_4\\) et \\(C_5\\) : On peut alors lire ce tableau d'un point de vue g\u00e9n\u00e9ral : la diagonale correspond aux vrais positifs \u00e0 maximiser. Mais on peut aussi le lire du point de vue d'une classe ( \\(C_3\\) dans notre illustration), et calculer les nombres de TP, FP, TN et FN correspondant. La matrice de confusion est la repr\u00e9sentation la plus exhaustive possible des performances d'un classifieur, mais elle est d'autant plus difficile \u00e0 lire que le nombre de classes est grand . Ceci peut rendre complexe la comparaison entre 2 classifieurs. Pour cette raison, on va souvent utilis\u00e9 des scores d\u00e9riv\u00e9s du tableau de confusion.","title":"Matrice de confusion"},{"location":"Chap2_Classification_supervisee/#exactitude","text":"Le score d' exactitude (\"accuracy\" en anglais) est le plus classique pour \u00e9valuer les performances d'un classifieur. D\u00e9finition L' exactitude : est la taux d'individus class\u00e9s correctement parmi tous les individus class\u00e9s. Dans le cas d'une classification binaire, il s'agit donc de : \\(\\frac{TP+TN}{TP+FP+TN+FN}\\) Dans le cas d'une classification multi-classe, il s'agira de la trace de la matrice de confusion divis\u00e9e par le nombre total d'individus class\u00e9s. Si cet indicateur est intuitif et permet de condenser l'information en un score unique, il aura tendance \u00e0 \u00eatre biais\u00e9 s'il y a un fort d\u00e9s\u00e9quilibre entre classes. En effet, comme on somme TP et TN, l'exactitude aura tendance \u00e0 favoriser la classe majoritaire . C'est pourquoi dans un cas d\u00e9s\u00e9quilibr\u00e9, on pr\u00e9f\u00e8rera utiliser un duo de scores de performances : pr\u00e9cision-rappel ou rappel-fausse alarme.","title":"Exactitude"},{"location":"Chap2_Classification_supervisee/#precision-rappel-et-score-f1","text":"Si la classe consid\u00e9r\u00e9e est majoritaire , ou si pour notre application nous pr\u00e9f\u00e9rons r\u00e9duire les faux positifs quitte \u00e0 augmenter les faux n\u00e9gatifs, on utilisera plut\u00f4t les indicateurs de pr\u00e9cision et de rappel . D\u00e9finitions - La pr\u00e9cision : est le taux d'individus attribu\u00e9s correctement \u00e0 une classe parmi toutes les pr\u00e9dictions de cette classe. - Le rappel (\"sensibilit\u00e9\" ou \"recall\" en anglais) : est le taux d'individus attribu\u00e9s correctement \u00e0 une classe tous les individus appartenant r\u00e9ellement \u00e0 cette classe. Dans un cas binaire, il s'agit donc de : Pr\u00e9cision : \\(\\frac{TP}{TP+FP}\\) Rappel : \\(\\frac{TP}{TP+FN}\\) Ces 2 scores sont antagonistes : on doit donc choisir un compromis entre les 2 suivant notre application. Si on veut obtenir un compromis donnant une pr\u00e9cision et un rappel similaires, on peut utiliser la moyenne harmonique de ces 2 scores : \\(F_1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{rappel}}\\) C'est ce que l'on appelle le score F1 . Mais si on veut trouver un compromis donnant une pr\u00e9cision et un rappel en particulier, il faut utiliser une courbe pr\u00e9cision-rappel . L'id\u00e9e est de faire varier le seuil de d\u00e9cision pour chaque classe, et d'afficher les compromis entre pr\u00e9cision et rappel obtenus pour chaque seuil : La ligne diagonale correspond \u00e0 la performance th\u00e9orique d'un classifieur al\u00e9atoire.","title":"Pr\u00e9cision-rappel et score F1"},{"location":"Chap2_Classification_supervisee/#courbe-roc","text":"Si la classe consid\u00e9r\u00e9e est minoritaire , ou si pour notre application nous pr\u00e9f\u00e9rons r\u00e9duire les faux n\u00e9gatifs quitte \u00e0 augmenter les faux positifs, on utilisera plut\u00f4t les indicateurs de rappel et de fausse alarme . D\u00e9finitions - Le taux de fausse alarme : est taux d'individus attribu\u00e9s incorrectement \u00e0 une classe parmi tous les individus n'appartenant pas \u00e0 cette classe. Pour des raisons historiques, on appelle souvent dans ce contexte le rappel \" taux de vrais positifs \" (TPR) et le taux de fausse alarme \" taux de faux positifs \" (FPR). Dans un cas binaire, il s'agit donc de : TPR : \\(\\frac{TP}{TP+FN}\\) FPR : \\(\\frac{FP}{FP+TN}\\) Ces 2 scores sont \u00e9galement antagonistes : on doit donc aussi choisir un compromis entre les 2 suivant notre application. Une fois encore, pour trouver un compromis donnant un TPR et un FPR en particulier, on peut tracer les compromis obtenus pour diff\u00e9rents seuils de d\u00e9cision. On appelle ce type de courbe \"Reicever Operating Caracteristic\" (ROC) : Le nom \u00e9trange de cette courbe a une origine historique : elle aurait \u00e9t\u00e9 invent\u00e9e durant la 2nde guerre mondiale, dans le cadre de la classification binaire de signaux radar entre \"avion ennemi\" et \"bruit\".","title":"Courbe ROC"},{"location":"Chap2_Classification_supervisee/#methodes-de-base","text":"","title":"M\u00e9thodes de base"},{"location":"Chap2_Classification_supervisee/#decision-bayesienne","text":"La d\u00e9cision Bayesienne , aussi connue sous le nom de \"classification Bayesienne na\u00efve\" est une m\u00e9thode de classification se basant sur un mod\u00e8le probabiliste des features, consid\u00e9r\u00e9es ind\u00e9pendantes , et du th\u00e9or\u00e8me de Bayes .","title":"D\u00e9cision Bayesienne"},{"location":"Chap2_Classification_supervisee/#principe","text":"Imaginons que nous avons un probl\u00e8me de classification avec \\(q\\) classes \\(C_1\\) , \\(C_2\\) , ..., \\(C_q\\) . Nous voulons pr\u00e9dire la classe \u00e0 laquelle appartient un individu. La probabilit\u00e9 de chaque classe \\(i\\) not\u00e9e \\(p(C_i)\\) , aussi appel\u00e9e \" probabilit\u00e9 a priori \". On a \\(\\sum_{i=1}^{q} p(C_i) = 1\\) et on peut facilement estimer les diff\u00e9rents \\(p(C_i)\\) \u00e0 partir du nombre d'occurences de \\(C_i\\) dans les donn\u00e9es divis\u00e9e par la taille de la base de donn\u00e9es. En ne connaissant que les probabilit\u00e9s a priori de chaque classe, nous serions oblig\u00e9s de classer n'importe quel individu comme appartenant \u00e0 la classe \\(C_i\\) ayant le \\(p(C_i)\\) le plus \u00e9lev\u00e9. Nous aurions alors un classifieur retournant toujours la m\u00eame classe. Pas tr\u00e8s utile... Or, nous avons en r\u00e9alit\u00e9 acc\u00e8s \u00e0 plus d'informations : nos fameuses \"features\", dont nous voulons nous servir pour pr\u00e9dire la classe d'un individu. Mettons que nous avons acc\u00e8s \u00e0 une feature d'int\u00e9r\u00eat pour cette classification. On notera \\(X\\) l'espace des observations associ\u00e9. Pour d\u00e9terminer la classe d'un individu, on peut alors partir du principe suivant : choisir le \\(C_i\\) tel que \\(p(C_i \\mid x)\\) soit maximal . Nous expliquerons pourquoi dans la suite. On nomme \\(p(C_i \\mid x)\\) \" probabilit\u00e9s a posteriori \". D'apr\u00e8s le th\u00e9or\u00e8me de Bayes : \\(p(C_i \\mid x) = \\frac{p(x \\mid C_i)p(C_i)}{p(x)}\\) avec \\(p(x) = \\sum_{i=1}^{q} p(x \\mid C_i)p(C_i)\\) On nomme \\(p(x)\\) la densit\u00e9 de \" probabilit\u00e9 d'observation \", et \\(p(x \\mid C_i)\\) la densit\u00e9 de \" probabilit\u00e9 conditionnelle d'observation \". Toute la difficult\u00e9 de la m\u00e9thode est d' estimer \\(p(x \\mid C_i)\\) . On va en g\u00e9n\u00e9ral chercher \u00e0 mod\u00e9liser ces densit\u00e9s de probabilit\u00e9 conditionnelle. NB : Il est \u00e0 noter que rechercher la classe \\(C_i\\) maximisant \\(p(C_i \\mid x)\\) est \u00e9quivalent \u00e0 rechercher \\(C_i\\) maximisant \\(p(x \\mid C_i)p(C_i)\\) . Il n'est donc en th\u00e9orie pas utile de calculer \\(p(x)\\) pour obtenir le classifieur. Mais il est n\u00e9cessaire d'avoir \\(p(x)\\) pour obtenir des probabilit\u00e9s d'appartenance \u00e0 une classe. Attention ! En g\u00e9n\u00e9ral, il y a des recouvrements entre les diff\u00e9rentes densit\u00e9s de probabilit\u00e9 conditionnelle. On ne peut alors pas obtenir classifieur parfait. On cherchera juste le mod\u00e8le permettant de minimiser les erreurs de classification. Cas particulier : Si tous les \\(p(x \\mid C_i)\\) sont \u00e9gaux, alors la feature s\u00e9lectionn\u00e9e n'est pas pertinente pour la classification. Ce principe est g\u00e9n\u00e9ralisable aux cas de classifications avec \\(m\\) features d'espaces de probabilit\u00e9 \\(X_1\\) , \\(X_2\\) , ... \\(X_m\\) . On cherchera la classe \\(C_i\\) qui maximise \\(p(C_i) \\prod_{j=1}^{m}p(x_j \\mid C_i)\\) .","title":"Principe"},{"location":"Chap2_Classification_supervisee/#frontiere-de-decision-et-erreur","text":"Comme nous l'avons expliqu\u00e9 pr\u00e9c\u00e9demment, sauf cas particulier, on ne peut pas obtenir un classifieur parfait. On va donc essayer d'\u00e9tablir des fronti\u00e8res de d\u00e9cision entre les classes : des intervalles de \\(x\\) pour lesquels on attribura une classe. Et nous recherchons m\u00eame les fronti\u00e8res de d\u00e9cision optimales : celles qui minimisent le risque d'erreurs. Prenons un cas simple de classification binaire entre 2 classes \\(C_1\\) et \\(C_2\\) . Nous noterons \\(x = D\\) la fronti\u00e8re de d\u00e9cision choisie. On a alors 2 types d'erreurs de classification possibles : Classifier l'individu en \\(C_1\\) alors qu'il appartient \u00e0 \\(C_2\\) . Classifier l'individu en \\(C_2\\) alors qu'il appartient \u00e0 \\(C_1\\) . Les probabilit\u00e9s d'erreurs associ\u00e9es sont \\(\\int_{-\\infty}^{D} p(x \\mid C_2)p(C_2) dx\\) et \\(\\int_{D}^{+\\infty} p(x \\mid C_1)p(C_1) dx\\) . Elles correspondent aux aires repr\u00e9sent\u00e9es sur ce sch\u00e9ma : Pour obtenir la fronti\u00e8re de d\u00e9cision optimale \\(x = O\\) , on va chercher \u00e0 minimiser la somme de ces erreurs. L'aire entour\u00e9e en vert correspond \u00e0 ce que l'on appelle \"l' erreur r\u00e9ductible \" : c'est la portion de l'erreur totale que l'on peut r\u00e9duire pour obtenir la fronti\u00e8re optimale. On retrouve bien que : Si \\(p(C_1 \\mid x) > p(C_2 \\mid x)\\) alors on classifie l'individu comme appartenant \u00e0 \\(C_1\\) . Si \\(p(C_1 \\mid x) < p(C_2 \\mid x)\\) alors on classifie l'individu comme appartenant \u00e0 \\(C_2\\) . On peut g\u00e9n\u00e9raliser \u00e0 \\(q\\) classes : comme dit pr\u00e9c\u00e9demment, pour obtenir les fronti\u00e8res de d\u00e9cision optimales, on choisi le \\(C_i\\) tel que \\(p(C_i \\mid x)\\) soit maximal .","title":"Fronti\u00e8re de d\u00e9cision et erreur"},{"location":"Chap2_Classification_supervisee/#choix-du-modele","text":"Comme nous l'avons expliqu\u00e9, la d\u00e9cision Bayesienne n\u00e9cessite un mod\u00e8le des probabilit\u00e9s conditionnelles d'observation \\(p(x \\mid C_i)\\) pour chaque classe \\(C_i\\) . Pour ce faire, on ajuste une fonction de densit\u00e9 de probabilit\u00e9 pour chaque \\(p(C_i \\mid x)\\) \u00e0 notre jeu d'entrainement. Ceci implique donc 2 choix : Une fonction de densit\u00e9 de probabilit\u00e9 , ce qui implique de faire une hypoth\u00e8se forte sur la distribution des observations pour chaque classe. Une m\u00e9thode d'ajustement de loi de probabilit\u00e9 . La fonction de densit\u00e9 de probabilit\u00e9 la plus classique est celle de la loi normale : \\(f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\) avec 2 param\u00e8tres \u00e0 ajuster \\(\\mu\\) (la moyenne) et \\(\\sigma\\) (l'\u00e9cart-type). La m\u00e9thode d'ajustement la plus classique pour la d\u00e9cision Bayesienne est celle du maximum de vraisemblance . C'est elle que nous allons d\u00e9tailler.","title":"Choix du mod\u00e8le"},{"location":"Chap2_Classification_supervisee/#maximum-de-vraisemblance","text":"D\u00e9finition Soit une loi de probabilit\u00e9 \\(f(x,\\theta)\\) , d\u00e9finie par des param\u00e8tres \\(\\theta\\) . Pour un \u00e9chantillon observ\u00e9 \\((x_1,x_2,...,x_n)\\) , on nomme vraisemblance (\"likelihood\" en anglais) la probabilit\u00e9 que cet \u00e9chantillon provienne d'un tirage de \\(f(x,\\theta)\\) . Si les tirages sont ind\u00e9pendants, on peut exprimer la vraisemblance de la mani\u00e8re suivantes : \\(L(x_1,x_2,...,x_n,\\theta) = \\prod_{k=1}^{n} f(x_k,\\theta)\\) La m\u00e9thode du maximum de vraisemblance d\u00e9coule du fait que le mod\u00e8le \\(f\\) de param\u00e8tres \\(\\theta\\) repr\u00e9sentant le mieux les observations est celui qui maximise la vraisemblance, c'est-\u00e0-dire la probabilit\u00e9 que l'\u00e9chantillon provienne de cette loi . L'id\u00e9e est donc de rechercher les \\(\\theta\\) maximisant \\(L(x_1,x_2,...,x_n,\\theta)\\) . Souvent, pour simplifier les calculs, on ne va pas rechercher le maximum de la vraisemblance, mais de la log-vraisemblance : \\(log(L(x_1,x_2,...,x_n,\\theta)) = \\sum_{k=1}^{n} log(f(x_k,\\theta))\\) En effet, rechercher les param\u00e8tres \\(\\theta\\) maximisant \\(L\\) ou \\(logL\\) est \u00e9quivalent, et rechercher un maximum implique un calcul de d\u00e9riv\u00e9e, ce qui est plus simple pour des sommes que pour des produits. On va donc pour chaque param\u00e8tre \\(\\theta_j\\) de \\(\\theta\\) la valeur qui v\u00e9rifie \\(\\frac{\\partial}{\\partial{\\theta_j}} \\sum_{k=1}^{n} log(f(x_k,\\theta)) = 0\\) . Prenons l'exemple de la loi normale : \\(f(x,\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\) On cherchera alors les param\u00e8tres \\(\\theta = (\\mu,\\sigma)\\) v\u00e9rifiant : \\(\\frac{\\partial}{\\partial{\\theta}} \\sum_{k=1}^{n} (-log(\\sigma) - log(\\sqrt{2 \\pi}) - \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) soit \\(\\frac{\\partial}{\\partial{\\theta}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) soit pour chaque param\u00e8tre : \\(\\frac{\\partial}{\\partial{\\mu}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) \\(\\frac{\\partial}{\\partial{\\sigma}} (- n log(\\sigma) - n log(\\sqrt{2 \\pi}) - \\sum_{k=1}^{n} \\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2) = 0\\) On montre alors que les param\u00e8tres v\u00e9rifiant ces \u00e9quations sont : \\(\\mu = \\frac{1}{n} \\sum_{k=1}^{n} x_k\\) et \\(\\sigma^2 = \\frac{1}{n} \\sum_{k=1}^{n} (x_k - \\mu)^2\\) Ce qui \u00e9tait attendu.","title":"Maximum de vraisemblance"},{"location":"Chap2_Classification_supervisee/#implementation-scipy","text":"Afin de r\u00e9aliser un ajustement de loi de probabilit\u00e9, on peut utiliser la biblioth\u00e8que de calculs scientifiques Scipy, et en particulier son module de statistiques \"scipy.stat\". Par exemple, pour un ajustement avec une loi normale, on pourra importer l'objet \"norm\" avec : from scipy.stats import norm On peut alors ajuster une loi normale \u00e0 un ensemble d'observations contenu dans un conteneur x_obs , et r\u00e9cup\u00e9rer la moyenne mu et l'\u00e9cart-type sigma avec : mu,sigma = norm.fit(x_obs) Par d\u00e9faut, la m\u00e9thode du maximum de vraisemblance est utilis\u00e9e. Mais on peut \u00e9galement utiliser la \"m\u00e9thode des moments\" (que nous ne pr\u00e9senterons pas dans ce cours), en ajoutant un param\u00e8tre method = 'MM' en entr\u00e9e. Une fois la loi normale ajust\u00e9e, on a acc\u00e8s \u00e0 la densit\u00e9 de probabilit\u00e9 dp associ\u00e9e \u00e0 une r\u00e9alisation x avec : dp = norm.pdf(x,mu,sigma) Bien d'autres lois de probabilit\u00e9 sont disponibles dans le module \"scipy.stats\", et fonctionnent sur le m\u00eame principe que \"norm\".","title":"Impl\u00e9mentation Scipy"},{"location":"Chap2_Classification_supervisee/#implementation-scikit-learn","text":"Il est \u00e0 noter qu'il existe aussi une impl\u00e9mentation de la classification Bayesienne dans Scikit-Learn, dans l'hypoth\u00e8se de distributions des features suivant des lois normales. Elle peut \u00eatre import\u00e9e avec : from sklearn.naive_bayes import GaussianNB On doit alors cr\u00e9er un objet \"GaussianNB\" qui contiendra notre mod\u00e8le, ici bayes_classifier : bayes_classifier = GaussianNB() Il faut ensuite l'entrainer avec nos features et labels d'entrainement, nomm\u00e9s ici feature_train et label_train : bayes_classifier.fit(feature_train,label_train) Et enfin, on peut r\u00e9aliser une pr\u00e9diction \u00e0 partir de features de test, nomm\u00e9s feature_test : label_test = bayes_classifier.predict(feature_test) Cette impl\u00e9mentation peut \u00eatre pratique dans certains cas, mais elle ne permet pas de jouer sur les hyperparam\u00e8tres suivants : la loi de probabilit\u00e9 et la m\u00e9thode d'ajustement. Une optimisation de ces hyperparam\u00e8tres n'est donc pas possible avec Scikit-Learn.","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap2_Classification_supervisee/#application-a-notre-exemple","text":"Nous allons \u00e0 pr\u00e9sent appliquer la classification Bayesienne \u00e0 notre probl\u00e8me exemple. Afin de rendre la visualisation plus facile, nous allons simplifier le probl\u00e8me : Mettons que nous voulons juste effectuer une classification binaire de nos enregistrements, entre les classes \"flute\" ou \"trompette\", en utilisant comme unique feature l'amplitude relative de la 1\u00e8re harmonique. Pour ce faire, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, et nous s\u00e9lectionnons les variables et les individus qui nous int\u00e9ressent : df_dataset = pd.read_csv(input_path) df_dataset = df_dataset[['instrument','harmo1']] df_dataset = df_dataset[(df_dataset['instrument']=='flute')|(df_dataset['instrument']=='trumpet')] Nous diviserons ici nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 2 DataFrames : df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) On peut alors tracer un histogramme de notre feature pour les donn\u00e9es d'entrainement, en s\u00e9pararant les 2 classes : On peut noter qu'il y a peu de recouvrement entre les 2 distributions, ce qui laisse entrevoir qu'il est possible d'entrainer un mod\u00e8le \u00e0 classifier ces donn\u00e9es. En 1\u00e8re approche, nous choisissons d'ajuster \u00e0 ces 2 distributions des mod\u00e8les de lois normales. Il faudra se poser la question de la pertinence de ce choix. Tout d'abord, nous allons s\u00e9parer le jeu d'entrainement en 2 Series (DataFrame Pandas ne contenant qu'une colonnes) suivant si l'instrument est une flute ou une trompette : sr_harmo1_flute_train = df_train[df_train['instrument']=='flute']['harmo1'] sr_harmo1_trumpet_train = df_train[df_train['instrument']=='trumpet']['harmo1'] On peut alors r\u00e9aliser nos 2 ajustements, et r\u00e9cup\u00e9rer les param\u00e8tres \\(mu\\) et \\(\\sigma\\) correspondants : from scipy.stats import norm mu_harmo1_flute,sig_harmo1_flute = norm.fit(sr_harmo1_flute_train) mu_harmo1_trumpet,sig_harmo1_trumpet = norm.fit(sr_harmo1_trumpet_train) Maintenant que nous avons les param\u00e8tres de nos mod\u00e8les, nous pouvons \u00e9valuer la densit\u00e9 des probabilit\u00e9s conditionnelles pour la classe \"flute\" et la classe \"trompette\". Voici par exemple 301 \u00e9valuations pour des valeurs d'amplitude de la 1\u00e8re harmonique entre -30 et 0 dB : x_axis = np.linspace(-30,0,301) proba_norm_flute = norm.pdf(x_axis,mean_harmo1_flute,sig_harmo1_flute) proba_norm_trumpet = norm.pdf(x_axis,mean_harmo1_trumpet,sig_harmo1_trumpet) Nous pouvons alors tracer les courbes correspondantes par-dessus notre histogramme (affich\u00e9 en densit\u00e9 de probabilit\u00e9) : Si nos mod\u00e8les ne paraissent pas compl\u00e9ment inadapt\u00e9s, on peut noter qu'ils ne capturent pas la l\u00e9g\u00e8re asym\u00e9trie de nos distributions. On pourrait donc se poser la question d'essayer d'autres lois de probabilit\u00e9s, asym\u00e9triques. Continuons avec nos mod\u00e8les pour les probabilit\u00e9s conditionnelles. La prochaine \u00e9tape est d'estimer la probabilit\u00e9 de chaque classe, \u00e0 partir de leurs densit\u00e9s relatives : proba_flute = len(sr_harmo1_flute_train)/len(df_train) proba_trumpet = len(sr_harmo1_trumpet_train)/len(df_train) On peut alors utiliser estimer la densit\u00e9 de probabilit\u00e9 d'observation : proba_obs = proba_norm_flute*proba_flute + proba_norm_trumpet*proba_trumpet Si nous l'affichons avec les histogrammes, nous pouvons v\u00e9rifier qu'elle est bien coh\u00e9rente avec la distribution des observations. Enfin, nous pouvons calculer les probabilit\u00e9s a posteriori, en se basant sur la formule de Bayes : proba_bayes_flute = proba_norm_flute*proba_flute/proba_obs proba_bayes_trumpet = proba_norm_trumpet*proba_trumpet/proba_obs On peut alors afficher ces probabilit\u00e9s, et tracer la fronti\u00e8re de d\u00e9cision : La fronti\u00e8re de d\u00e9cision se trouve \u00e0 environ -13.16 dB : Si on mesure une 1\u00e8re harmonique ayant une amplitude inf\u00e9rieure, on classifiera l'instrument comme \u00e9tant une flute. Si on mesure une 1\u00e8re harmonique ayant une amplitude sup\u00e9rieure, on classifiera l'instrument comme \u00e9tant une trompette. Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9demment, si la probabilit\u00e9 d'appartenance aux classes ne nous int\u00e9resse pas, nous pourrions juste comparer \\(p(x \\mid C = 'flute')p(C='flute')\\) et \\(p(x \\mid C = 'trumpet')p(C='trumpet')\\) pour classifier les observations. Maintenant que nous avons vu le principe, on voudrait pouvoir r\u00e9-entrainer notre mod\u00e8le afin d'optimiser les hyperparam\u00e8tres, et r\u00e9aliser des pr\u00e9dictions sur les jeux d'entrainement et de test, le tout de mani\u00e8re efficace. Dans ce but, nous pouvons mettre notre classification binaire Bayesienne sous la forme d'une classe binary_bayes avec 2 m\u00e9thodes train et predict pour l'entrainement et la pr\u00e9diction. Voici un exemple d'impl\u00e9mentation : class binary_bayes: def __init__(self,stat_model): self.stat_model = stat_model self.true_params = None self.false_params = None self.proba_true = None def train(self,x_true,x_false): self.true_params = self.stat_model.fit(x_true) self.false_params = self.stat_model.fit(x_false) len_true = len(x_true) len_false = len(x_false) self.proba_true = len_true/(len_true+len_false) def predict(self,x): proba_norm_true = self.stat_model.pdf(x,*self.true_params) proba_norm_false = self.stat_model.pdf(x,*self.false_params) proba_obs = proba_norm_true*self.proba_true + proba_norm_false*(1-self.proba_true) proba_bayes_true = proba_norm_true*self.proba_true/proba_obs return proba_bayes_true On peut alors facilement d\u00e9finir un classifieur binaire \"est-ce une flute ?\" utilisant la loi normale telle qu'impl\u00e9ment\u00e9e par Scipy : from scipy.stats import norm is_a_flute = binary_bayes(norm) Entrainer ce classifieur sur notre jeu d'entrainement : is_a_flute.train(sr_harmo1_flute_train,sr_harmo1_trumpet_train) Et r\u00e9aliser des pr\u00e9dictions sur nos donn\u00e9es d'entrainement et de test : prediction_train = (is_a_flute.predict(df_train['harmo1'])) prediction_test = (is_a_flute.predict(df_test['harmo1'])) En partant du principe que nous positionnons la fronti\u00e8re de d\u00e9cision \u00e0 une probabilit\u00e9 d'appartenance \u00e0 classe \"flute\" de 0.5, nous pouvons obtenir les matrices de confusion en entrainement et en test avec les commandes suivantes : from sklearn.metrics import confusion_matrix #Label encoding: ground_truth_train = (df_train['instrument']=='flute').astype(int) ground_truth_test = (df_test['instrument']=='flute').astype(int) cm_train = confusion_matrix(ground_truth_train, prediction_train>0.5) cm_test = confusion_matrix(ground_truth_test, prediction_test>0.5) Voici les r\u00e9sultats en entrainement obtenus pour notre exemple : On observe que les performances du mod\u00e8le sont tr\u00e8s similaires entre les donn\u00e9es d'entrainement et de test. Ceci tend \u00e0 montrer que l'on a pas de probl\u00e8me de sur-ajustement important, ce qui laisse pr\u00e9sager des performances similaires en g\u00e9n\u00e9ralisation. Il n'y a aucun faux positif, mais on a quelques faux n\u00e9gatifs : parfois notre mod\u00e8le classifie des enregistrements de flutes comme n'\u00e9tant pas des flutes. Suivant les applications, on peut vouloir choisir une fronti\u00e8re de d\u00e9cision diff\u00e9rente, pour diminuer le nombre de faux n\u00e9gatifs, au prix d'une augmentation du nombre de faux positifs. Afin de voir les effets d'un tel choix, on tracer une courbe ROC \u00e0 partir des probabilit\u00e9s pr\u00e9dites par notre mod\u00e8le : from sklearn.metrics import roc_curve fp_rate_train, tp_rate_train, thresholds_train = roc_curve(ground_truth_train, prediction_train) fp_rate_test, tp_rate_test, thresholds_test = roc_curve(ground_truth_test, prediction_test)","title":"Application \u00e0 notre exemple"},{"location":"Chap2_Classification_supervisee/#remarques","text":"La m\u00e9thode de la classification de Bayes a les avantages suivants : Elle fonctionne pour tous les types de classification et variables . Elle est relativement simple \u00e0 mettre en place, avec peu de param\u00e8tres . Les d\u00e9cisions qu'elle prend sont compl\u00e8tement expliqu\u00e9es et interpr\u00e9tables : un humain peut les comprendre. Mais cette m\u00e9thode a aussi les limites suivantes : Elle fait l'hypoth\u00e8se de l' ind\u00e9pendance des variables entre elles, ce qui dans la pratique limite son application aux probl\u00e8mes avec peu de features. Elle fait une hypoth\u00e8se forte sur la distribution des observations pour chaque variable. Il s'agit souvent d'une hypoth\u00e8se de normalit\u00e9 .","title":"Remarques"},{"location":"Chap2_Classification_supervisee/#k-plus-proches-voisins","text":"","title":"K Plus Proches Voisins"},{"location":"Chap2_Classification_supervisee/#principe_1","text":"La m\u00e9thode de la classification Bayesienne que nous venons de voir avait pour d\u00e9savantage de n\u00e9cessiter une hypoth\u00e8se sur la distribution des observations. Dans cette section, nous allons pr\u00e9senter une m\u00e9thode ne n\u00e9cessitant aucun a priori sur les donn\u00e9es : les K Plus Proches Voisin , aussi connue sous l'acronyme KPPV. Les KPPV est une m\u00e9thode dite de \"lazy learning\" : il n'y a pas de r\u00e9el apprentissage pr\u00e9alable \u00e0 la pr\u00e9diction. Le jeu de donn\u00e9es d'apprentissage est stock\u00e9 en m\u00e9moire , et utilis\u00e9 au moment de la pr\u00e9diction. L'id\u00e9e est la suivante : pour classer un nouvel individu, on va calculer sa distance aux \\(k\\) individus les plus proches dans les donn\u00e9es d'entrainement. On attibura alors \u00e0 l'individu la classe la plus repr\u00e9sent\u00e9e parmi ses \\(k\\) \"plus proches voisins\". Pr\u00e9dire la classe d'un individu avec cette m\u00e9thode implique : (1) De mesurer la distance entre l'individu \u00e0 classifier et tous les individus du jeu d'entrainement . C'est ce que l'on appelle \"l'approche brute\". Et plus le jeu d'entrainement est grand, plus le temps de calcul sera long. Pour cette raison, on choisi de stocker le jeu d'entrainement dans une structure de donn\u00e9e la plus efficace \u00e0 parcourir possible (exemple : KD-Tree). (2) De choisir une mesure de distance adapt\u00e9e au probl\u00e8me. (3) De choisir le nombre de \"plus proches voisins\" \u00e0 l'individu \u00e0 consid\u00e9rer. (4) De choisir de quelle mani\u00e8re on va affecter une classe \u00e0 l'individu \u00e0 partir de la classe de ses voisins : Un vote majoritaire ? S'il y a un gros d\u00e9s\u00e9quilibre entre classes, ce type de vote risque d'\u00eatre biais\u00e9. On pr\u00e9f\u00e9rera alors un vote avec des poids diff\u00e9rents suivant les classes.","title":"Principe"},{"location":"Chap2_Classification_supervisee/#choix-de-la-distance","text":"Suivant le probl\u00e8me de classification auquel on est confront\u00e9, la \"distance\" entre 2 individus n'a pas le m\u00eame sens. En effet, on comprend bien qu'on utilisera pas les m\u00eames crit\u00e8res pour mesurer la distance entre 2 valeurs r\u00e9elles, entre 2 images, ou entre 2 mots du dictionnaire. D'o\u00f9 l'importance lorsqu'on utilise les KPPV de choisir une mesure de distance pertinente pour notre probl\u00e8me. Parmi les mesures de distances classiques, on peut citer : Distance Euclidienne : Si on veut mesurer la distance Euclidienne entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\) Il s'agit de la mesure de distance la plus connue et la plus utilis\u00e9e. Elle fonctionne bien lorsque l'on est confront\u00e9s \u00e0 des valeurs r\u00e9elles continues, normalis\u00e9es, et avec une dimensionnalit\u00e9 faible. Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme 2. Distance de Manhattan : Si on veut mesurer la distance de Manhattan entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = \\sum_{i=1}^{n} \\mid x_i - y_i \\mid\\) Suivant l'espace des features de notre probl\u00e8me, tracer une \"ligne droite\" entre individus peut ne pas avoir de sens. La distance de Manhattan est alors une alternative \u00e0 la distance Euclidienne. Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme 1. Distance de Chebychev : Si on veut mesurer la distance de Chebychev entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = max(\\mid x_i - y_i \\mid)\\) La distance de Chebychev est assez peu utilis\u00e9e, car elle a des cas d'applications tr\u00e8s sp\u00e9cifiques. (Par exemple, les d\u00e9placements d'un roi sur un jeu d'\u00e9chec ou les automates cellulaires). Cette distance peut \u00eatre vue comme la mesure de distance associ\u00e9e \u00e0 la norme infinie. Minkowski : Si on veut mesurer la distance de Minkowski entre \\(x\\) et \\(y\\) , 2 vecteurs de dimension \\(n\\) : \\(D(x,y) = (\\sum_{i=1}^{n} \\mid x_i - y_i \\mid^p)^{1/p}\\) La distance de Minkowski est une g\u00e9n\u00e9ralisation des 3 distances pr\u00e9c\u00e9dentes. En effet, on remarque que si \\(p=1\\) elle revient \u00e0 la distance de Manhattan, si \\(p=2\\) elle revient \u00e0 la distance Euclidienne, et si \\(p\\) tend vers l'infini elle revient \u00e0 la distance de Chebychev. Elle permet donc de chercher un compromis entre ces diff\u00e9rentes distances. Hamming : Soit 2 chaines de caract\u00e8res de m\u00eame taille. La distance de Hamming entre ces 2 chaines est alors \u00e9gale au nombre de positions pour lesquelles les caract\u00e8res sont diff\u00e9rents. Cette mesure de distance est couramment utilis\u00e9e lorsque l'on veut comparer des morceaux de textes caract\u00e8re par caract\u00e8re, ou de mani\u00e8re g\u00e9n\u00e9rale pour des donn\u00e9es qualitatives. Similarit\u00e9 cosinus : Si on veut mesurer la \"similarit\u00e9 cosinus\" entre 2 vecteurs \\(x\\) et \\(y\\) : \\(D(x,y) = cos(\\theta) = \\frac{x.y}{\\|x\\| \\|y\\|}\\) Il s'agit du cosinus de l'angle entre les 2 vecteurs. Cette mesure de distance est couramment utilis\u00e9e lorsque l'on doit comparer des vecteurs de haute dimensionnalit\u00e9, et o\u00f9 la norme du vecteur a peu d'importance. Par exemple, c'est la mesure de distance privil\u00e9gi\u00e9e pour de la \"fouille de texte\" (comparaison mot \u00e0 mot de chaines de caract\u00e8re). La distance est donc un hyperparam\u00e8tre \u00e0 optimiser lorsque l'on utilise les KPPV.","title":"Choix de la distance"},{"location":"Chap2_Classification_supervisee/#choix-du-parametre-k","text":"Il est \u00e9vident que le choix de \\(k\\) va avoir un impact sur les pr\u00e9dictions obtenues \u00e0 partir des donn\u00e9es d'entrainement. C'est donc \u00e9galement un hyperparam\u00e8tre \u00e0 optimiser . Pour choisir des valeurs de \\(k\\) \u00e0 tester, on peut partir des grands principes suivants : S'il y a un fort d\u00e9s\u00e9quilibre entre classes, il vaut mieux ne pas choisir un \\(k\\) faible . S'il y a beaucoup de recouvrement entre les classes, il vaut mieux choisir un \\(k\\) \u00e9lev\u00e9 . Avec un \\(k\\) trop faible on risque le sur-apprentissage . Avec un \\(k\\) trop grand on risque le sous-apprentissage . Pour \u00e9viter les cas d'\u00e9galit\u00e9, on va en g\u00e9n\u00e9ral choisir une valeur de \\(k\\) impaire.","title":"Choix du param\u00e8tre K"},{"location":"Chap2_Classification_supervisee/#implementation-scikit-learn_1","text":"Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode des KPPV. Elle peut \u00eatre import\u00e9e avec : from sklearn.neighbors import KNeighborsClassifier On peut ensuite initialiser un classifieur KPPV knn avec un objet \"KNeighborsClassifier\" de param\u00e8tre k correspondant au nombre de plus proches voisins : knn = KNeighborsClassifier(n_neighbors=k) Pour donner le jeu d'entrainement (features avec feature_train et labels avec label_train ) \u00e0 ce classifieur, on utilise la m\u00e9thode : knn.fit(feature_train,label_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions label_test \u00e0 partir de features de test feature_test : label_test = knn.predict(feature_test) Si on veut effectuer un test de notre classifieur sur un jeu de donn\u00e9es lab\u00e9liser, on peut obtenir un score d'exactitude avec la commande : knn.score(feature_test,label_test)","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap2_Classification_supervisee/#outils-de-visualisation-mlxtend","text":"Pour afficher les fronti\u00e8res de d\u00e9cision donn\u00e9es par un classifieur dans un cas 1D ou 2D, il existe une fonction de la biblioth\u00e8que \"MLxtend\". Une fois la biblioth\u00e8que install\u00e9e, vous pouvez importer la fonction avec : from mlxtend.plotting import plot_decision_regions Pour afficher les fronti\u00e8res de d\u00e9cision d'une classifieur model , avec les donn\u00e9es d'entrainement feature_train et label_train , on utilisera la m\u00e9thode : plot_decision_regions(feature_train, label_train, clf=model) Pour un probl\u00e8me de dimensionnalit\u00e9 plus \u00e9lev\u00e9e que 2, la visualisation des fronti\u00e8res de d\u00e9cision est toujours difficile.","title":"Outils de visualisation MLxtend"},{"location":"Chap2_Classification_supervisee/#application-a-notre-exemple_1","text":"Nous allons \u00e0 pr\u00e9sent appliquer les KPPV \u00e0 notre probl\u00e8me exemple. Afin de rendre la visualisation plus facile, nous allons simplifier le probl\u00e8me : Mettons que nous voulons effectuer une classification de nos enregistrements entre les classes \"flute\", \"hautbois\" ou \"trompette\", en utilisant en features l'amplitude relative de la 1\u00e8re harmonique et de la 2\u00e8me harmonique. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) M\u00eame si en th\u00e9orie les KPPV n'ont pas besoin de labels num\u00e9riques pour fonctionner, certaines des fonctions que nous utiliserons dans la suite ne fonctionnent qu'avec des valeurs num\u00e9riques. Nous allons donc encoder les labels \"par \u00e9tiquette\" : from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() df_dataset['instrument'] = encoder.fit_transform(df_dataset['instrument']) Comme les KPPV se contentent de calculer des distances sur les features, on peut utiliser un encodage par \u00e9tiquette malgr\u00e9 le fait que nos labels ne sont pas ordinaux. Attention : pour les m\u00e9thodes utilisant une fonction de co\u00fbt sur les labels, un encodage \"one-hot\" devra \u00eatre utilis\u00e9 ! Nous r\u00e9cup\u00e9rons ensuite les features et les labels que nous allons utiliser dans 2 DataFrames : df_features = df_dataset[['harmo1','harmo2']] df_labels = df_dataset['instrument'] Nous s\u00e9parons ensuite nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 4 DataFrames (2 pour les features, 2 pour les labels) : from sklearn.model_selection import train_test_split df_features_train, df_features_test, df_labels_train, df_labels_test = train_test_split(df_features,df_labels,test_size=0.2,random_state=0) Enfin, nous r\u00e9alisons une transformation \"centrage-r\u00e9duction\" (voir Chapitre 1), pour s'assurer que les 2 features \u00e9voluent sur des intervalles comparables. Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_features_train) df_features_train[['harmo1','harmo2']] = scaler.transform(df_features_train) df_features_test[['harmo1','harmo2']] = scaler.transform(df_features_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre classifieur. Voici comment initialiser un classifieur KPPV avec \\(k=3\\) : from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=3) Pour lui fournir les donn\u00e9es d'entrainement, comme vu pr\u00e9c\u00e9demment, il nous suffit d'utiliser la commande suivante : knn.fit(df_features_train,df_labels_train) Nous pouvons \u00e0 pr\u00e9sent utiliser notre mod\u00e8le pour classifier des donn\u00e9es. Tout d'abord, nous allons \u00e9valuer les performances de notre mod\u00e8le en entrainement et en test. On peut d\u00e9j\u00e0 mesurer l'exactitude de notre classifieur sur ces 2 jeux de donn\u00e9es : print(knn.score(df_features_train,df_labels_train)) print(knn.score(df_features_test,df_labels_test)) Pour \\(k=3\\) , on obtient plus de 99.4% d'exactitude en entrainement, et environ 98.8% d'exactitude en test. Ces scores laissent \u00e0 penser que notre mod\u00e8le aura de plut\u00f4t bonnes performances en g\u00e9n\u00e9ralisation. Mais n'oublions pas que l'exactitude peut \u00eatre biais\u00e9e en cas de d\u00e9s\u00e9quilibre entre classes. Dans de tels cas, d'autres indicateurs doivent \u00eatre utilis\u00e9s. Voici une matrice de confusion compl\u00e8te pour nous aider \u00e0 conclure : On observe qu'\u00e0 l'entrainement comme en test, le hautbois est tr\u00e8s bien s\u00e9par\u00e9 des autres instruments, alors que la trompette et la flute sont parfois confondus. Ce r\u00e9sultat \u00e9tait pr\u00e9visible au vu de la matrice de nuages de points que nous avions obtenue lors de notre \u00e9tude pr\u00e9liminaire. Les proportions d'erreurs restent cependant relativement faibles compar\u00e9es aux nombres d'observations. Nous n'avons pour l'instant test\u00e9 qu'une valeur de \\(k\\) . Pour visualiser l'effet de cet hyperparam\u00e8tre, nous pouvons utiliser les affichages graphiques de la biblioth\u00e8que MLxtend. Voici les graphiques obtenus pour \\(k=3\\) (volontairement faible) et \\(k=31\\) (volontairement \u00e9lev\u00e9) : On peut noter que comme attendu, le choix de \\(k\\) a le plus d'effet \u00e0 la fronti\u00e8re entre \"flute\" et \"trompette\". En effet, comme il y a du recouvrement entre ces 2 classes, on sait qu'il vaut mieux choisir un \\(k\\) \u00e9lev\u00e9 pour \u00e9viter le sur-apprentissage. Ceci est confirm\u00e9 par le \"lissage\" de la fronti\u00e8re de d\u00e9cision lorsque l'on utilise \\(k=31\\) . Le choix d'un \\(k\\) \u00e9lev\u00e9 a donc l'air plus appropri\u00e9 ici, mais il faudrait r\u00e9aliser une r\u00e9elle optimisation de cet hyperparam\u00e8tre. Par d\u00e9faut, la distance utilis\u00e9e par l'impl\u00e9mentation Scikit-Learn des KPPV est la distance Euclidienne. Nous pouvons \u00e9galement r\u00e9aliser des affichages pour visualiser l'impact de diff\u00e9rentes distances pour une m\u00eame valeur de \\(k\\) . Voici le r\u00e9sultat pour la distance Euclidienne et la distance de Manhattan, avec \\(k=31\\) . On observe en effet que le choix de la distance impacte significativement les fronti\u00e8res de d\u00e9cision obtenues, m\u00eame s'il est difficile ici de juger de la pertinence d'une des 2 distances essay\u00e9es. Tout comme pour \\(k\\) , il faudrait r\u00e9aliser une v\u00e9ritable optimisation de cet hyperparam\u00e8tre.","title":"Application \u00e0 notre exemple"},{"location":"Chap2_Classification_supervisee/#remarques_1","text":"La m\u00e9thode des KPPV a les avantages suivants : Il s'agit d'une m\u00e9thode non-param\u00e9trique, qui ne fait aucune hypoth\u00e8se sur la structure des donn\u00e9es. Elle est relativement simple, et n'a que 2 hyperparam\u00e8tres ( \\(k\\) et la distance), ce qui est peu compar\u00e9 \u00e0 certaines m\u00e9thodes. Si de nouvelles observations doivent \u00eatre ajout\u00e9es au jeu d'entrainement, la mise \u00e0 jour du mod\u00e8le est directe . Mais cette m\u00e9thode a aussi les limites suivantes : Le mod\u00e8le ayant besoin de stocker les donn\u00e9es d'entrainement, il peut vite devenir tr\u00e8s lourd . Elle fonctionne mal avec des donn\u00e9es de grande dimension . Elle est tr\u00e8s sujette au sur-apprentissage .","title":"Remarques"},{"location":"Chap2_Classification_supervisee/#perceptron-multicouche","text":"La m\u00e9thode de classification que nous allons voir \u00e0 pr\u00e9sent est une ouverture vers les r\u00e9seaux de neurones artificiels , et la discipline qui leur est associ\u00e9e : l' apprentissag profond (ou \"Deep Learning\" en anglais).","title":"Perceptron multicouche"},{"location":"Chap2_Classification_supervisee/#perceptron-un-neurone-artificiel","text":"Lorsque l'on parle d'apprentissage pour un humain, on pense tout de suite \u00e0 son cerveau, et plus particuli\u00e8rement \u00e0 ses neurones . Un neurone est en effet une machine \u00e0 apprendre : Il prend plusieurs signaux \u00e9lectriques en entr\u00e9e, donne plus ou moins d'importance \u00e0 chacun, et transmet ou non un signal \u00e9lectrique en sortie en fonction de ces entr\u00e9es pond\u00e9r\u00e9es avec un seuil. Le neurone apprendra les poids \u00e0 donner \u00e0 chaque entr\u00e9e pour fournir une sortie pertinente pour une application. Un neurone se comporte donc comme un classifieur binaire . D'o\u00f9 l'id\u00e9e s\u00e9duisante de s'inspirer des neurones pour l'apprentissage de ce type de mod\u00e8le. Le 1er mod\u00e8le math\u00e9matique d'un neurone, appel\u00e9 \"neurone formel\" ou \"neurone artificiel\", est propos\u00e9 par McCulloch et Pitts en 1943. Dans le cadre de l'apprentissage automatique, il est plus connu sous le nom de \" perceptron \", concept invent\u00e9 par Rosenblatt en 1957. Voici le principe du perceptron \"historique\" de 1957 : Les \\(p\\) r\u00e9alisations de nos \\(p\\) features correspondant \u00e0 un individu sont fournies comme \\(p\\) entr\u00e9es \u00e0 notre neurone. Une entr\u00e9e suppl\u00e9mentaire toujours fix\u00e9e \u00e0 1 sera \u00e9galement fournie, on la nommera \" biais \". On applique \u00e0 chaque entr\u00e9e \\(x_i\\) un coefficient \\(w_i\\) . C'est ce que l'on appellera les param\u00e8tres du mod\u00e8le. Toutes les entr\u00e9es \\(x_i\\) pond\u00e9r\u00e9es par \\(w_i\\) sont somm\u00e9es , donnant la combinaison lin\u00e9aire \\(w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p\\) . Un seuil est finalement appliqu\u00e9 \u00e0 cette somme : suivant si elle d\u00e9passe ou non une certaine valeur, la sortie sera soit 0 ou 1, soit -1 ou 1 suivant la fonction de seuil choisie. L'apprentissage de ce mod\u00e8le consistera en l' optimisation des param\u00e8tres , afin qu'\u00e0 partir des features il soit capable d'associer ou non l'individu \u00e0 une classe (0 ou 1 en sortie). On utilisera le processus d'entrainement suivant : On initialise les param\u00e8tres al\u00e9atoirement. On fait passer un \u00e0 un les individus du jeu d'entrainement \u00e0 travers le mod\u00e8le. Pour chaque individu on compare la sortie du mod\u00e8le \u00e0 celle attendue, et on met \u00e0 jour les param\u00e8tres en cons\u00e9quence. On rep\u00e8te les 2 \u00e9tapes pr\u00e9c\u00e9dentes jusqu'\u00e0 convergence. On appelle la fonction apprise \\(f(x_1,x_2,...,x_p) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p\\) la fonction discriminante . Comme cette fonction est lin\u00e9aire, le mod\u00e8le ne pourra \u00e9tablir que des fronti\u00e8res de d\u00e9cision lin\u00e9aires (un point en 1D, une droite en 2D, un plan en 3D, un hyperplan dans le cas g\u00e9n\u00e9ral). Nous verrons que ceci est assez limitant en pratique : tous les probl\u00e8mes ne sont pas lin\u00e9airement s\u00e9parables ! Voici une repr\u00e9sentation sch\u00e9matique d'une fronti\u00e8re de d\u00e9cision 2D : Comme nous l'avons expliqu\u00e9 pr\u00e9c\u00e9demment, on peut r\u00e9aliser de la classification multi-classe \u00e0 partir de plusieurs classifieurs binaires, avec une strat\u00e9gie One-versus-All ou One-Versus-One. Pour un perceptron, il suffira donc d'utiliser plusieurs neurones en parall\u00e8le avec les m\u00eames entr\u00e9es. Voici un exemple de \\(n\\) perceptrons \\(N\\) en parall\u00e8le pour \\(n\\) classes : Pour pouvoir entrainer un perceptron, il reste \u00e0 choisir une m\u00e9thode pour mettre \u00e0 jour les param\u00e8tres du mod\u00e8le \u00e0 partir des erreurs de pr\u00e9diction. La \"r\u00e8gle d'apprentissage du perceptron\" propos\u00e9e par Rosenblatt est la suivante. A l'it\u00e9ration \\(k\\) , pour le \\(i\\) -\u00e8me param\u00e8tre, on applique : \\(w_i^{(k+1)} = w_i^{(k)} - \\gamma (y^{(k)}-\\hat{y}^{(k)})\\) avec \\(y\\) la sortie attendue, et \\(\\hat{y}\\) la pr\u00e9diction. On reconnait une m\u00e9thode type descente de gradient (voir Chapitre 1), mais tr\u00e8s simple. Ici, on ne calcule pas le gradient de la fonction de co\u00fbt \u00e0 partir de la totalit\u00e9 du jeu d'entrainement, mais \u00e0 partir d'un seul individu. Ce qui rend cet algorithme d'optimisation rapide, mais aussi sensible aux minima locaux. N\u00e9anmoins, on peut montrer que si les classes sont lin\u00e9airement s\u00e9parables alors la m\u00e9thode convergence forc\u00e9ment . C'est le \"th\u00e9or\u00e8me de convergence du perceptron\". Par contre, dans les cas non-lin\u00e9airement s\u00e9parables , cette m\u00e9thode ne donnera pas de r\u00e9sultats satisfaisants .","title":"Perceptron : un neurone artificiel"},{"location":"Chap2_Classification_supervisee/#perceptron-multicouche-un-reseau-de-neurones-artificiels","text":"Le perceptron est une 1\u00e8re approche simple \u00e0 mettre en place et \u00e0 entrainer. Mais il n'est applicable en pratique que pour les probl\u00e8mes lin\u00e9airement s\u00e9parables. C'est pourquoi dans les ann\u00e9es 1960, a \u00e9merg\u00e9 l'id\u00e9e de relier plusieurs perceptron en sens direct. On appellera ce type de r\u00e9seau de neurones un perceptron multicouche (PMC). L'id\u00e9e est la suivante : si une combinaison lin\u00e9aire avec un seuil ne peut produire que des fronti\u00e8res de d\u00e9cisions lin\u00e9aires, une combinaison de s\u00e9parateurs lin\u00e9aires avec chacun un seuil peut donner un s\u00e9parateur non-lin\u00e9aire . Le PMC le plus basique poss\u00e8de 3 couches totalement connect\u00e9es : La couche d'entr\u00e9e : il s'agit simplement des diff\u00e9rentes features d'entr\u00e9e du mod\u00e8le. La couche cach\u00e9e : une couche de neurones (perceptrons), chacun ayant ses param\u00e8tres, son biais, son seuil. Elle est \"cach\u00e9e\" car ses sorties sont invisibles pour l'utilisateur. La couche de sortie : une couche de neurone en fin de r\u00e9seau, chacun ayant ses param\u00e8tres, son biais, son seuil, qui va renvoyer la sortie du mod\u00e8le pour chaque classe. On peut d\u00e9montrer que ce mod\u00e8le est capable de tracer n'importe quelle fronti\u00e8re de d\u00e9cision , \u00e0 condition d'avoir assez de neurones dans la couche cach\u00e9e. C'est ce que l'on appelle le th\u00e9or\u00e8me de l'approximation universelle . Le probl\u00e8me est que plus la fronti\u00e8re de d\u00e9cision \u00e0 tracer est complexe, et plus il faut de neurones dans la couche limite. Pour r\u00e9soudre ce probl\u00e8me, on peut ajouter plusieurs couches cach\u00e9es entre les couches d'entr\u00e9e et de sortie : pour un nombre de neurones par couche donn\u00e9, plus on aura de couches, et plus complexes les fronti\u00e8res de d\u00e9cisions pourront \u00eatre. D\u00e8s que l'on a plus d'une couche cach\u00e9e, on parle d' apprentissage profond (\"Deep Learning\"). Nous comprenons bien le potentiel du PMC... \u00e0 condition de pouvoir l'entrainer ! En effet, comment r\u00e9ussir \u00e0 optimiser efficacement les param\u00e8tres de nos diff\u00e9rentes couches, sachant que les couches sont totalement connect\u00e9es ? Ce probl\u00e8me est rest\u00e9 un point de blocage jusqu'en 1985, gr\u00e2ce aux travaux de Rumlhart et son \u00e9quipe. La m\u00e9thode qu'ils ont propos\u00e9e pour entrainer un PMC, encore utilis\u00e9e aujourd'hui, est connue sous le nom de r\u00e9tropropagation du gradient .","title":"Perceptron multicouche : un r\u00e9seau de neurones artificiels"},{"location":"Chap2_Classification_supervisee/#retropropagation-du-gradient","text":"Voici le principe sur lequel fonctionne l'algorithme de la propagation du gradient : Algorithme de retropropagation du gradient Les param\u00e8tres du mod\u00e8le sont initialis\u00e9s al\u00e9atoirement. A chaque it\u00e9ration (\u00e9poque) de l'algorithme : - Un \u00e9chantillon d'individus est s\u00e9lectionn\u00e9. - Passage direct : cet \u00e9chantillon est fourni en entr\u00e9e du mod\u00e8le, et la sortie est r\u00e9cup\u00e9r\u00e9e. - Une fonction de co\u00fbt est utilis\u00e9e pour \u00e9valuer l'erreur entre la sortie du mod\u00e8le et ce qui \u00e9tait attendu. - Passage inverse : le gradient de l'erreur associ\u00e9e \u00e0 chaque param\u00e8tre est calcul\u00e9 en \u00e9valuant la contribution de chaque param\u00e8tre \u00e0 l'erreur, en allant de la sortie vers l'entr\u00e9e. - Ces gradients sont fournis \u00e0 l'algorithme de descente de gradient (voir Chapitre 1), qui va mettre \u00e0 jour les param\u00e8tres du mod\u00e8le. On en d\u00e9duit que plus le nombre de couches cach\u00e9e et de neurones par couche cach\u00e9e sera \u00e9lev\u00e9, plus l'apprentissage sera long. Aussi, un nombre de couches trop \u00e9lev\u00e9 peut rendre difficile l'apprentissage des couches les plus en amont : un ph\u00e9nom\u00e8ne connu sous le nom de \"probl\u00e8me de la disparition du grandient\". Pour que cet algorithme fonctionne avec le PMC, il a fallu modifier la fonction de sortie des neurones. En effet, la fonction \"seuil\" ne permet pas de calculer un gradient : elle a un probl\u00e8me de diff\u00e9rentiabilit\u00e9, et une pente nulle partout sauf en un point. C'est pourquoi d'autres \" fonctions d'activation \" sont utilis\u00e9es en sortie des neurones d'un PMC. On peut citer les 3 plus utilis\u00e9es : Sigmo\u00efde : \\(g(u) = \\frac{1}{1+exp(-u)}\\) Il s'agit de la fonction historique, directement inspir\u00e9e de la fonction d'activation d'un neurone biologique. Elle a l'avantage d'\u00eatre continue et \u00e0 d\u00e9riv\u00e9e non nulle partout. Elle retourne un score entre 0 et 1. Tangente hyperbolique : \\(g(u) = tanh(u)\\) Tout comme la sigmo\u00efde, elle a une forme proche de la fonction d'activation d'un neurone biologique. Elle est \u00e9galement continue et \u00e0 d\u00e9riv\u00e9e non nulle partout. Elle retourne un score entre -1 et 1. ReLU : \\(g(u) = max(0,u)\\) Tr\u00e8s utilis\u00e9e depuis les ann\u00e9es 2010, cette fonction n'a aucune inspiration biologique. Elle a un probl\u00e8me de d\u00e9rivabilit\u00e9 en 0, et elle ne retourne pas un score born\u00e9. Par contre, elle permet de lutter contre les probl\u00e8me de \"disparition du gradient\". Lorsque l'on veut r\u00e9soudre un probl\u00e8me de classification multi-classe (mais pas multi-sorties) avec un PMC, au lieu d'une approche One-versus-All ou One-versus-One, on peut utiliser la fonction d'activation suivante : Softmax : pour chaque neurone de sortie \\(i\\) on calcule \\(g(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{n} e^{y_j}}\\) et on retient la classe correspondant \u00e0 la sortie maximisant cette fonction. L'id\u00e9e est que cette fonction prend en entr\u00e9e les scores renvoy\u00e9s par chaque neurone de sortie, et les transforme en probabilit\u00e9 d'appartenance \u00e0 chaque classe (la somme donnant 1). Nota Bene Pour que l'apprentissage d'un perceptron ou d'un PMC se d\u00e9roule correctement, il est recommand\u00e9 d'effectuer une transformation des donn\u00e9es d'entr\u00e9e (voir Chapitre 1), afin d'\u00e9viter que le mod\u00e8le donne artificiellement plus de poids \u00e0 une feature juste parce qu'elle varie sur une plus grande plage de valeurs.","title":"Retropropagation du gradient"},{"location":"Chap2_Classification_supervisee/#choix-des-hyperparametres","text":"Une des difficult\u00e9 de l'apprentissage d'un PMC est le nombre \u00e9lev\u00e9 de param\u00e8tres et d'hyperparam\u00e8tres \u00e0 optimiser. Ceci rend le PMC particuli\u00e8rement sensible au sur-apprentissage . Une strat\u00e9gie de validation par exclusion ou de validation crois\u00e9e est donc plus que recommand\u00e9e ! Parmi les hyperparam\u00e8tres \u00e0 optimiser, on peut citer : Le nombre de couches cach\u00e9es. Le nombre de neurones par couche cach\u00e9e. La fen\u00eatre d'activation pour les couches cach\u00e9es. La fen\u00eatre d'activation pour la couche de sortie. Le taux d'apprentissage pour la descente de gradient. Le nombre maximum d'\u00e9poques d'apprentissage, et pratiquer ou non de l'arr\u00eat pr\u00e9matur\u00e9. La fonction de co\u00fbt utilis\u00e9e pour l'apprentissage. De plus, l'initialisation des param\u00e8tre du mod\u00e8le se faisant de mani\u00e8re al\u00e9atoire, 2 apprentissages ne donneront pas le m\u00eame mod\u00e8le. Il convient donc de tester plusieurs initialisations.","title":"Choix des hyperparam\u00e8tres"},{"location":"Chap2_Classification_supervisee/#implementation-scikit-learn_2","text":"Il existe une impl\u00e9mentation Scikit-Learn du PMC pour la classification. Elle peut \u00eatre import\u00e9e avec : from sklearn.neural_network import MLPClassifier On peut ensuite initialiser un classifieur PMC mlp avec les hyperparam\u00e8tres par d\u00e9faut en utilisant la commande : mlp = MLPClassifier() Nous verrons plus loin quels sont ces hyperparam\u00e8tres. Pour donner le jeu d'entrainement (features avec feature_train et labels avec label_train ) \u00e0 ce classifieur, on utilise la m\u00e9thode : mlp.fit(feature_train,label_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions label_test \u00e0 partir de features de test feature_test : label_test = mlp.predict(feature_test) Si on veut effectuer un test de notre classifieur sur un jeu de donn\u00e9es lab\u00e9liser, on peut obtenir un score d'exactitude avec la commande : mlp.score(feature_test,label_test) Voici les hyperparam\u00e8tres par d\u00e9faut de l'impl\u00e9mentation Scikit-Learn du PMC : Nombre de couches cach\u00e9es : 1 Nombre de neurones par couche cach\u00e9e : 100 La fen\u00eatre d'activation pour les couches cach\u00e9es : ReLU La fen\u00eatre d'activation pour la couche de sortie : Sigmo\u00efde si binaire, Softmax si multi-classe (non modifiable) Le taux d'apprentissage pour la descente de gradient : 0.001 Le nombre maximum d'\u00e9poques d'apprentissage : 200 (par d\u00e9faut sans arr\u00eat pr\u00e9matur\u00e9, mais on peut l'activer) La fonction de co\u00fbt : Log-loss (non modifiable) Ces hyperparam\u00e8tres sont presque tous modifiables par l'utilisateur.","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap2_Classification_supervisee/#application-a-notre-exemple_2","text":"Nous allons \u00e0 pr\u00e9sent appliquer le PMC par d\u00e9faut de Scikit-Learn \u00e0 notre probl\u00e8me exemple. Cette fois-ci, nous n'allons pas simplifier notre exemple : nous traiterons directement le probl\u00e8me 3D. Comme pour les exemples pr\u00e9c\u00e9dents, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Nous allons ensuite encoder les labels \"par \u00e9tiquette\" : from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() df_dataset['instrument'] = encoder.fit_transform(df_dataset['instrument']) Ce choix peut paraitre \u00e9tonnant, puisque nos labels ne sont pas ordinaux. On aurait plut\u00f4t tendance \u00e0 utiliser de l'encodage \"one-hot\". L'astuce est que l'impl\u00e9mentation \"MLPClassifier\" de Scikit-Learn r\u00e9alise implicitement un encodage \"one-hot\" \u00e0 partir de labels \"par \u00e9tiquette\". Nous r\u00e9cup\u00e9rons ensuite les features et les labels que nous allons utiliser dans 2 DataFrames : df_features = df_dataset[['harmo1','harmo2','harmo3']] df_labels = df_dataset['instrument'] Nous s\u00e9parons ensuite nos donn\u00e9es en un jeu d'entrainement (80%) et un jeu de test (20%), sous la forme de 4 DataFrames (2 pour les features, 2 pour les labels) : from sklearn.model_selection import train_test_split df_features_train, df_features_test, df_labels_train, df_labels_test = train_test_split(df_features,df_labels,test_size=0.2,random_state=0) Afin d'aider le PMC \u00e0 converger, nous allons effectuer une transformation centrage-r\u00e9duction (voir Chapitre 1) afin de s'assurer que les 3 features \u00e9voluent sur des intervalles comparables. Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_features_train) df_features_train[['harmo1','harmo2','harmo3']] = scaler.transform(df_features_train) df_features_test[['harmo1','harmo2','harmo3']] = scaler.transform(df_features_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre classifieur. Voici comment initialiser un classifieur PMC avec les param\u00e8tres par d\u00e9faut : from sklearn.neural_network import MLPClassifier mlp = MLPClassifier() Pour l'entrainer, il nous suffit d'utiliser la commande suivante : mlp.fit(df_features_train,df_labels_train) Nous pouvons \u00e0 pr\u00e9sent utiliser notre mod\u00e8le pour classifier des donn\u00e9es. Tout d'abord, nous allons \u00e9valuer les performances de notre mod\u00e8le en entrainement et en test. On peut d\u00e9j\u00e0 mesurer l'exactitude de notre classifieur sur ces 2 jeux de donn\u00e9es : print(mlp.score(df_features_train,df_labels_train)) print(mlp.score(df_features_test,df_labels_test)) On obtient facilement plus de 99% d'exactitude en entrainement, et plus de 98.5% d'exactitude en test. Ces r\u00e9sultats laissent \u00e0 pense que les performances en g\u00e9n\u00e9ralisation de notre mod\u00e8le seront plut\u00f4t bonnes. Mais comme nous l'avons fait remarquer plus t\u00f4t, le PMC est sensible au sur-apprentissage. Pour cette raison, on peut vouloir appliquer la m\u00e9thode de r\u00e9gularisation par \"arr\u00eat pr\u00e9matur\u00e9\" (voir Chapitre 1). Si on active le param\u00e8tre early_stopping du classifieur PMC de Scikit-Learn, au moment de l'apprentissage il va automatiquement mettre de c\u00f4t\u00e9 une partie du jeu d'entrainement pour faire un jeu de validation. On peut m\u00eame choisir la fraction du jeu d'entrainement \u00e0 utiliser pour la validation, avec le param\u00e8tre validation_fraction . Voici un exemple de d\u00e9finition d'un classifieur, avec de l'arr\u00eat pr\u00e9matur\u00e9 et 20% des donn\u00e9es d'entrainement utilis\u00e9es pour la validation : mlp = MLPClassifier(early_stopping=True,validation_fraction=0.2) Il y a aussi une astuce pour afficher l'\u00e9valuation de la fonction de co\u00fbt au cours des \u00e9poques, pour l'ensemble d'entrainement et de validation. Elle se base sur : Extraire le jeu de validation avec la m\u00e9thode train_test_split de Scikit-Learn. Utiliser la m\u00e9thode partial_fit de notre classifieur PMC, qui permet de r\u00e9aliser une it\u00e9ration \u00e0 la fois. R\u00e9cup\u00e9rer la valeur de la fonction de co\u00fbt sur le jeu d'entrainement, avec l'attribut loss_ de notre classifieur. Evaluer la valeur de la fonction de co\u00fbt sur le jeu de validation, en utilisant la fonction log_loss de Scikit-Learn. Voici l'affichage des 2 courbes Matplotlib obtenues sur notre base de donn\u00e9es, pour 50 \u00e9poques : import matplotlib.pyplot as plt from sklearn.metrics import log_loss df_features_train, df_features_validation, df_labels_train, df_labels_validation = train_test_split(df_features_train,df_labels_train,test_size=0.2,random_state=0) mlp = MLPClassifier() loss_train = [] loss_validation = [] for idx in range(50): mlp.partial_fit(df_features_train,df_labels_train,classes=[0,1,2]) loss_train.append(mlp.loss_) loss_validation.append(log_loss(df_labels_validation,mlp.predict_proba(df_features_validation))) plt.plot(loss_train, label=\"train loss\",c='r') plt.plot(loss_validation, label=\"validation loss\",c='g') plt.xlabel('Epoques') plt.ylabel('Fonction de co\u00fbt') plt.legend() Les performances obtenues avec ce mod\u00e8le sont d\u00e9j\u00e0 excellentes, mais il faudrait \u00e0 pr\u00e9sent se baser sur ces codes pour effectuer une optimisation des hyperparam\u00e8tres.","title":"Application \u00e0 notre exemple"},{"location":"Chap2_Classification_supervisee/#remarques_2","text":"La m\u00e9thode du Perceptron Multi-Couche a les avantages suivants : Elle permet de dessiner des fronti\u00e8res de d\u00e9cision complexes entre les classes sans faire de grosses hypoth\u00e8ses statistiques au pr\u00e9alable. Une fois le mod\u00e8le entrain\u00e9, il prend moins de m\u00e9moire et est plus rapide pour faire des pr\u00e9diction que les KPPV. On peut entrainer ce type de mod\u00e8le sur de tr\u00e8s grandes bases de donn\u00e9es . Mais cette m\u00e9thode a aussi les limites suivantes : Elle a de nombreux param\u00e8tres et hyperparam\u00e8tres \u00e0 optimiser. Elle est sensible au sur-apprentissage . Les d\u00e9cisions qu'elle prend sont difficilement expliqu\u00e9es et interpr\u00e9tables : il est difficile voir impossible pour un humain de les comprendre. On parle de \"bo\u00eete noire\". L'initialisation de la m\u00e9thode se faisant de mani\u00e8re al\u00e9atoire , 2 apprentissages ne donneront pas exactement le m\u00eame mod\u00e8le. Les r\u00e9seaux de neurones sont \u00e0 la base des mod\u00e8les d'apprentissage modernes, fondant ainsi une nouvelle sous-discipline : l' apprentissage profond . Nous verrons dans le chapitre suivant que le PMC peut aussi \u00eatre utilis\u00e9 pour r\u00e9soudre des probl\u00e8mes de r\u00e9gression...","title":"Remarques"},{"location":"Chap3_Regression/","text":"Chapitre III : R\u00e9gression Ce chapitre est une introduction \u00e0 la r\u00e9gression : principe, mesures de performances et m\u00e9thodes de base. Probl\u00e8me de r\u00e9gression Comme mentionn\u00e9 lors du Chapitre I, par \" r\u00e9gression \" on entend associer une r\u00e9alisation d'une variable quantitative continue \u00e0 un individu (labels), \u00e0 partir des r\u00e9alisations d'autres variables (features). On cherche donc une relation entre les variables d'entr\u00e9e (aussi appel\u00e9es \"variables explicatives\") et la variable de sortie (aussi appel\u00e9e \"variable de r\u00e9ponse\"). Il existe 2 grands types de relations entre variables : La relation est d\u00e9terministe si on consid\u00e8re que la variable de sortie peut \u00eatre connue exactement \u00e0 partir des variables d'entr\u00e9e. Par exemple, si je veux d\u00e9terminer le rayon \\(R\\) d'une \u00e9toile par rapport \u00e0 sa luminosit\u00e9 \\(L\\) et sa temp\u00e9rature \\(T\\) , en m'appuyant sur la th\u00e9orie du corps noir, j'utilise la formule \\(R = \\sqrt{\\frac{L}{4 \\pi \\sigma T^4}}\\) . La relation est probabiliste si on consid\u00e8re que d' autres facteurs que les variables d'entr\u00e9e vont influer sur la valeur de la variable de sortie. Dans ce cas, une m\u00eame r\u00e9alisation des variables d'entr\u00e9e pourra \u00eatre associ\u00e9e \u00e0 plusieurs valeurs de la variable de sortie, et inversement. Mais si les variables d'entr\u00e9e et de sortie sont corr\u00e9l\u00e9es , la relation sera tout de m\u00eame utile pour r\u00e9aliser des pr\u00e9dictions avec une certaine marge d'erreur. C'est souvent le cas en Physique, lorsque l'on r\u00e9alise des mesures pour expliquer un ph\u00e9nom\u00e8ne. Si on reprend notre exemple pr\u00e9c\u00e9dent : la th\u00e9orie du corps noir n'explique pas parfaitement la rayonnement d'une \u00e9toile, et on peut avoir des erreurs de mesures de \\(L\\) , \\(T\\) et \\(R\\) . Mais si \\(L\\) , \\(T\\) et \\(R\\) sont correl\u00e9es, alors on peut essayer de pr\u00e9dire \\(R\\) \u00e0 partir de \\(L\\) et \\(T\\) , moyennant une certaine erreur. C'est tout le principe de la r\u00e9gression : d\u00e9terminer une relation probabiliste entre les variables d'entr\u00e9e et la variable de sortie . Nous avions aussi mentionn\u00e9 lors du Chapitre I qu'entrainer un mod\u00e8le de r\u00e9gression ne peut se faire que par apprentissage supervis\u00e9 . L'id\u00e9e est encore une fois que le mod\u00e8le soit ensuite capable de g\u00e9n\u00e9raliser \u00e0 de nouvelles observations. Anecdote historique Le nom de \"r\u00e9gression\" vient du g\u00e9n\u00e9ticien Francis Galton, qui l'utilisa pour son \u00e9tude sur la \"r\u00e9gression vers la moyenne\". En fait, Galton cherchait \u00e0 mod\u00e9liser un ph\u00e9nom\u00e8ne d'h\u00e9r\u00e9dit\u00e9 qu'il observait dans la population humaine : La taille des fils avait tendance \u00e0 se rapprocher de la moyenne de la population par rapport \u00e0 celle de leur p\u00e8re. Les diff\u00e9rents types de r\u00e9gression Suivant le nombre d'entr\u00e9es et le type de mod\u00e8le \u00e0 ajuster aux donn\u00e9es, on a diff\u00e9rents types de probl\u00e8mes de r\u00e9gression. Nous allons voir ici les 3 plus courants. R\u00e9gression lin\u00e9aire simple La r\u00e9gression lin\u00e9aire simple est le probl\u00e8me de r\u00e9gression le plus basique qui soit. On recherche une relation lin\u00e9aire entre une variable d'entr\u00e9e \\(x\\) et une variable de sortie \\(y\\) . L'erreur est une variable al\u00e9atoire not\u00e9e \\(\\epsilon\\) . Le mod\u00e8le \u00e0 ajuster est le suivant : \\(y = \\alpha x + \\beta + \\epsilon\\) avec les param\u00e8tres \\(\\alpha\\) et \\(\\beta\\) \u00e0 d\u00e9terminer. Il s'agit d'un probl\u00e8me d' inf\u00e9rence statistique : nous disposons d'un jeu d'entrainement qui est un \u00e9chantillon de la population totale, et nous voulons en d\u00e9duire une estimation de \\(\\alpha\\) et \\(\\beta\\) nous permettant de r\u00e9aliser des pr\u00e9dictions. Nous verrons que l'on fait en g\u00e9n\u00e9ral les hypoth\u00e8ses suivantes sur \\(\\epsilon\\) : Ind\u00e9pedance de ses r\u00e9alisations. Moyenne nulle . Ecart-type constant avec \\(x\\) . Afin de pouvoir donner un \"intervalle de confiance\" aux pr\u00e9dictions, on va en plus ajouter une hypoth\u00e8se de normalit\u00e9 : \\(\\epsilon\\) suit une loi normale. Nous en reparlerons plus tard. R\u00e9gression lin\u00e9aire multiple On peut g\u00e9n\u00e9raliser le mod\u00e8le de la section pr\u00e9c\u00e9dente aux probl\u00e8mes avec plusieurs variables d'entr\u00e9e . Si on note \\(x_1\\) , \\(x_2\\) , ..., \\(x_n\\) les \\(n\\) variables d'entr\u00e9e, et \\(y\\) notre variable de sortie. Le mod\u00e8le \u00e0 ajuster devient : \\(y = \\alpha_1 x_1 + \\alpha_2 x_2 + ... + \\alpha_n x_n + \\beta + \\epsilon\\) On reconnait la formule d'un hyperplan de dimension \\(n\\) . On peut mettre cette formule sous forme matricielle : \\(y = A.x + \\epsilon\\) avec \\(A = \\begin{pmatrix} \\beta\\\\ \\alpha_1\\\\ \\alpha_2\\\\ \\vdots\\\\ \\alpha_n \\end{pmatrix}\\) et \\(x = \\begin{pmatrix} 1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\) Nous verrons dans la suite comment g\u00e9n\u00e9raliser les m\u00e9thodes aux probl\u00e8mes multiples. R\u00e9gression polynomiale Comment faire lorsqu'un mod\u00e8le lin\u00e9aire n'est pas pertinent pour repr\u00e9senter la relation entre nos variables ? Afin de r\u00e9aliser une r\u00e9gression non-lin\u00e9aire , il y a une astuce : on cherche \u00e0 ajuster un mod\u00e8le polynomial . Pour un ordre \\(k\\) , il aura la forme : \\(y = \\alpha_1 x + \\alpha_2 x^2 + ... + \\alpha_n x^n + b + \\epsilon\\) Il y a alors une astuce : si on consid\u00e8re \\(x\\) , \\(x^2\\) , ..., \\(x^n\\) comme \\(n\\) variables d'entr\u00e9e, alors on reconnait un probl\u00e8me de r\u00e9gression multiple ! On peut donc le traiter avec les m\u00eames m\u00e9thodes qu'un probl\u00e8me lin\u00e9aire. Il existe d'autres techniques d'ajustement de mod\u00e8les non-lin\u00e9aires, que nous ne traiterons pas dans le cadre de ce cours. Exemple de probl\u00e8me Est-il possible de pr\u00e9dire l'intensit\u00e9 du rayonnement solaire \u00e0 partir du nombre de taches solaires ? Les taches solaires sont des zones de \"faible\" temp\u00e9rature (4500 K contre 5800 K) la surface du soleil (photosph\u00e8re). Elles apparaissent lorsque l'apport de chaleur \u00e0 la surface par convection est inhib\u00e9 par une concentration de lignes de champs magn\u00e9tique. On sait observer les taches solaires depuis l'antiquit\u00e9, et on a des mesures du nombre taches solaires depuis le XVII\u00e8me, qui varie entre 0 et 350 environ. On l'utilise comme un marqueur de l'activit\u00e9 solaire depuis le milieu XIX\u00e8me si\u00e8cle. Un autre marqueur connu de l'activit\u00e9 solaire est l'\"irradiance solaire\" totale ou TSI en anglais. Il s'agit de la puissance du rayonnement solaire re\u00e7ue en haut de l'atmosph\u00e8re ter restre, par unit\u00e9 de surface. Sa valeur est d'environ 1363 \\(W/m^2\\) , avec de l\u00e9g\u00e8res variations suivant l'activit\u00e9 solaire. La TSI est un param\u00e8tre cl\u00e9 en climatologie, puisqu'il correspond \u00e0 l'\u00e9nergie apport\u00e9e par le soleil \u00e0 la Terre. Mais sa mesure n'est pas ais\u00e9e : il ne peut \u00eatre obtenu que par des satellites. Voici l'\u00e9volution du nombre de taches solaires et du TSI depuis 1948 jusqu'\u00e0 2024 : On a ici 20 points par an, avec un filtrage de moyenne glissante sur une 1/2 ann\u00e9e, soit 1531 points au total. Elles sont issues de l'Observatoire Royal de Belgique (SILSO, Dewitte et al. 2022). On observe que le nombre de taches solaires comme la TSI suivent les m\u00eames cycles de 11 ans environ, correspondant aux cycles de l'activit\u00e9 solaire. On imagine alors que la corr\u00e9lation entre les 2 grandeurs doit \u00eatre forte. D'o\u00f9 l'id\u00e9e suivante : peut-on entrainer un mod\u00e8le \u00e0 estimer la TSI \u00e0 partir du nombre de taches solaires ? Voici les donn\u00e9es d'o\u00f9 sont issues les courbes pr\u00e9c\u00e9dentes, au format CSV : Chap3_sunspots_dataset Le tableau de donn\u00e9es qu'il contient est de la forme : year tsi sunspots 1948.25 1363.743 193.667 1948.30 1363.729 196.541 1948.35 1363.722 205.891 1948.40 1363.713 215.623 1948.45 1363.729 218.060 ... ... ... 2024.65 1364.015 172.536 2024.70 1364.013 170.525 2024.75 1364.038 171.563 Il contient pour chacun des 1531 points l'ann\u00e9e (sous forme d\u00e9cimale), la TSI moyenne (en \\(W/m^2\\) ) et le nombre de taches solaires moyen sur une fen\u00eatre d'une 1/2 ann\u00e9e. Notre probl\u00e8me de r\u00e9gression sera la suivant : pr\u00e9dire la TSI moyenne sur une 1/2 ann\u00e9e \u00e0 partir du nombre de taches solaires moyen sur cette m\u00eame fen\u00eatre . Voyons d'abord si une telle r\u00e9gression est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) On peut alors utiliser la m\u00e9thode \"plot\" des DataFrames pandas pour afficher la TSI en fonction du nombre de taches solaires, sous la forme d'un nuage de points : df_dataset.plot(x='sunspots',y='tsi',kind='scatter',c='r',marker='+') Voici le r\u00e9sultat : On observe comme attendu que les 2 grandeurs ont l'air fortement corr\u00e9l\u00e9es . Cependant, on peut d\u00e9j\u00e0 constater que : (1) la relation n'a l'air lin\u00e9aire que pour des nombres de taches solaires faibles (moins de 150-200), (2) la dispersions des points a l'air d'augmenter avec le nombre de taches solaires. Ces observations seront importantes dans la suite. On peut \u00e9galement calculer le coefficient de corr\u00e9lation entre la TSI et le nombre taches solaires, en utilisant la m\u00e9thode \"corr\" des DataFrames Pandas : df_dataset['tsi'].corr(df_dataset['sunspots']) On trouve un coefficient de corr\u00e9lation de 0.89 environ, ce qui confirme une forte corr\u00e9lation entre les variables. Vouloir entrainer un mod\u00e8le \u00e0 pr\u00e9dire la TSI \u00e0 partir du nombre de taches solaires a donc un sens. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP. Mesures de performances Nous allons passer en revue dans cette section les principaux indicateurs de performances applicables \u00e0 la r\u00e9gression lin\u00e9aire. Table ANOVA Soit un probl\u00e8me de r\u00e9gression dont la variable de sortie est not\u00e9e \\(y\\) . On veut \u00e9valuer les performances d'un mod\u00e8le de r\u00e9gression sur un jeu de donn\u00e9es issu de ce probl\u00e8me. La moyenne des valeurs de \\(y\\) au sein de cet \u00e9chantillon est not\u00e9e \\(\\overline{y}\\) . La valeur de \\(y\\) pour le i-\u00e8me individu de cet \u00e9chantillon sera not\u00e9e \\(y_i\\) . Admettons que l'on a ait d\u00e9termin\u00e9 un mod\u00e8le de regression lin\u00e9aire pour ces donn\u00e9es. On note \\(\\hat{y_i}\\) la pr\u00e9diction de ce mod\u00e8le lin\u00e9aire pour le i-\u00e8me individu. Pour juger de la qualit\u00e9 du mod\u00e8le, on divise les \u00e9carts en 2 groupes : Les \u00e9carts r\u00e9siduels , ou \"r\u00e9sidus\" : \\(y_i - \\hat{y_i}\\) Il s'agit des \u00e9carts non-expliqu\u00e9s par le mod\u00e8le. On remarque qu'ils correspondent aux \\(\\epsilon_i\\) de notre mod\u00e8le. Les \u00e9carts \u00e9carts de r\u00e9gression , ou \"\u00e9carts expliqu\u00e9s\" : \\(\\hat{y_i} - \\overline{y}\\) Il s'agit des \u00e9carts expliqu\u00e9s par le mod\u00e8le. On a alors l' \u00e9cart total : \\(y_i - \\overline{y} = (y_i - \\hat{y_i}) - (\\hat{y_i} - \\overline{y})\\) On met en g\u00e9n\u00e9ral ces \u00e9carts sous la forme de variances, en prenant la somme des carr\u00e9s des \\(p\\) individus de cet \u00e9chantillon : SCR (\"somme des carr\u00e9s des r\u00e9sidus\") : \\(\\sum_{i=1}^{p} (y_i - \\hat{y_i})^2\\) SCE (\"somme des carr\u00e9s expliqu\u00e9s\") : \\(\\sum_{i=1}^{p} (\\hat{y_i} - \\overline{y})^2\\) SCT (\"somme des carr\u00e9s totale\") : \\(\\sum_{i=1}^{p} (y_i - \\overline{y})^2\\) avec \\(SCT = SCR + SCE\\) Un mod\u00e8le sera d'autant plus performant que la SCR sera faible compar\u00e9e \u00e0 la SCT . L'id\u00e9e est la suivante : plus la SCE est grande (et donc plus la SCR est faible), et plus le mod\u00e8le explique \\(y\\) \u00e0 partir des entr\u00e9es. On range en g\u00e9n\u00e9ral ces valeurs sous la forme d'un tableau, nomm\u00e9 \"table ANOVA\" (contraction en anglais de \"ANalysis Of VAriance\") : \\(x_i\\) \\(y_i\\) \\(\\hat{y_i}\\) \\(\\hat{y_i} - \\overline{y}\\) \\((\\hat{y_i} - \\overline{y})^2\\) \\(y_i - \\hat{y_i}\\) \\((y_i - \\hat{y_i})^2\\) ... ... ... ... ... ... ... \\(\\overline{x}\\) \\(\\overline{y}\\) SCE SCR SCT ... ... ... ... ... On peut trouver des variantes de cette table, mais elle contient toujours au moins la SCE, la SCR et la SCT. Coefficient de d\u00e9termination La table ANOVA est une repr\u00e9sentation plut\u00f4t exhaustive des performances en r\u00e9gression lin\u00e9aire. Mais comme souvent, on voudrait pouvoir r\u00e9sumer au mieux les performances avec un score unique d\u00e9riv\u00e9 de cette table. Le crit\u00e8re le plus utilis\u00e9 est le coefficient de d\u00e9termination , not\u00e9 \\(R^2\\) : \\(R^2 = \\frac{SCE}{SCT} = \\frac{\\sum_{i=1}^{p} (\\hat{y_i} - \\overline{y})^2}{\\sum_{i=1}^{p} (y_i - \\overline{y})^2} = 1 - \\frac{SCR}{SCT} = 1 - \\frac{\\sum_{i=1}^{p} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{p} (y_i - \\overline{y})^2}\\) Le \\(R^2\\) s'interpr\u00e8te comme la proportion de l'\u00e9cart total expliqu\u00e9e par le mod\u00e8le . Il s'agit donc d'un score entre 0 et 1 : plus la valeur est proche de 1, et meilleur est le mod\u00e8le. Par exemple, mettons que l'on utilise la luminosit\u00e9 d'une \u00e9toile pour essayer de pr\u00e9dire son rayon, gr\u00e2ce \u00e0 une r\u00e9gression lin\u00e9aire. Si le \\(R^2\\) du mod\u00e8le est de 0.75 sur un \u00e9chantillon de donn\u00e9es, cela veut dire que le mod\u00e8le explique 75% de la variation du rayon de l'\u00e9toile. Les 25% restants sont expliqu\u00e9s par les erreurs. On remarque que le \\(R^2\\) correspond au carr\u00e9 du coefficient de corr\u00e9lation (voir Chapitre 1) entre les valeurs observ\u00e9es \\(y_i\\) et les valeurs pr\u00e9dites \\(\\hat{y_i}\\) . Nota Bene En r\u00e9gression lin\u00e9aire simple, le \\(R^2\\) est \u00e9gal au carr\u00e9 du coefficient de corr\u00e9lation entre \\(x\\) et \\(y\\) . Ce n'est pas vrai pour la r\u00e9gression lin\u00e9aire multiple. Analyse des r\u00e9sidus Lorsque les performances d'un mod\u00e8le de r\u00e9gression lin\u00e9aire ont l'air mauvaises, on a envie de comprendre pourquoi. La bonne approche est de r\u00e9aliser une analyse des r\u00e9sidus . Dans un 1er temps, cette analyse peut \u00eatre visuelle . On affiche simplement les r\u00e9sidus en fonction de \\(x\\) , ou sous la forme d'un histogramme, et on v\u00e9rifie s'ils ont l'air d'avoir le comportement attendu de \\(\\epsilon\\) : Ind\u00e9pendance des observations. Moyenne nulle. Ecart-type constant, aussi appel\u00e9 \"homosc\u00e9dasticit\u00e9\". Normalit\u00e9. Dans l'id\u00e9al, on attend donc un nuage de points al\u00e9atoires , d'\u00e9cart-type constant, sans tendances en fonction de \\(x\\) . Si ce n'est pas le cas, alors il faut soit : Revoir notre mod\u00e8le (une r\u00e9gression lin\u00e9aire simple n'est peut-\u00eatre pas adapt\u00e9e). Nettoyer nos donn\u00e9es (des outliers ou des donn\u00e9es ab\u00e9rrantes sont peut-\u00eatre la cause du mauvais ajustement). Ajouter des variables explicatives ( \\(x\\) n'est peut-\u00eatre pas suffisant pour expliquer \\(y\\) de mani\u00e8re satisfaisante). En cas de doute, on peut proc\u00e9der \u00e0 des tests de ces hypoth\u00e8ses, mais ils ne sont pas tous simples \u00e0 mettre en place. En voici quelques exemples : Hypoth\u00e8se Test Normalit\u00e9 Droite de Henry (quantiles des r\u00e9sidus en fonction de ceux attendus) Homosc\u00e9dasticit\u00e9 Test de White (hypoth\u00e8se nulle : variance des r\u00e9sidus sachant \\(x\\) constante) Ind\u00e9pendance Test de Durbin-Watson (hypoth\u00e8se nulle : non-corr\u00e9lation des r\u00e9sidus) M\u00e9thodes de base Moindres carr\u00e9s ordinaire Les moindres carr\u00e9s ordinaire (MCO) est la m\u00e9thode de r\u00e9gression lin\u00e9aire la plus basique qui soit. S'il s'agit originellement d'une m\u00e9thode de statistiques descriptives , nous verrons que l'on peut s'en servir pour faire de l' inf\u00e9rence statistique . Principe Comme nous venons de le mentionner, les MCO a originellement un but descriptif. Si on a un jeu de donn\u00e9es contenant une variable explicative \\(x\\) et une variable de r\u00e9ponse \\(y\\) , on cherche : quelle droite d'\u00e9quation \\(y = a x + b\\) repr\u00e9sente le mieux la distribution des \\(p\\) points \\((x_i,y_i)\\) de cet \u00e9chantillon ? \\(a\\) et \\(b\\) seront alors 2 indicateurs statistiques caract\u00e9risant notre \u00e9chantillon. Mais comment d\u00e9terminer qu'une droite repr\u00e9sente au mieux un nuage de points ? La m\u00e9thode des MCO consid\u00e8re que la droite d'\u00e9quation \\(y = a x + b\\) repr\u00e9sentant le mieux les \\(p\\) point de notre \u00e9chantillon est celle qui minimise : \\(\\sum_{i=1}^{p} (y_i - a x_i - b)^2\\) c'est-\u00e0-dire la SCR du mod\u00e8le. D'o\u00f9 le nom de la m\u00e9thode : on cherche les \"moindres carr\u00e9s\". On peut montrer que les param\u00e8tres \\(a\\) et \\(b\\) minimisant cette fonction sont : \\(a = \\frac{\\sum_{i=1}^{p} (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^{p} (x_i-\\overline{x})^2}\\) \\(b = \\overline{y} - a \\overline{x}\\) On notera pour simplifier les expressions : \\(sc_{xx} = \\sum_{i=1}^{p} (x_i-\\overline{x})^2\\) \\(sc_{yy} = \\sum_{i=1}^{p} (y_i-\\overline{y})^2\\) \\(sc_{xy} = \\sum_{i=1}^{p} (x_i-\\overline{x})(y_i-\\overline{y})\\) D'o\u00f9 \\(a = \\frac{sc_{xy}}{sc_{xx}}\\) Nota Bene La droite d\u00e9termin\u00e9e par les MCO passera toujours par le point \\((\\overline{x},\\overline{y})\\) . Meilleur Estimateur Lin\u00e9aire Non-biais\u00e9 (BLUE) Revenons \u00e0 notre probl\u00e8me de r\u00e9gression lin\u00e9aire simple : \u00e0 partir de notre \u00e9chantillon, nous voulons trouver un mod\u00e8le liant nos variables \\(x\\) et \\(y\\) , de la forme \\(y = \\alpha x + \\beta + \\epsilon\\) . Sous certaines conditions sur \\(\\epsilon\\) , nous pouvons appliquer le th\u00e9or\u00e8me de Gauss-Markov \u00e0 notre probl\u00e8me : Th\u00e9or\u00e8me de Gauss-Markov On cherche \u00e0 mod\u00e9liser une relation \\(y = \\alpha x + \\beta + \\epsilon\\) entre 2 variables \\(x\\) et \\(y\\) , \u00e0 partir d'un \u00e9chantillon de r\u00e9alisations \\((x_i,y_i)\\) . Si \\(\\epsilon\\) v\u00e9rifie : - Une moyenne nulle. - Un \u00e9cart-type constant avec \\(x\\) . - Une non-corr\u00e9lation de ses r\u00e9alisations. Alors, les param\u00e8tres \\(a\\) et \\(b\\) de la droite d\u00e9termin\u00e9e par les MCO est le Meilleur Estimateur Lin\u00e9aire Non-biais\u00e9 (\"BLUE\" en anglais) de \\(\\alpha\\) et \\(\\beta\\) . On peut donc se servir de la m\u00e9thode des MCO pour estimer \\(\\alpha\\) et \\(\\beta\\) \u00e0 partir de notre \u00e9chantillon de points \\((x_i,y_i)\\) . Il est m\u00eame possible d'estimer l' \u00e9cart-type de \\(\\epsilon\\) avec l'estimateur suivant : \\(s = \\sqrt{\\frac{\\sum_{i=1}^{p} (y_i-\\hat{y_i})^2}{p-2}} = \\sqrt{\\frac{\\sum_{i=1}^{p} \\epsilon_i^2}{p-2}}\\) Reste alors une probl\u00e9matique : Si j'utilise mon mod\u00e8le pour r\u00e9aliser une pr\u00e9diction \\(\\hat{y_{p+1}}\\) \u00e0 partir d'une nouvelle valeur \\(x_{p+1}\\) , c'est-\u00e0-dire en calculant \\(\\hat{y_{p+1}} = \\alpha x_{p+1} + \\beta\\) , \u00e0 quel point puis-je avoir confiance en ma pr\u00e9diction ? Intervalles de confiance et de pr\u00e9diction Comme pour tout probl\u00e8me d'inf\u00e9rence statistique, lorsque l'on a obtenu notre mod\u00e8le de r\u00e9gression lin\u00e9aire, on se pose alors les questions suivantes : Quelle est mon incertitude sur les \\(\\alpha\\) et \\(\\beta\\) trouv\u00e9s \u00e0 partir de mon \u00e9chantillon ? Pour une valeur de \\(x\\) fix\u00e9e, quelle est mon incertitude sur la moyenne des \\(y\\) avec mon mod\u00e8le de r\u00e9gression lin\u00e9aire ? Pour un nouvelle observation de \\(x\\) , quelle est mon incertitude sur la valeur de \\(y\\) pr\u00e9dite par mon mod\u00e8le de r\u00e9gression lin\u00e9aire ? Pour r\u00e9pondre \u00e0 ces questions, nous allons utiliser des intervalles de confiance . L'hypoth\u00e8se de normalit\u00e9 de \\(\\epsilon\\) implique que les estimations de \\(\\alpha\\) et de \\(\\beta\\) \u00e0 partir d'un \u00e9chantillon suivent une loi normale . Mais nous ne pouvons qu'estimer son \u00e9cart-type, puisque nous ne disposons que d'un \u00e9chantillon. Il nous faut donc utiliter la loi de Student , et plus particuli\u00e8rement le \"t de Student\". Rappels sur le t de Student Soit une population de moyenne \\(\\mu\\) et d'\u00e9cart-type inconnu, dont on r\u00e9cup\u00e8re un \u00e9chantillon de \\(p\\) points, de moyenne estim\u00e9e \\(\\overline{x}\\) et d'\u00e9cart-type estim\u00e9 \\(s\\) . Alors la variable al\u00e9atoire \\(t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{p}}\\) suit une loi de Student, dont on peut se servir pour \u00e9tablir un intervalle de confiance sur l'estimation \\(\\overline{x}\\) de \\(\\mu\\) . On note \\(t_{\\gamma}^{k}\\) le quantile de seuil d'erreur \\(\\gamma\\) de la loi de Student \u00e0 \\(k\\) degr\u00e9s de libert\u00e9 . Le seuil de confiance est alors \\(1-\\gamma\\) : pour seuil de confiance \u00e0 99% on prendra \\(\\gamma = 0.01\\) . La loi normale \u00e9tant sym\u00e9trique, pour d\u00e9terminer un intervalle de confiance de seuil \\(1-\\gamma\\) , il faut en r\u00e9alit\u00e9 utiliser \\(t_{\\gamma/2}^{k}\\) . Donc pour un intervalle de confiance \u00e0 99% on prendra \\(\\gamma = 0.005\\) . On a alors : \\(p(\\overline{x} - t_{\\gamma/2}^{p-1} \\frac{s}{\\sqrt{p}} \\leq \\mu \\leq \\overline{x} + t_{\\gamma/2}^{p-1} \\frac{s}{\\sqrt{p}}) = 1 - \\gamma\\) avec \\(k = p-1\\) car on a utilis\u00e9 1 degr\u00e9 de libert\u00e9 pour estimer \\(\\mu\\) . Nota Bene : Il est \u00e0 noter que plus \\(p\\) est grand (et donc plus \\(k\\) est grand) et plus le \\(t\\) se rapproche d'une loi normale. Dans notre cas, nous avons utilis\u00e9 2 degr\u00e9s de libert\u00e9 pour estimer \\(\\alpha\\) et \\(\\beta\\) , nous utiliserons donc le t de Student pour \\(p-2\\) degr\u00e9s de libert\u00e9. On peut donc \u00e9tablir les intervalles de confiance \u00e0 \\(1-\\gamma\\) suivants sur \\(a\\) et \\(b\\) : \\(\\alpha \\in [a - t_{\\gamma/2}^{p-2} s(a) ; a + t_{\\gamma/2}^{p-2} s(a)]\\) \\(\\beta \\in [b - t_{\\gamma/2}^{p-2} s(b) ; b + t_{\\gamma/2}^{p-2} s(b)]\\) avec les \u00e9cart-types estim\u00e9s : \\(s(a) = \\frac{s}{\\sqrt{sc_{xx}}}\\) \\(s(b) = s \\sqrt{\\frac{1}{p} + \\frac{\\overline{x}^2}{sc_{xx}}}\\) Nota Bene Il est \u00e0 noter que si tous les \\(x_i\\) de l'\u00e9chantillon sont \u00e9gaux, alors \\(x_i = \\overline{x}\\) , d'o\u00f9 \\(sc_{xx} = 0\\) et donc les intervalles de confiance deviennent infinis. Ce r\u00e9sultat est attendu, puisqu'on ne peut pas tirer d'information sur la relation entre \\(x\\) et \\(y\\) avec des points pour un seul \\(x_i\\) . De la m\u00eame mani\u00e8re, on peut estimer pour une valeur de \\(x\\) donn\u00e9e \\(x=u\\) l' intervalle de confiance \u00e0 \\(1-\\gamma\\) sur la moyenne des \\(y\\) sachant \\(x=u\\) : \\(\\alpha u + \\beta \\in [a u + b - t_{\\gamma/2}^{p-2} s(\\hat{y}(u)) ; a u + b + t_{\\gamma/2}^{p-2} s(\\hat{y}(u))]\\) avec \\(s(\\hat{y}(u)) = s \\sqrt{\\frac{1}{p} + \\frac{(u-\\overline{x})^2}{sc_{xx}}}\\) Enfin, on peut estimer l' intervalle de pr\u00e9diction sur \\(y_{p+1}\\) pour une nouvelle donn\u00e9e \\(x_{p+1}\\) : \\(y_{p+1} \\in [a x_{p+1} + b - t_{\\gamma/2}^{p-2} s(y_{p+1}) ; a x_{p+1} + b + t_{\\gamma/2}^{p-2} s(y_{p+1})]\\) avec \\(s(y_{p+1}) = s \\sqrt{1 + \\frac{1}{p} + \\frac{(x_{p+1}-\\overline{x})^2}{sc_{xx}}}\\) En g\u00e9n\u00e9ral, lorsque l'on affiche par-dessus le nuage de points la droite du mod\u00e8le obtenu par MCO, on affiche aussi l'intervalle de confiance sur la moyenne des \\(y\\) , et l'intervalle de pr\u00e9diction choisis. Le graphique obtenu est de la forme suivante : Nota Bene Il est \u00e0 noter que : - Les intervalles de confiance sur la moyenne des \\(y\\) sont toujours plus petits que les intervalles de pr\u00e9vision. - La droite obtenue par MCO passe toujours par \\((\\overline{x},\\overline{y})\\) , donc plus on s'\u00e9loigne de ce point, plus les intervalles de confiance et de pr\u00e9diction vont augmenter. Impl\u00e9mentation Scipy Afin de r\u00e9aliser une r\u00e9gression lin\u00e9aire simple avec la m\u00e9thode des MCO, on peut utiliser la biblioth\u00e8que de calculs scientifiques Scipy, et en particulier son module de statistiques \"scipy.stat\". Il suffit d'importer l'objet \"linregress\" avec : from scipy.stats import linregress Pour ajuster un mod\u00e8le de r\u00e9gression lin\u00e9aire mco \u00e0 une variable d'entr\u00e9e x et une variable de sortie y on utilise la commande : mco = linregress(x,y) On peut alors r\u00e9cup\u00e9rer le coefficient directeur a et l'ordonn\u00e9e \u00e0 l'origine b de ce mod\u00e8le lin\u00e9aire avec : a = mco.slope b = mco.intercept Il suffit alors d'utiliser ces 2 param\u00e8tres pour r\u00e9aliser une pr\u00e9diction. Pour d\u00e9terminer les intervalles de confiance et de pr\u00e9diction, la biblioth\u00e8que Scipy propose aussi une impl\u00e9mentation de la loi de Student, que l'on peut importer avec : from scipy.stats import t Pour obtenir le quantile tq de seuil s correspondant \u00e0 \\(1-\\gamma\\) , de la loi de Student de \u00e0 k degr\u00e9s de libert\u00e9s, on alors simple utiliser la m\u00e9thode : tq = t.ppf(s,k) Il ne reste alors qu'\u00e0 impl\u00e9menter les formules des estimateurs d'\u00e9cart-types que nous avons vues pr\u00e9c\u00e9demment pour calculer les intervalles de confiances et de pr\u00e9diction. Il est \u00e9galement possible d'obtenir le \\(R^2\\) de la r\u00e9gression gr\u00e2ce au param\u00e8tre r_value du mod\u00e8le : r_2 = mco.rvalue**2 (On reconnait que r_value correspond au coefficient de corr\u00e9lation tel que vu au Chapitre 1). Si cette impl\u00e9mentation est pratique pour faire de l'inf\u00e9rence statistique, elle ne g\u00e8re malheureusement que la r\u00e9gression lin\u00e9aire simple. Pour de la r\u00e9gression lin\u00e9aire multiple, nous verrons que l'on peut utiliser Scikit-Learn. G\u00e9n\u00e9ralisation \u00e0 la r\u00e9gression lin\u00e9aire multiple Les MCO peut \u00eatre g\u00e9n\u00e9ralis\u00e9e pour les probl\u00e8mes \u00e0 plus d'une variable explicative (nombre de variables explicatives \\(n>1\\) ). (Comme mentionn\u00e9 pr\u00e9c\u00e9demment, un probl\u00e8me de r\u00e9gression polynomiale peut \u00e9galement \u00eatre r\u00e9solu en utilisant de la r\u00e9gression lin\u00e9aire multiple). Rappelons que le mod\u00e8le de r\u00e9gression lin\u00e9aire multiple \u00e0 ajuster est le suivant : \\(y = \\alpha_1 x_1 + \\alpha_2 x_2 + ... + \\alpha_n x_n + \\beta + \\epsilon\\) Si nous disposons de \\(p\\) observations dans notre jeu de donn\u00e9es d'entrainement, il faut que nos param\u00e8tres \\(\\alpha_1\\) , ..., \\(\\alpha_n\\) et \\(\\beta\\) v\u00e9rifient : \\(\\begin{cases} y_1 = \\alpha_1 x_{1,1} + \\alpha_2 x_{1,2} + ... + \\alpha_n x_{1,n} + \\beta + \\epsilon_1\\\\ y_2 = \\alpha_1 x_{2,1} + \\alpha_2 x_{2,2} + ... + \\alpha_n x_{2,n} + \\beta + \\epsilon_2\\\\ ...\\\\ y_p = \\alpha_1 x_{p,1} + \\alpha_2 x_{p,2} + ... + \\alpha_n x_{p,n} + \\beta + \\epsilon_p\\\\ \\end{cases}\\) Un syst\u00e8me d'\u00e9quations lin\u00e9aires que l'on peut mettre sous la forme matricielle suivante : \\(Y = X A + E\\) avec \\(Y = \\begin{pmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_p \\end{pmatrix}\\) \\(X = \\begin{pmatrix} 1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\ 1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{p,1} & x_{p,2} &\\cdots & x_{p,n} \\end{pmatrix}\\) \\(A = \\begin{pmatrix} \\beta\\\\ \\alpha_1\\\\ \\alpha_2\\\\ \\vdots\\\\ \\alpha_n \\end{pmatrix}\\) \\(E = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_p \\end{pmatrix}\\) Avec les m\u00eames hypoth\u00e8ses sur \\(\\epsilon\\) que pour la r\u00e9gression lin\u00e9aire simple, on peut appliquer les MCO pour trouver le meilleur estimateur lin\u00e9aire non-biais\u00e9 de \\(A\\) . Cette fois-ci, il s'agit de la matrice \\(\\hat{A}\\) minimisant l' erreur quadratique moyenne (ou \"MSE\" en anglais) : \\(MSE = \\frac{1}{p} \\sum_{i=1}^{p} (y_i - \\sum_{j=1}^{n} \\alpha_j x_{i,j} - \\beta)^2\\) On peut montrer que ce minimum est obtenu pour \\(\\hat{A}\\) v\u00e9rifiant l' \u00e9quation normale suivante : \\(\\hat{A} = (X^T X)^{-1} X^T Y\\) En pratique, il est rare que l'on r\u00e9solve directement l'\u00e9quation normale : (1) sa r\u00e9solution est complexe, (2) la matrice \\(X^T X\\) peut ne pas \u00eatre inversible (si \\(p<n\\) ou si certaines equations sont redondantes). C'est pourquoi la plupart des impl\u00e9mentations des MCO pour de la r\u00e9gression lin\u00e9aire multiple calculent plut\u00f4t : \\(\\hat{A} = X^{+} Y\\) avec \\(X^{+}\\) le pseudo-inverse de \\(X\\) . Celui-ci est calcul\u00e9 en utilisant la d\u00e9composition en valeurs singuli\u00e8res (SVD) de \\(X\\) . Cette m\u00e9thode \u00e0 l'avantage d'\u00eatre plus rapide que de r\u00e9soudre l'\u00e9quation normale directement, et que le pseudo-inverse de \\(X\\) existe toujours. On peut \u00e9galement g\u00e9n\u00e9raliser les formules de d\u00e9termination des intervalles de confiance et de pr\u00e9diction vues pr\u00e9c\u00e9demment. Tout d'abord, dans le cas multiple l'estimateur de l'\u00e9cart-type de \\(\\epsilon\\) devient : \\(s = \\sqrt{\\frac{\\sum_{i=1}^{p} (y_i-\\hat{y_i})^2}{p-n-1}} = \\sqrt{\\frac{\\sum_{i=1}^{p} \\epsilon_i^2}{p-n-1}}\\) Soit une r\u00e9alisation donn\u00e9e des variables d'entr\u00e9e : \\(\\begin{pmatrix} x_0 & x_1 & \\cdots & x_n \\end{pmatrix} = \\begin{pmatrix} u_0 & u_1 & \\cdots & u_n \\end{pmatrix} = U\\) L'intervalle de confiance \u00e0 \\(1-\\gamma\\) sur la moyenne des \\(y\\) sachant que les variables d'entr\u00e9e sont \u00e0 \\(U\\) est alors : \\(U A \\in [U \\hat{A} - t_{\\gamma/2}^{p-n-1} s(\\hat{y}(U)) ; U \\hat{A} + t_{\\gamma/2}^{p-n-1} s(\\hat{y}(U))]\\) avec \\(s(\\hat{y}(U)) = s \\sqrt{U(X^T X)^{-1}U^T}\\) Soit une nouvelle r\u00e9alisation des variables d'entr\u00e9es : \\(\\begin{pmatrix} x_{p+1,0} & x_{p+1,1} & \\cdots & x_{p+1,n} \\end{pmatrix} = V\\) L'intervalle de pr\u00e9diction \u00e0 \\(1-\\gamma\\) de \\(V\\) est : \\(y_{p+1} \\in [V \\hat{A} - t_{\\gamma/2}^{p-n-1} s(y_{p+1}) ; V \\hat{A} + t_{\\gamma/2}^{p-n-1} s(y_{p+1})]\\) avec \\(s(y_{p+1}) = s \\sqrt{1+V(X^T X)^{-1}V^T}\\) Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn des MCO, qui permet la r\u00e9gression lin\u00e9aire multiple (et donc la r\u00e9gression polynomiale). Elle peut \u00eatre import\u00e9e avec : from sklearn.linear_model import LinearRegression On peut ensuite initialiser un mod\u00e8le de r\u00e9gression lin\u00e9aire mco avec un objet \"LinearRegression\" : mco = LinearRegression() Pour donner le jeu d'entrainement (matrice des variables d'entr\u00e9e X et vecteur de la variable de sortie y ) \u00e0 ce mod\u00e8le, on utilise la m\u00e9thode : mco.fit(X,y) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions y_pred \u00e0 partir d'une matrice X_pred : y_pred = mco.predict(X_pred) Pour obtenir le \\(R^2\\) de notre mod\u00e8le sur ses donn\u00e9es d'entrainement X et y , il suffit d'utiliser la m\u00e9thode suivante : r_2 = mco.score(X,y) On peut de m\u00eame le calculer sur des donn\u00e9es de test. Malheureusement, contrairement \u00e0 Scipy, Scikit-Learn ne permet pas de faire l'inf\u00e9rence statistique avec les MCO : il n'y a pas de fonctionnalit\u00e9 pour d\u00e9terminer des intervalles de confiance ou de pr\u00e9diction. On doit donc calculer nous m\u00eame ces intervalles, \u00e0 partir de la loi de Student impl\u00e9ment\u00e9e par Scipy, et des formules g\u00e9n\u00e9ralis\u00e9es. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer la r\u00e9gression lin\u00e9aire avec les Moindres Carr\u00e9s Ordinaires \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, puis nous s\u00e9lectionnons les variables d'entr\u00e9e et de sortie sous la forme de matrices Numpy : df_dataset = pd.read_csv(input_path) df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) x_train = df_train['sunspots'].to_numpy() y_train = df_train['tsi'].to_numpy() x_test = df_test['sunspots'].to_numpy() y_test = df_test['tsi'].to_numpy() Si les hypoth\u00e8ses associ\u00e9es sont bien respect\u00e9es, les MCO permettent en th\u00e9orie d'obtenir des intervalles de confiance et de pr\u00e9diction fiables. Dans la r\u00e9alit\u00e9, il est compliqu\u00e9 d'avoir ces hypoth\u00e8ses exactement vraies. D'o\u00f9 le fait que l'on applique ici une strat\u00e9gie de s\u00e9paration entre ensemble d'entrainement et de test (80% / 20%), afin de v\u00e9rifier les performances en g\u00e9n\u00e9ralisation . On d\u00e9termine notre mod\u00e8le de r\u00e9gression lin\u00e9aire mco par les MCO en utilisant la biblioth\u00e8que Scipy : from scipy.stats import linregress mco = linregress(x_train,y_train) Nous pouvons \u00e0 pr\u00e9sent estimer des TSI \u00e0 partir de nombres de t\u00e2ches solaires. Voici par exemple pour 321 nombres de t\u00e2ches solaires entre 0 et 320 : x_mco = np.linspace(0,320,321) y_mco = mco.intercept+mco.slope*x_mco Nous avons ainsi les pr\u00e9dictions de notre mod\u00e8le pour chaque nombre entiers de t\u00e2ches solaires sur l'intervalle possible. Pour obtenir les intervalles de confiance et de pr\u00e9diction de notre mod\u00e8le, il nous faut d'abord estimer l'\u00e9cart-type des r\u00e9sidus : res_train = y_train-(mco.intercept+mco.slope*x_train) s = np.sqrt(np.sum(res_train**2)/(len(x_train)-2)) Ensuite, il nous faut calculer \\(sc_{xx}\\) : x_mean_train = np.mean(x_train) sc_xx_train = np.sum((x_train-x_mean_train)**2) Nous pouvons alors en d\u00e9duire l'estimation de l'\u00e9cart-type sur la moyenne conditionnelle de \\(y\\) , et l'estimation de l'\u00e9cart-type sur les pr\u00e9dictions : s_y_conf = s*np.sqrt((1/len(x_train))+(((x_mco-x_mean_train)**2)/sc_xx_train)) s_y_pred = s*np.sqrt(1+(1/len(x_train))+(((x_mco-x_mean_train)**2)/sc_xx_train)) Nous avons \u00e0 pr\u00e9sent tous les \u00e9lements pour calculer les intervalles de confiance et de pr\u00e9diction \u00e0 95% pour chaque nombre de t\u00e2ches solaires entre 0 et 320. Il suffit d'utiliser le t de Student adapt\u00e9 : from scipy.stats import t t_student = t.ppf(1-(0.05/2),len(x_train)-2) y_conf_inf = y_mco - (t_student*s_y_conf) y_conf_sup = y_mco + (t_student*s_y_conf) y_pred_inf = y_mco - (t_student*s_y_pred) y_pred_sup = y_mco + (t_student*s_y_pred) On peut tracer le mod\u00e8le lin\u00e9aire obtenu par les MCO par-dessus le nuage de points des donn\u00e9es d'entrainement, avec les intervalles de confiance et de pr\u00e9diction : Pour \u00e9valuer les performances du mod\u00e8le en entrainement, on peut simplement calculer le \\(R^2\\) sur les donn\u00e9es d'entrainement : print(mco.rvalue**2) On obtient \\(R^2 \\approx 0.792\\) , ce qui veut dire que 79.2% des \u00e9carts sont expliqu\u00e9s par le mod\u00e8le. Un score relativement bon, mais pas excellent puisque plus de 1/5 des \u00e9carts restent inexpliqu\u00e9s par le mod\u00e8le. Pour \u00e9valuer les performances du mod\u00e8le en test, on a pas d'autre choix que de calculer le \\(R^2\\) sur les donn\u00e9es de test \"\u00e0 la main\" : y_mean_test = np.mean(y_test) sct_test = np.sum((y_test-y_mean_test)**2) scr_test = np.sum((y_test-(mco.intercept+mco.slope*x_test))**2) r2_test = 1-(scr_test/sct_test) print(r2_test) On obtient alors \\(R^2 \\approx 0.795\\) soit un score tr\u00e8s similaire aux performances en entrainement. Des performances similaires peuvent donc \u00eatre attendues en g\u00e9n\u00e9ralisation. Pour valider ou invalider notre mod\u00e8le, et comprendre d'o\u00f9 viennent ses limites en terme de performances, affichons les r\u00e9sidus en fonction de la variable d'entr\u00e9e : On observe qu'il y a clairement une tendance des r\u00e9sidus en fonction de la variable d'entr\u00e9e. Les hypoth\u00e8ses sur les r\u00e9sidus ne sont donc pas respect\u00e9es, ce qui peut expliquer les performances modestes du mod\u00e8le. Pour d'obtenir un mod\u00e8le plus performant, nous proposons donc d'essayer une r\u00e9gression polynomiale de degr\u00e9 2. Cette r\u00e9gression polynomiale de degr\u00e9 2 va revenir \u00e0 faire une r\u00e9gression lin\u00e9aire multiple, avec pour variables d'entr\u00e9e le nombre de t\u00e2ches solaires et son carr\u00e9. Cette fois-ci, on va donc rassembler les variables d'entr\u00e9e au sein d'une matrice : x_train = np.ones((len(df_train),3)) x_train[:,1] = df_train['sunspots'].to_numpy() x_train[:,2] = x_train[:,1]**2 x_test = np.ones((len(df_test),3)) x_test[:,1] = df_test['sunspots'].to_numpy() x_test[:,2] = x_test[:,1]**2 La 1\u00e8re colonne correspond \u00e0 l'ordonn\u00e9e \u00e0 l'origine et ne contient que des 1, la 2nde colonne correspond au nombre de t\u00e2ches solaires, la 3\u00e8me au carr\u00e9 du nombre de t\u00e2ches solaires. La variable de sortie reste inchang\u00e9e par rapport \u00e0 la r\u00e9gression lin\u00e9aire simple. On d\u00e9termine notre mod\u00e8le de r\u00e9gression lin\u00e9aire multiple mco par les MCO en utilisant la biblioth\u00e8que Scikit-Learn : from sklearn.linear_model import LinearRegression mco = LinearRegression() mco.fit(x_train[:,1:],y_train) Attention , la m\u00e9thode fit des objets LinearRegression Scikit-Learn ne prend pas en entr\u00e9e la 1\u00e8re colonne de la matrice des variables d'entr\u00e9e (correspondant \u00e0 l'ordonn\u00e9e \u00e0 l'origine) ! Cependant, nous aurons bien besoin de la matrice compl\u00e8te pour calculer les intervalles de confiance et de pr\u00e9diction. Voici comment estimer les TSI correspondant \u00e0 321 de nombres de t\u00e2ches solaires entre 0 et 320 : x_mco = np.ones((100,3)) x_mco[:,1] = np.linspace(0,320,100) x_mco[:,2] = np.linspace(0,320,100)**2 y_mco = mco.predict(x_mco[:,1:]) Comme pour fit , la m\u00e9thode predict ne prend pas en entr\u00e9e la 1\u00e8re colonne de la matrice des variables d'entr\u00e9e. On calcule l'estimation de l'\u00e9cart-type des r\u00e9sidus, en faisant attention au fait qu'il y a maintenant une variable d'entr\u00e9e de plus : res_train = y_train-mco.predict(x_train[:,1:]) s = np.sqrt(np.sum(res_train**2)/(len(x_train)-3)) On peut alors estimer les \u00e9cart-types sur la moyenne conditionnelle de \\(y\\) , et sur les pr\u00e9dictions. Ce calcul se fait en s\u00e9lectionnant chaque ligne de la matrice x_mco cr\u00e9e pr\u00e9c\u00e9demment, de mani\u00e8re it\u00e9rative : s_y_conf = np.zeros(len(x_mco)) s_y_pred = np.zeros(len(x_mco)) for idx in range(len(x_mco)): x_mco_idx = x_mco[idx,:] s_y_conf[idx] = s*np.sqrt(x_mco_idx@np.linalg.inv(x_train.T@x_train)@x_mco_idx.T) s_y_pred[idx] = s*np.sqrt(1+x_mco_idx@np.linalg.inv(x_train.T@x_train)@x_mco_idx.T) On peut \u00e0 pr\u00e9sent d\u00e9terminer les intervalles de confiance et de pr\u00e9diction de notre mod\u00e8le lin\u00e9aire multiple : from scipy.stats import t t_student = t.ppf(1-(0.05/2),len(x_train)-3) conf_inf = y_mco - (t_student*s_y_conf) conf_sup = y_mco + (t_student*s_y_conf) pred_inf = y_mco - (t_student*s_y_pred) pred_sup = y_mco + (t_student*s_y_pred) Il est maintenant possible d'afficher par dessus le nuage de points des donn\u00e9es d'entrainement le mod\u00e8le polynomial, ainsi que ses intervalles de confiance et de pr\u00e9diction : Pour \u00e9valuer les performances du mod\u00e8le en entrainement et en test, on calcule le \\(R^2\\) : print(mco.score(x_train[:,1:],y_train)) On obtient \\(R^2 \\approx 0.896\\) sur les donn\u00e9es d'entrainement, ce qui veut dire que 89.6% des \u00e9carts sont expliqu\u00e9s par le mod\u00e8le. Un tr\u00e8s bon score ! Maintenant seul 1/10 des \u00e9carts restent inexpliqu\u00e9s par le mod\u00e8le. En test, on obtient un score tr\u00e8s similaire de \\(R^2 \\approx 0.904\\) , ce qui laisse pr\u00e9sager ce niveau de performance en g\u00e9n\u00e9ralisation. Pour valider ou invalider notre mod\u00e8le, v\u00e9rifions que les r\u00e9sidus en fonction de la variable d'entr\u00e9e se comportent conform\u00e9ment \u00e0 nos hypoth\u00e8ses : Cette fois-ci, on n'observe aucune tendance claire se d\u00e9gager des r\u00e9sidus en fonction de la variable d'entr\u00e9e : ce qui est attendu. Par contre, on peut noter que l'\u00e9cart-type des r\u00e9sidus a l'air de varier l\u00e9g\u00e8rement avec le nombre de t\u00e2ches solaires, et l'hypoth\u00e8se de normalit\u00e9 est assez difficile \u00e0 confirmer. C'est pourquoi il faudrait en toute rigueur appliquer des tests statistiques de nos hypoth\u00e8ses. Remarques La m\u00e9thode des Moindres Carr\u00e9s Ordinaire a les avantages suivants : Elle est relativement simple \u00e0 mettre en place, avec peu de param\u00e8tres , et aucun hyperparam\u00e8tre . Les pr\u00e9dictions qu'elle r\u00e9alise sont compl\u00e8tement expliqu\u00e9es et interpr\u00e9tables : un humain peut les comprendre. On peut \u00e9tablir des intervalles de confiance sur les pr\u00e9dictions. Une fois le mod\u00e8le entrain\u00e9, le temps de calcul des pr\u00e9dictions est rapide (lin\u00e9aire par rapport au nombre de pr\u00e9dictions). Mais cette m\u00e9thode a aussi les limites suivantes : Le temps de calcul de la SVD augmente quadratiquement avec le nombre de variables explicatives ! Elle demande beaucoup de m\u00e9moire pour manipuler la matrice \\(X\\) . Ces 2 d\u00e9savantages sont les raisons pour lesquelles on utilise tr\u00e8s peu les MCO dans les cas o\u00f9 le nombre de variables explicatives est grand . M\u00eame s'il est possible d'utiliser les MCO pour de la r\u00e9gression non-lin\u00e9aire, avec l'astuce de la r\u00e9gression polynomiale, plus l'ordre du polyn\u00f4me est grand et plus le nombre de variables explicatives est grand. On utilise donc rarement les MCO pour des mod\u00e8les non-lin\u00e9aires complexes . Perceptron multicouche Perceptron multicouche pour la r\u00e9gression Nous avons vu lors du chapitre pr\u00e9c\u00e9dent que le perceptron multicouche (PMC) est un r\u00e9seau de neurones initialement invent\u00e9 pour r\u00e9soudre des probl\u00e8mes de classification binaire, qui peut \u00eatre adapt\u00e9 pour r\u00e9soudre n'importe quel probl\u00e8me de classification. Et bien le PMC peut \u00e9galement \u00eatre utilis\u00e9 pour r\u00e9soudre des probl\u00e8mes de r\u00e9gression non-lin\u00e9aire , \u00e0 condition de choisir des hyperparam\u00e8tres adapt\u00e9s. Si on applique le th\u00e9or\u00e8me de l'approximation universelle \u00e0 la r\u00e9gression, il en ressort que le PMC est capable de tracer n'importe quelle relation , \u00e0 condition d'avoir assez de neurones en couche cach\u00e9e. D'o\u00f9 l'int\u00e9r\u00eat de ce type de mod\u00e8le. Pour adapter le PMC \u00e0 la r\u00e9gression, il faut tout d'abord adapter l' architecture : Le nombre d'entr\u00e9es correspond au nombre de variables d'entr\u00e9e \\(n\\) du probl\u00e8me. Le nombre de sorties correspond au nombre de variables de sortie du probl\u00e8me (1 dans notre cas, mais on peut imaginer un mod\u00e8le multi-sorties). Ensuite, il faut choisir une fonction de co\u00fbt pertinente pour l'entrainement. La fonction la plus couramment utilis\u00e9e est l' erreur quadratique moyenne (not\u00e9e \"MSE\" en anglais) : \\(MSE = \\frac{1}{p} \\sum_{i=1}^{p} (y_i-\\hat{y_i})^2\\) On reconnait une g\u00e9n\u00e9ralisation de la formule des \"moindres carr\u00e9s\" utilis\u00e9e pr\u00e9c\u00e9demment. D'autres fonctions donnant plus ou moins de poids aux outliers existent. Autre particularit\u00e9 du PMC pour la r\u00e9gression : nous ne voulons pas en sortie un seuil entre 0 et 1 (ou -1 et 1), mais une valeur continue correspondant \u00e0 la variable de sortie pr\u00e9dite. Il faut donc que le neurone de sortie soit sans fonction de seuil . Et voil\u00e0, nous avons un PMC adapt\u00e9 \u00e0 la r\u00e9gression ! De la m\u00eame mani\u00e8re que pour la classification, pour un nombre de neurones par couche cach\u00e9e donn\u00e9, plus on aura de couches et plus complexes les fronti\u00e8res de d\u00e9cisions pourront \u00eatre. Et d\u00e8s que l'on a plus d'une couche cach\u00e9e, on parle d' apprentissage profond (\"Deep Learning\"). Nota Bene Pour que l'apprentissage d'un PMC pour de la r\u00e9gression se d\u00e9roule correctement, il est recommand\u00e9 d'effectuer une transformation des donn\u00e9es d'entr\u00e9e et de sortie (voir Chapitre 1). Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn du PMC pour la r\u00e9gression. Elle peut \u00eatre import\u00e9e avec : from sklearn.neural_network import MLPRegressor On peut ensuite initialiser un PMC pour de la r\u00e9gression mlp avec les hyperparam\u00e8tres par d\u00e9faut en utilisant la commande : mlp = MLPRegressor() Nous verrons plus loin quels sont ces hyperparam\u00e8tres. Pour donner le jeu d'entrainement (variables d'entr\u00e9e avec x_train et variables de sortie avec y_train ) \u00e0 ce mod\u00e8le, on utilise la m\u00e9thode : mlp.fit(x_train,y_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions y_test \u00e0 partir de variables d'entr\u00e9e de test x_test : y_test = mlp.predict(x_test) Si on veut effectuer un test de notre mod\u00e8le de r\u00e9gression sur un jeu de donn\u00e9es, on peut obtenir un \\(R^2\\) avec la commande : mlp.score(x_test,y_test) Voici les hyperparam\u00e8tres par d\u00e9faut de l'impl\u00e9mentation Scikit-Learn du PMC : Nombre de couches cach\u00e9es : 1 Nombre de neurones par couche cach\u00e9e : 100 La fen\u00eatre d'activation pour les couches cach\u00e9es : ReLU La fen\u00eatre d'activation pour la couche de sortie : Aucune. Le taux d'apprentissage pour la descente de gradient : 0.001 Le nombre maximum d'\u00e9poques d'apprentissage : 200 (par d\u00e9faut sans arr\u00eat pr\u00e9matur\u00e9, mais on peut l'activer) La fonction de co\u00fbt : erreur quadratique moyenne Ces hyperparam\u00e8tres sont presque tous modifiables par l'utilisateur. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent entrainer un PMC \u00e0 r\u00e9soudre notre probl\u00e8me de r\u00e9gression. Comme pr\u00e9c\u00e9demment, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, puis nous s\u00e9lectionnons les variables d'entr\u00e9e et de sortie sous la forme de matrices Numpy : df_dataset = pd.read_csv(input_path) df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) x_train = df_train[['sunspots']].to_numpy().reshape(-1, 1) y_train = df_train[['tsi']].to_numpy().reshape(-1, 1) x_test = df_test[['sunspots']].to_numpy().reshape(-1, 1) y_test = df_test[['tsi']].to_numpy().reshape(-1, 1) Afin d'aider le PMC \u00e0 converger, nous allons effectuer une transformation centrage-r\u00e9duction des entr\u00e9es et des sorties (voir Chapitre 1). C'est pourquoi nous avons fait attention \u00e0 ce que les dimensions des variables d'entr\u00e9e et de sortie soient celles attendues par StandardScaler . Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler x_scaler = StandardScaler() x_scaler.fit(x_train) x_train = x_scaler.transform(x_train) x_test = x_scaler.transform(x_test) y_scaler = StandardScaler() y_scaler.fit(y_train) y_train = y_scaler.transform(y_train) y_test = y_scaler.transform(y_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre mod\u00e8le de r\u00e9gression. Voici comment initialiser un PMC pour de la r\u00e9gression, avec les param\u00e8tres par d\u00e9faut : from sklearn.neural_network import MLPRegressor mlp = MLPRegressor() Pour l'entrainer, il nous suffit ensuite d'utiliser la commande suivante : mlp.fit(x_train,y_train.ravel()) On a fait attention ici \u00e0 ce que les dimensions de la sortie soient celles attendues par MLPRegressor . Et pour obtenir le \\(R^2\\) de notre mod\u00e8le en entrainement et en test : print(mlp.score(x_train,y_train)) print(mlp.score(x_test,y_test)) Pour r\u00e9aliser des pr\u00e9dictions y_predict \u00e0 partir d'entr\u00e9es x_predict , il ne faudra pas oublier d'effectuer une transformation centrage-r\u00e9duction inverse des sorties : x_scaler.fit(x_predict) y_predict = mlp.predict(x_predict) y_scaler.inverse_transform(y_predict) Mais comme nous l'avons fait remarquer dans le chapitre pr\u00e9c\u00e9dent, le PMC est sensible au sur-apprentissage. Pour cette raison, on peut vouloir appliquer la m\u00e9thode de r\u00e9gularisation par \"arr\u00eat pr\u00e9matur\u00e9\" (voir Chapitre 1). Si on active le param\u00e8tre early_stopping du classifieur PMC de Scikit-Learn, au moment de l'apprentissage il va automatiquement mettre de c\u00f4t\u00e9 une partie du jeu d'entrainement pour faire un jeu de validation. On peut m\u00eame choisir la fraction du jeu d'entrainement \u00e0 utiliser pour la validation, avec le param\u00e8tre validation_fraction . Voici un exemple de d\u00e9finition d'un classifieur, avec de l'arr\u00eat pr\u00e9matur\u00e9 et 20% des donn\u00e9es d'entrainement utilis\u00e9es pour la validation : mlp = MLPRegressor(early_stopping=True,validation_fraction=0.2) Il y a aussi une astuce pour afficher l'\u00e9valuation de la fonction de co\u00fbt au cours des \u00e9poques, pour l'ensemble d'entrainement et de validation. Elle se base sur : Subdiviser le jeu d'entrainement en un nouveau jeu d'entrainement et un jeu de validation. Utiliser la m\u00e9thode partial_fit de notre PMC, qui permet de r\u00e9aliser une it\u00e9ration \u00e0 la fois. R\u00e9cup\u00e9rer la valeur de la fonction de co\u00fbt sur le jeu d'entrainement, avec l'attribut loss_ de notre classifieur. Evaluer la valeur de la fonction de co\u00fbt sur le jeu de validation, en utilisant la fonction mean_squared_error de Scikit-Learn. Voici l'affichage des 2 courbes Matplotlib obtenues sur notre base de donn\u00e9es, pour 50 \u00e9poques : import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error df_train2=df_train.sample(frac=0.8,random_state=0) df_validation=df_dataset.drop(df_train2.index) x_train = df_train2[['sunspots']].to_numpy().reshape(-1, 1) y_train = df_train2[['tsi']].to_numpy().reshape(-1, 1) x_validation = df_validation[['sunspots']].to_numpy().reshape(-1, 1) y_validation = df_validation[['tsi']].to_numpy().reshape(-1, 1) x_scaler = StandardScaler() x_scaler.fit(x_train) x_train = x_scaler.transform(x_train) x_validation = x_scaler.transform(x_validation) y_scaler = StandardScaler() y_scaler.fit(y_train) y_train = y_scaler.transform(y_train) y_validation = y_scaler.transform(y_validation) mlp = MLPRegressor() loss_train = [] loss_validation = [] for idx in range(50): mlp.partial_fit(x_train,y_train.ravel()) loss_train.append(mlp.loss_) loss_validation.append(mean_squared_error(y_validation,mlp.predict(x_validation))) plt.plot(loss_train, label=\"train loss\",c='r') plt.plot(loss_validation, label=\"validation loss\",c='g') plt.xlabel('Epoques') plt.ylabel('Fonction de co\u00fbt') plt.legend() Voici un exemple de mod\u00e8le de r\u00e9gression obtenu avec un PMC ayant les hyperparam\u00e8tres par d\u00e9faut : On voit bien que le PMC a \u00e9t\u00e9 capable d'apprendre une fonction non-lin\u00e9aire entre les variables d'entr\u00e9e et de sortie. Avec ce mod\u00e8le, on obtient \\(R^2 \\approx 0.900\\) en entrainement, et \\(R^2 \\approx 0.907\\) en test. On s'attend donc \u00e0 de tr\u00e8s bonnes performances en g\u00e9n\u00e9ralisation pour ce mod\u00e8le, mais on peut probablement obtenir mieux avec d'autres hyperparam\u00e8tres. Il faudrait donc \u00e0 pr\u00e9sent se baser sur ces codes pour effectuer une optimisation des hyperparam\u00e8tres. Remarques La m\u00e9thode du Perceptron Multi-Couche a les avantages suivants pour la r\u00e9gression : Elle permet de dessiner de d\u00e9terminer des relations complexes entre les variables d'entr\u00e9e et de sortie sans faire de grosses hypoth\u00e8ses statistiques au pr\u00e9alable. Pour des nombres de variables d'entr\u00e9es importants , et des nombres d'\u00e9chantillons importants , l'entrainement d'un PMC devient plus int\u00e9ressant en termes de temps de calcul que les MCO. Une fois le mod\u00e8le entrain\u00e9, l' espace m\u00e9moire pour contenir le mod\u00e8le est plus faible que celui n\u00e9cessaire pour manipuler le \\(X\\) des MCO Mais cette m\u00e9thode a aussi les limites suivantes, les m\u00eames que pour la classification : Elle a de nombreux param\u00e8tres et hyperparam\u00e8tres \u00e0 optimiser. Elle est sensible au sur-apprentissage . Les d\u00e9cisions qu'elle prend sont difficilement expliqu\u00e9es et interpr\u00e9tables : on a aucun intervalle de confiance ou de pr\u00e9diction sur les sorties. On parle encore une fois de \"bo\u00eete noire\". L'initialisation de la m\u00e9thode se faisant de mani\u00e8re al\u00e9atoire , 2 apprentissages ne donneront pas exactement le m\u00eame mod\u00e8le. Comme nous l'avons d\u00e9j\u00e0 \u00e9voqu\u00e9, les r\u00e9seaux de neurones sont \u00e0 la base des mod\u00e8les d'apprentissage modernes, fondant ainsi une nouvelle sous-discipline : l' apprentissage profond .","title":"III. R\u00e9gression"},{"location":"Chap3_Regression/#chapitre-iii-regression","text":"Ce chapitre est une introduction \u00e0 la r\u00e9gression : principe, mesures de performances et m\u00e9thodes de base.","title":"Chapitre III : R\u00e9gression"},{"location":"Chap3_Regression/#probleme-de-regression","text":"Comme mentionn\u00e9 lors du Chapitre I, par \" r\u00e9gression \" on entend associer une r\u00e9alisation d'une variable quantitative continue \u00e0 un individu (labels), \u00e0 partir des r\u00e9alisations d'autres variables (features). On cherche donc une relation entre les variables d'entr\u00e9e (aussi appel\u00e9es \"variables explicatives\") et la variable de sortie (aussi appel\u00e9e \"variable de r\u00e9ponse\"). Il existe 2 grands types de relations entre variables : La relation est d\u00e9terministe si on consid\u00e8re que la variable de sortie peut \u00eatre connue exactement \u00e0 partir des variables d'entr\u00e9e. Par exemple, si je veux d\u00e9terminer le rayon \\(R\\) d'une \u00e9toile par rapport \u00e0 sa luminosit\u00e9 \\(L\\) et sa temp\u00e9rature \\(T\\) , en m'appuyant sur la th\u00e9orie du corps noir, j'utilise la formule \\(R = \\sqrt{\\frac{L}{4 \\pi \\sigma T^4}}\\) . La relation est probabiliste si on consid\u00e8re que d' autres facteurs que les variables d'entr\u00e9e vont influer sur la valeur de la variable de sortie. Dans ce cas, une m\u00eame r\u00e9alisation des variables d'entr\u00e9e pourra \u00eatre associ\u00e9e \u00e0 plusieurs valeurs de la variable de sortie, et inversement. Mais si les variables d'entr\u00e9e et de sortie sont corr\u00e9l\u00e9es , la relation sera tout de m\u00eame utile pour r\u00e9aliser des pr\u00e9dictions avec une certaine marge d'erreur. C'est souvent le cas en Physique, lorsque l'on r\u00e9alise des mesures pour expliquer un ph\u00e9nom\u00e8ne. Si on reprend notre exemple pr\u00e9c\u00e9dent : la th\u00e9orie du corps noir n'explique pas parfaitement la rayonnement d'une \u00e9toile, et on peut avoir des erreurs de mesures de \\(L\\) , \\(T\\) et \\(R\\) . Mais si \\(L\\) , \\(T\\) et \\(R\\) sont correl\u00e9es, alors on peut essayer de pr\u00e9dire \\(R\\) \u00e0 partir de \\(L\\) et \\(T\\) , moyennant une certaine erreur. C'est tout le principe de la r\u00e9gression : d\u00e9terminer une relation probabiliste entre les variables d'entr\u00e9e et la variable de sortie . Nous avions aussi mentionn\u00e9 lors du Chapitre I qu'entrainer un mod\u00e8le de r\u00e9gression ne peut se faire que par apprentissage supervis\u00e9 . L'id\u00e9e est encore une fois que le mod\u00e8le soit ensuite capable de g\u00e9n\u00e9raliser \u00e0 de nouvelles observations. Anecdote historique Le nom de \"r\u00e9gression\" vient du g\u00e9n\u00e9ticien Francis Galton, qui l'utilisa pour son \u00e9tude sur la \"r\u00e9gression vers la moyenne\". En fait, Galton cherchait \u00e0 mod\u00e9liser un ph\u00e9nom\u00e8ne d'h\u00e9r\u00e9dit\u00e9 qu'il observait dans la population humaine : La taille des fils avait tendance \u00e0 se rapprocher de la moyenne de la population par rapport \u00e0 celle de leur p\u00e8re.","title":"Probl\u00e8me de r\u00e9gression"},{"location":"Chap3_Regression/#les-differents-types-de-regression","text":"Suivant le nombre d'entr\u00e9es et le type de mod\u00e8le \u00e0 ajuster aux donn\u00e9es, on a diff\u00e9rents types de probl\u00e8mes de r\u00e9gression. Nous allons voir ici les 3 plus courants.","title":"Les diff\u00e9rents types de r\u00e9gression"},{"location":"Chap3_Regression/#regression-lineaire-simple","text":"La r\u00e9gression lin\u00e9aire simple est le probl\u00e8me de r\u00e9gression le plus basique qui soit. On recherche une relation lin\u00e9aire entre une variable d'entr\u00e9e \\(x\\) et une variable de sortie \\(y\\) . L'erreur est une variable al\u00e9atoire not\u00e9e \\(\\epsilon\\) . Le mod\u00e8le \u00e0 ajuster est le suivant : \\(y = \\alpha x + \\beta + \\epsilon\\) avec les param\u00e8tres \\(\\alpha\\) et \\(\\beta\\) \u00e0 d\u00e9terminer. Il s'agit d'un probl\u00e8me d' inf\u00e9rence statistique : nous disposons d'un jeu d'entrainement qui est un \u00e9chantillon de la population totale, et nous voulons en d\u00e9duire une estimation de \\(\\alpha\\) et \\(\\beta\\) nous permettant de r\u00e9aliser des pr\u00e9dictions. Nous verrons que l'on fait en g\u00e9n\u00e9ral les hypoth\u00e8ses suivantes sur \\(\\epsilon\\) : Ind\u00e9pedance de ses r\u00e9alisations. Moyenne nulle . Ecart-type constant avec \\(x\\) . Afin de pouvoir donner un \"intervalle de confiance\" aux pr\u00e9dictions, on va en plus ajouter une hypoth\u00e8se de normalit\u00e9 : \\(\\epsilon\\) suit une loi normale. Nous en reparlerons plus tard.","title":"R\u00e9gression lin\u00e9aire simple"},{"location":"Chap3_Regression/#regression-lineaire-multiple","text":"On peut g\u00e9n\u00e9raliser le mod\u00e8le de la section pr\u00e9c\u00e9dente aux probl\u00e8mes avec plusieurs variables d'entr\u00e9e . Si on note \\(x_1\\) , \\(x_2\\) , ..., \\(x_n\\) les \\(n\\) variables d'entr\u00e9e, et \\(y\\) notre variable de sortie. Le mod\u00e8le \u00e0 ajuster devient : \\(y = \\alpha_1 x_1 + \\alpha_2 x_2 + ... + \\alpha_n x_n + \\beta + \\epsilon\\) On reconnait la formule d'un hyperplan de dimension \\(n\\) . On peut mettre cette formule sous forme matricielle : \\(y = A.x + \\epsilon\\) avec \\(A = \\begin{pmatrix} \\beta\\\\ \\alpha_1\\\\ \\alpha_2\\\\ \\vdots\\\\ \\alpha_n \\end{pmatrix}\\) et \\(x = \\begin{pmatrix} 1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\) Nous verrons dans la suite comment g\u00e9n\u00e9raliser les m\u00e9thodes aux probl\u00e8mes multiples.","title":"R\u00e9gression lin\u00e9aire multiple"},{"location":"Chap3_Regression/#regression-polynomiale","text":"Comment faire lorsqu'un mod\u00e8le lin\u00e9aire n'est pas pertinent pour repr\u00e9senter la relation entre nos variables ? Afin de r\u00e9aliser une r\u00e9gression non-lin\u00e9aire , il y a une astuce : on cherche \u00e0 ajuster un mod\u00e8le polynomial . Pour un ordre \\(k\\) , il aura la forme : \\(y = \\alpha_1 x + \\alpha_2 x^2 + ... + \\alpha_n x^n + b + \\epsilon\\) Il y a alors une astuce : si on consid\u00e8re \\(x\\) , \\(x^2\\) , ..., \\(x^n\\) comme \\(n\\) variables d'entr\u00e9e, alors on reconnait un probl\u00e8me de r\u00e9gression multiple ! On peut donc le traiter avec les m\u00eames m\u00e9thodes qu'un probl\u00e8me lin\u00e9aire. Il existe d'autres techniques d'ajustement de mod\u00e8les non-lin\u00e9aires, que nous ne traiterons pas dans le cadre de ce cours.","title":"R\u00e9gression polynomiale"},{"location":"Chap3_Regression/#exemple-de-probleme","text":"Est-il possible de pr\u00e9dire l'intensit\u00e9 du rayonnement solaire \u00e0 partir du nombre de taches solaires ? Les taches solaires sont des zones de \"faible\" temp\u00e9rature (4500 K contre 5800 K) la surface du soleil (photosph\u00e8re). Elles apparaissent lorsque l'apport de chaleur \u00e0 la surface par convection est inhib\u00e9 par une concentration de lignes de champs magn\u00e9tique. On sait observer les taches solaires depuis l'antiquit\u00e9, et on a des mesures du nombre taches solaires depuis le XVII\u00e8me, qui varie entre 0 et 350 environ. On l'utilise comme un marqueur de l'activit\u00e9 solaire depuis le milieu XIX\u00e8me si\u00e8cle. Un autre marqueur connu de l'activit\u00e9 solaire est l'\"irradiance solaire\" totale ou TSI en anglais. Il s'agit de la puissance du rayonnement solaire re\u00e7ue en haut de l'atmosph\u00e8re ter restre, par unit\u00e9 de surface. Sa valeur est d'environ 1363 \\(W/m^2\\) , avec de l\u00e9g\u00e8res variations suivant l'activit\u00e9 solaire. La TSI est un param\u00e8tre cl\u00e9 en climatologie, puisqu'il correspond \u00e0 l'\u00e9nergie apport\u00e9e par le soleil \u00e0 la Terre. Mais sa mesure n'est pas ais\u00e9e : il ne peut \u00eatre obtenu que par des satellites. Voici l'\u00e9volution du nombre de taches solaires et du TSI depuis 1948 jusqu'\u00e0 2024 : On a ici 20 points par an, avec un filtrage de moyenne glissante sur une 1/2 ann\u00e9e, soit 1531 points au total. Elles sont issues de l'Observatoire Royal de Belgique (SILSO, Dewitte et al. 2022). On observe que le nombre de taches solaires comme la TSI suivent les m\u00eames cycles de 11 ans environ, correspondant aux cycles de l'activit\u00e9 solaire. On imagine alors que la corr\u00e9lation entre les 2 grandeurs doit \u00eatre forte. D'o\u00f9 l'id\u00e9e suivante : peut-on entrainer un mod\u00e8le \u00e0 estimer la TSI \u00e0 partir du nombre de taches solaires ? Voici les donn\u00e9es d'o\u00f9 sont issues les courbes pr\u00e9c\u00e9dentes, au format CSV : Chap3_sunspots_dataset Le tableau de donn\u00e9es qu'il contient est de la forme : year tsi sunspots 1948.25 1363.743 193.667 1948.30 1363.729 196.541 1948.35 1363.722 205.891 1948.40 1363.713 215.623 1948.45 1363.729 218.060 ... ... ... 2024.65 1364.015 172.536 2024.70 1364.013 170.525 2024.75 1364.038 171.563 Il contient pour chacun des 1531 points l'ann\u00e9e (sous forme d\u00e9cimale), la TSI moyenne (en \\(W/m^2\\) ) et le nombre de taches solaires moyen sur une fen\u00eatre d'une 1/2 ann\u00e9e. Notre probl\u00e8me de r\u00e9gression sera la suivant : pr\u00e9dire la TSI moyenne sur une 1/2 ann\u00e9e \u00e0 partir du nombre de taches solaires moyen sur cette m\u00eame fen\u00eatre . Voyons d'abord si une telle r\u00e9gression est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) On peut alors utiliser la m\u00e9thode \"plot\" des DataFrames pandas pour afficher la TSI en fonction du nombre de taches solaires, sous la forme d'un nuage de points : df_dataset.plot(x='sunspots',y='tsi',kind='scatter',c='r',marker='+') Voici le r\u00e9sultat : On observe comme attendu que les 2 grandeurs ont l'air fortement corr\u00e9l\u00e9es . Cependant, on peut d\u00e9j\u00e0 constater que : (1) la relation n'a l'air lin\u00e9aire que pour des nombres de taches solaires faibles (moins de 150-200), (2) la dispersions des points a l'air d'augmenter avec le nombre de taches solaires. Ces observations seront importantes dans la suite. On peut \u00e9galement calculer le coefficient de corr\u00e9lation entre la TSI et le nombre taches solaires, en utilisant la m\u00e9thode \"corr\" des DataFrames Pandas : df_dataset['tsi'].corr(df_dataset['sunspots']) On trouve un coefficient de corr\u00e9lation de 0.89 environ, ce qui confirme une forte corr\u00e9lation entre les variables. Vouloir entrainer un mod\u00e8le \u00e0 pr\u00e9dire la TSI \u00e0 partir du nombre de taches solaires a donc un sens. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP.","title":"Exemple de probl\u00e8me"},{"location":"Chap3_Regression/#mesures-de-performances","text":"Nous allons passer en revue dans cette section les principaux indicateurs de performances applicables \u00e0 la r\u00e9gression lin\u00e9aire.","title":"Mesures de performances"},{"location":"Chap3_Regression/#table-anova","text":"Soit un probl\u00e8me de r\u00e9gression dont la variable de sortie est not\u00e9e \\(y\\) . On veut \u00e9valuer les performances d'un mod\u00e8le de r\u00e9gression sur un jeu de donn\u00e9es issu de ce probl\u00e8me. La moyenne des valeurs de \\(y\\) au sein de cet \u00e9chantillon est not\u00e9e \\(\\overline{y}\\) . La valeur de \\(y\\) pour le i-\u00e8me individu de cet \u00e9chantillon sera not\u00e9e \\(y_i\\) . Admettons que l'on a ait d\u00e9termin\u00e9 un mod\u00e8le de regression lin\u00e9aire pour ces donn\u00e9es. On note \\(\\hat{y_i}\\) la pr\u00e9diction de ce mod\u00e8le lin\u00e9aire pour le i-\u00e8me individu. Pour juger de la qualit\u00e9 du mod\u00e8le, on divise les \u00e9carts en 2 groupes : Les \u00e9carts r\u00e9siduels , ou \"r\u00e9sidus\" : \\(y_i - \\hat{y_i}\\) Il s'agit des \u00e9carts non-expliqu\u00e9s par le mod\u00e8le. On remarque qu'ils correspondent aux \\(\\epsilon_i\\) de notre mod\u00e8le. Les \u00e9carts \u00e9carts de r\u00e9gression , ou \"\u00e9carts expliqu\u00e9s\" : \\(\\hat{y_i} - \\overline{y}\\) Il s'agit des \u00e9carts expliqu\u00e9s par le mod\u00e8le. On a alors l' \u00e9cart total : \\(y_i - \\overline{y} = (y_i - \\hat{y_i}) - (\\hat{y_i} - \\overline{y})\\) On met en g\u00e9n\u00e9ral ces \u00e9carts sous la forme de variances, en prenant la somme des carr\u00e9s des \\(p\\) individus de cet \u00e9chantillon : SCR (\"somme des carr\u00e9s des r\u00e9sidus\") : \\(\\sum_{i=1}^{p} (y_i - \\hat{y_i})^2\\) SCE (\"somme des carr\u00e9s expliqu\u00e9s\") : \\(\\sum_{i=1}^{p} (\\hat{y_i} - \\overline{y})^2\\) SCT (\"somme des carr\u00e9s totale\") : \\(\\sum_{i=1}^{p} (y_i - \\overline{y})^2\\) avec \\(SCT = SCR + SCE\\) Un mod\u00e8le sera d'autant plus performant que la SCR sera faible compar\u00e9e \u00e0 la SCT . L'id\u00e9e est la suivante : plus la SCE est grande (et donc plus la SCR est faible), et plus le mod\u00e8le explique \\(y\\) \u00e0 partir des entr\u00e9es. On range en g\u00e9n\u00e9ral ces valeurs sous la forme d'un tableau, nomm\u00e9 \"table ANOVA\" (contraction en anglais de \"ANalysis Of VAriance\") : \\(x_i\\) \\(y_i\\) \\(\\hat{y_i}\\) \\(\\hat{y_i} - \\overline{y}\\) \\((\\hat{y_i} - \\overline{y})^2\\) \\(y_i - \\hat{y_i}\\) \\((y_i - \\hat{y_i})^2\\) ... ... ... ... ... ... ... \\(\\overline{x}\\) \\(\\overline{y}\\) SCE SCR SCT ... ... ... ... ... On peut trouver des variantes de cette table, mais elle contient toujours au moins la SCE, la SCR et la SCT.","title":"Table ANOVA"},{"location":"Chap3_Regression/#coefficient-de-determination","text":"La table ANOVA est une repr\u00e9sentation plut\u00f4t exhaustive des performances en r\u00e9gression lin\u00e9aire. Mais comme souvent, on voudrait pouvoir r\u00e9sumer au mieux les performances avec un score unique d\u00e9riv\u00e9 de cette table. Le crit\u00e8re le plus utilis\u00e9 est le coefficient de d\u00e9termination , not\u00e9 \\(R^2\\) : \\(R^2 = \\frac{SCE}{SCT} = \\frac{\\sum_{i=1}^{p} (\\hat{y_i} - \\overline{y})^2}{\\sum_{i=1}^{p} (y_i - \\overline{y})^2} = 1 - \\frac{SCR}{SCT} = 1 - \\frac{\\sum_{i=1}^{p} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{p} (y_i - \\overline{y})^2}\\) Le \\(R^2\\) s'interpr\u00e8te comme la proportion de l'\u00e9cart total expliqu\u00e9e par le mod\u00e8le . Il s'agit donc d'un score entre 0 et 1 : plus la valeur est proche de 1, et meilleur est le mod\u00e8le. Par exemple, mettons que l'on utilise la luminosit\u00e9 d'une \u00e9toile pour essayer de pr\u00e9dire son rayon, gr\u00e2ce \u00e0 une r\u00e9gression lin\u00e9aire. Si le \\(R^2\\) du mod\u00e8le est de 0.75 sur un \u00e9chantillon de donn\u00e9es, cela veut dire que le mod\u00e8le explique 75% de la variation du rayon de l'\u00e9toile. Les 25% restants sont expliqu\u00e9s par les erreurs. On remarque que le \\(R^2\\) correspond au carr\u00e9 du coefficient de corr\u00e9lation (voir Chapitre 1) entre les valeurs observ\u00e9es \\(y_i\\) et les valeurs pr\u00e9dites \\(\\hat{y_i}\\) . Nota Bene En r\u00e9gression lin\u00e9aire simple, le \\(R^2\\) est \u00e9gal au carr\u00e9 du coefficient de corr\u00e9lation entre \\(x\\) et \\(y\\) . Ce n'est pas vrai pour la r\u00e9gression lin\u00e9aire multiple.","title":"Coefficient de d\u00e9termination"},{"location":"Chap3_Regression/#analyse-des-residus","text":"Lorsque les performances d'un mod\u00e8le de r\u00e9gression lin\u00e9aire ont l'air mauvaises, on a envie de comprendre pourquoi. La bonne approche est de r\u00e9aliser une analyse des r\u00e9sidus . Dans un 1er temps, cette analyse peut \u00eatre visuelle . On affiche simplement les r\u00e9sidus en fonction de \\(x\\) , ou sous la forme d'un histogramme, et on v\u00e9rifie s'ils ont l'air d'avoir le comportement attendu de \\(\\epsilon\\) : Ind\u00e9pendance des observations. Moyenne nulle. Ecart-type constant, aussi appel\u00e9 \"homosc\u00e9dasticit\u00e9\". Normalit\u00e9. Dans l'id\u00e9al, on attend donc un nuage de points al\u00e9atoires , d'\u00e9cart-type constant, sans tendances en fonction de \\(x\\) . Si ce n'est pas le cas, alors il faut soit : Revoir notre mod\u00e8le (une r\u00e9gression lin\u00e9aire simple n'est peut-\u00eatre pas adapt\u00e9e). Nettoyer nos donn\u00e9es (des outliers ou des donn\u00e9es ab\u00e9rrantes sont peut-\u00eatre la cause du mauvais ajustement). Ajouter des variables explicatives ( \\(x\\) n'est peut-\u00eatre pas suffisant pour expliquer \\(y\\) de mani\u00e8re satisfaisante). En cas de doute, on peut proc\u00e9der \u00e0 des tests de ces hypoth\u00e8ses, mais ils ne sont pas tous simples \u00e0 mettre en place. En voici quelques exemples : Hypoth\u00e8se Test Normalit\u00e9 Droite de Henry (quantiles des r\u00e9sidus en fonction de ceux attendus) Homosc\u00e9dasticit\u00e9 Test de White (hypoth\u00e8se nulle : variance des r\u00e9sidus sachant \\(x\\) constante) Ind\u00e9pendance Test de Durbin-Watson (hypoth\u00e8se nulle : non-corr\u00e9lation des r\u00e9sidus)","title":"Analyse des r\u00e9sidus"},{"location":"Chap3_Regression/#methodes-de-base","text":"","title":"M\u00e9thodes de base"},{"location":"Chap3_Regression/#moindres-carres-ordinaire","text":"Les moindres carr\u00e9s ordinaire (MCO) est la m\u00e9thode de r\u00e9gression lin\u00e9aire la plus basique qui soit. S'il s'agit originellement d'une m\u00e9thode de statistiques descriptives , nous verrons que l'on peut s'en servir pour faire de l' inf\u00e9rence statistique .","title":"Moindres carr\u00e9s ordinaire"},{"location":"Chap3_Regression/#principe","text":"Comme nous venons de le mentionner, les MCO a originellement un but descriptif. Si on a un jeu de donn\u00e9es contenant une variable explicative \\(x\\) et une variable de r\u00e9ponse \\(y\\) , on cherche : quelle droite d'\u00e9quation \\(y = a x + b\\) repr\u00e9sente le mieux la distribution des \\(p\\) points \\((x_i,y_i)\\) de cet \u00e9chantillon ? \\(a\\) et \\(b\\) seront alors 2 indicateurs statistiques caract\u00e9risant notre \u00e9chantillon. Mais comment d\u00e9terminer qu'une droite repr\u00e9sente au mieux un nuage de points ? La m\u00e9thode des MCO consid\u00e8re que la droite d'\u00e9quation \\(y = a x + b\\) repr\u00e9sentant le mieux les \\(p\\) point de notre \u00e9chantillon est celle qui minimise : \\(\\sum_{i=1}^{p} (y_i - a x_i - b)^2\\) c'est-\u00e0-dire la SCR du mod\u00e8le. D'o\u00f9 le nom de la m\u00e9thode : on cherche les \"moindres carr\u00e9s\". On peut montrer que les param\u00e8tres \\(a\\) et \\(b\\) minimisant cette fonction sont : \\(a = \\frac{\\sum_{i=1}^{p} (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^{p} (x_i-\\overline{x})^2}\\) \\(b = \\overline{y} - a \\overline{x}\\) On notera pour simplifier les expressions : \\(sc_{xx} = \\sum_{i=1}^{p} (x_i-\\overline{x})^2\\) \\(sc_{yy} = \\sum_{i=1}^{p} (y_i-\\overline{y})^2\\) \\(sc_{xy} = \\sum_{i=1}^{p} (x_i-\\overline{x})(y_i-\\overline{y})\\) D'o\u00f9 \\(a = \\frac{sc_{xy}}{sc_{xx}}\\) Nota Bene La droite d\u00e9termin\u00e9e par les MCO passera toujours par le point \\((\\overline{x},\\overline{y})\\) .","title":"Principe"},{"location":"Chap3_Regression/#meilleur-estimateur-lineaire-non-biaise-blue","text":"Revenons \u00e0 notre probl\u00e8me de r\u00e9gression lin\u00e9aire simple : \u00e0 partir de notre \u00e9chantillon, nous voulons trouver un mod\u00e8le liant nos variables \\(x\\) et \\(y\\) , de la forme \\(y = \\alpha x + \\beta + \\epsilon\\) . Sous certaines conditions sur \\(\\epsilon\\) , nous pouvons appliquer le th\u00e9or\u00e8me de Gauss-Markov \u00e0 notre probl\u00e8me : Th\u00e9or\u00e8me de Gauss-Markov On cherche \u00e0 mod\u00e9liser une relation \\(y = \\alpha x + \\beta + \\epsilon\\) entre 2 variables \\(x\\) et \\(y\\) , \u00e0 partir d'un \u00e9chantillon de r\u00e9alisations \\((x_i,y_i)\\) . Si \\(\\epsilon\\) v\u00e9rifie : - Une moyenne nulle. - Un \u00e9cart-type constant avec \\(x\\) . - Une non-corr\u00e9lation de ses r\u00e9alisations. Alors, les param\u00e8tres \\(a\\) et \\(b\\) de la droite d\u00e9termin\u00e9e par les MCO est le Meilleur Estimateur Lin\u00e9aire Non-biais\u00e9 (\"BLUE\" en anglais) de \\(\\alpha\\) et \\(\\beta\\) . On peut donc se servir de la m\u00e9thode des MCO pour estimer \\(\\alpha\\) et \\(\\beta\\) \u00e0 partir de notre \u00e9chantillon de points \\((x_i,y_i)\\) . Il est m\u00eame possible d'estimer l' \u00e9cart-type de \\(\\epsilon\\) avec l'estimateur suivant : \\(s = \\sqrt{\\frac{\\sum_{i=1}^{p} (y_i-\\hat{y_i})^2}{p-2}} = \\sqrt{\\frac{\\sum_{i=1}^{p} \\epsilon_i^2}{p-2}}\\) Reste alors une probl\u00e9matique : Si j'utilise mon mod\u00e8le pour r\u00e9aliser une pr\u00e9diction \\(\\hat{y_{p+1}}\\) \u00e0 partir d'une nouvelle valeur \\(x_{p+1}\\) , c'est-\u00e0-dire en calculant \\(\\hat{y_{p+1}} = \\alpha x_{p+1} + \\beta\\) , \u00e0 quel point puis-je avoir confiance en ma pr\u00e9diction ?","title":"Meilleur Estimateur Lin\u00e9aire Non-biais\u00e9 (BLUE)"},{"location":"Chap3_Regression/#intervalles-de-confiance-et-de-prediction","text":"Comme pour tout probl\u00e8me d'inf\u00e9rence statistique, lorsque l'on a obtenu notre mod\u00e8le de r\u00e9gression lin\u00e9aire, on se pose alors les questions suivantes : Quelle est mon incertitude sur les \\(\\alpha\\) et \\(\\beta\\) trouv\u00e9s \u00e0 partir de mon \u00e9chantillon ? Pour une valeur de \\(x\\) fix\u00e9e, quelle est mon incertitude sur la moyenne des \\(y\\) avec mon mod\u00e8le de r\u00e9gression lin\u00e9aire ? Pour un nouvelle observation de \\(x\\) , quelle est mon incertitude sur la valeur de \\(y\\) pr\u00e9dite par mon mod\u00e8le de r\u00e9gression lin\u00e9aire ? Pour r\u00e9pondre \u00e0 ces questions, nous allons utiliser des intervalles de confiance . L'hypoth\u00e8se de normalit\u00e9 de \\(\\epsilon\\) implique que les estimations de \\(\\alpha\\) et de \\(\\beta\\) \u00e0 partir d'un \u00e9chantillon suivent une loi normale . Mais nous ne pouvons qu'estimer son \u00e9cart-type, puisque nous ne disposons que d'un \u00e9chantillon. Il nous faut donc utiliter la loi de Student , et plus particuli\u00e8rement le \"t de Student\". Rappels sur le t de Student Soit une population de moyenne \\(\\mu\\) et d'\u00e9cart-type inconnu, dont on r\u00e9cup\u00e8re un \u00e9chantillon de \\(p\\) points, de moyenne estim\u00e9e \\(\\overline{x}\\) et d'\u00e9cart-type estim\u00e9 \\(s\\) . Alors la variable al\u00e9atoire \\(t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{p}}\\) suit une loi de Student, dont on peut se servir pour \u00e9tablir un intervalle de confiance sur l'estimation \\(\\overline{x}\\) de \\(\\mu\\) . On note \\(t_{\\gamma}^{k}\\) le quantile de seuil d'erreur \\(\\gamma\\) de la loi de Student \u00e0 \\(k\\) degr\u00e9s de libert\u00e9 . Le seuil de confiance est alors \\(1-\\gamma\\) : pour seuil de confiance \u00e0 99% on prendra \\(\\gamma = 0.01\\) . La loi normale \u00e9tant sym\u00e9trique, pour d\u00e9terminer un intervalle de confiance de seuil \\(1-\\gamma\\) , il faut en r\u00e9alit\u00e9 utiliser \\(t_{\\gamma/2}^{k}\\) . Donc pour un intervalle de confiance \u00e0 99% on prendra \\(\\gamma = 0.005\\) . On a alors : \\(p(\\overline{x} - t_{\\gamma/2}^{p-1} \\frac{s}{\\sqrt{p}} \\leq \\mu \\leq \\overline{x} + t_{\\gamma/2}^{p-1} \\frac{s}{\\sqrt{p}}) = 1 - \\gamma\\) avec \\(k = p-1\\) car on a utilis\u00e9 1 degr\u00e9 de libert\u00e9 pour estimer \\(\\mu\\) . Nota Bene : Il est \u00e0 noter que plus \\(p\\) est grand (et donc plus \\(k\\) est grand) et plus le \\(t\\) se rapproche d'une loi normale. Dans notre cas, nous avons utilis\u00e9 2 degr\u00e9s de libert\u00e9 pour estimer \\(\\alpha\\) et \\(\\beta\\) , nous utiliserons donc le t de Student pour \\(p-2\\) degr\u00e9s de libert\u00e9. On peut donc \u00e9tablir les intervalles de confiance \u00e0 \\(1-\\gamma\\) suivants sur \\(a\\) et \\(b\\) : \\(\\alpha \\in [a - t_{\\gamma/2}^{p-2} s(a) ; a + t_{\\gamma/2}^{p-2} s(a)]\\) \\(\\beta \\in [b - t_{\\gamma/2}^{p-2} s(b) ; b + t_{\\gamma/2}^{p-2} s(b)]\\) avec les \u00e9cart-types estim\u00e9s : \\(s(a) = \\frac{s}{\\sqrt{sc_{xx}}}\\) \\(s(b) = s \\sqrt{\\frac{1}{p} + \\frac{\\overline{x}^2}{sc_{xx}}}\\) Nota Bene Il est \u00e0 noter que si tous les \\(x_i\\) de l'\u00e9chantillon sont \u00e9gaux, alors \\(x_i = \\overline{x}\\) , d'o\u00f9 \\(sc_{xx} = 0\\) et donc les intervalles de confiance deviennent infinis. Ce r\u00e9sultat est attendu, puisqu'on ne peut pas tirer d'information sur la relation entre \\(x\\) et \\(y\\) avec des points pour un seul \\(x_i\\) . De la m\u00eame mani\u00e8re, on peut estimer pour une valeur de \\(x\\) donn\u00e9e \\(x=u\\) l' intervalle de confiance \u00e0 \\(1-\\gamma\\) sur la moyenne des \\(y\\) sachant \\(x=u\\) : \\(\\alpha u + \\beta \\in [a u + b - t_{\\gamma/2}^{p-2} s(\\hat{y}(u)) ; a u + b + t_{\\gamma/2}^{p-2} s(\\hat{y}(u))]\\) avec \\(s(\\hat{y}(u)) = s \\sqrt{\\frac{1}{p} + \\frac{(u-\\overline{x})^2}{sc_{xx}}}\\) Enfin, on peut estimer l' intervalle de pr\u00e9diction sur \\(y_{p+1}\\) pour une nouvelle donn\u00e9e \\(x_{p+1}\\) : \\(y_{p+1} \\in [a x_{p+1} + b - t_{\\gamma/2}^{p-2} s(y_{p+1}) ; a x_{p+1} + b + t_{\\gamma/2}^{p-2} s(y_{p+1})]\\) avec \\(s(y_{p+1}) = s \\sqrt{1 + \\frac{1}{p} + \\frac{(x_{p+1}-\\overline{x})^2}{sc_{xx}}}\\) En g\u00e9n\u00e9ral, lorsque l'on affiche par-dessus le nuage de points la droite du mod\u00e8le obtenu par MCO, on affiche aussi l'intervalle de confiance sur la moyenne des \\(y\\) , et l'intervalle de pr\u00e9diction choisis. Le graphique obtenu est de la forme suivante : Nota Bene Il est \u00e0 noter que : - Les intervalles de confiance sur la moyenne des \\(y\\) sont toujours plus petits que les intervalles de pr\u00e9vision. - La droite obtenue par MCO passe toujours par \\((\\overline{x},\\overline{y})\\) , donc plus on s'\u00e9loigne de ce point, plus les intervalles de confiance et de pr\u00e9diction vont augmenter.","title":"Intervalles de confiance et de pr\u00e9diction"},{"location":"Chap3_Regression/#implementation-scipy","text":"Afin de r\u00e9aliser une r\u00e9gression lin\u00e9aire simple avec la m\u00e9thode des MCO, on peut utiliser la biblioth\u00e8que de calculs scientifiques Scipy, et en particulier son module de statistiques \"scipy.stat\". Il suffit d'importer l'objet \"linregress\" avec : from scipy.stats import linregress Pour ajuster un mod\u00e8le de r\u00e9gression lin\u00e9aire mco \u00e0 une variable d'entr\u00e9e x et une variable de sortie y on utilise la commande : mco = linregress(x,y) On peut alors r\u00e9cup\u00e9rer le coefficient directeur a et l'ordonn\u00e9e \u00e0 l'origine b de ce mod\u00e8le lin\u00e9aire avec : a = mco.slope b = mco.intercept Il suffit alors d'utiliser ces 2 param\u00e8tres pour r\u00e9aliser une pr\u00e9diction. Pour d\u00e9terminer les intervalles de confiance et de pr\u00e9diction, la biblioth\u00e8que Scipy propose aussi une impl\u00e9mentation de la loi de Student, que l'on peut importer avec : from scipy.stats import t Pour obtenir le quantile tq de seuil s correspondant \u00e0 \\(1-\\gamma\\) , de la loi de Student de \u00e0 k degr\u00e9s de libert\u00e9s, on alors simple utiliser la m\u00e9thode : tq = t.ppf(s,k) Il ne reste alors qu'\u00e0 impl\u00e9menter les formules des estimateurs d'\u00e9cart-types que nous avons vues pr\u00e9c\u00e9demment pour calculer les intervalles de confiances et de pr\u00e9diction. Il est \u00e9galement possible d'obtenir le \\(R^2\\) de la r\u00e9gression gr\u00e2ce au param\u00e8tre r_value du mod\u00e8le : r_2 = mco.rvalue**2 (On reconnait que r_value correspond au coefficient de corr\u00e9lation tel que vu au Chapitre 1). Si cette impl\u00e9mentation est pratique pour faire de l'inf\u00e9rence statistique, elle ne g\u00e8re malheureusement que la r\u00e9gression lin\u00e9aire simple. Pour de la r\u00e9gression lin\u00e9aire multiple, nous verrons que l'on peut utiliser Scikit-Learn.","title":"Impl\u00e9mentation Scipy"},{"location":"Chap3_Regression/#generalisation-a-la-regression-lineaire-multiple","text":"Les MCO peut \u00eatre g\u00e9n\u00e9ralis\u00e9e pour les probl\u00e8mes \u00e0 plus d'une variable explicative (nombre de variables explicatives \\(n>1\\) ). (Comme mentionn\u00e9 pr\u00e9c\u00e9demment, un probl\u00e8me de r\u00e9gression polynomiale peut \u00e9galement \u00eatre r\u00e9solu en utilisant de la r\u00e9gression lin\u00e9aire multiple). Rappelons que le mod\u00e8le de r\u00e9gression lin\u00e9aire multiple \u00e0 ajuster est le suivant : \\(y = \\alpha_1 x_1 + \\alpha_2 x_2 + ... + \\alpha_n x_n + \\beta + \\epsilon\\) Si nous disposons de \\(p\\) observations dans notre jeu de donn\u00e9es d'entrainement, il faut que nos param\u00e8tres \\(\\alpha_1\\) , ..., \\(\\alpha_n\\) et \\(\\beta\\) v\u00e9rifient : \\(\\begin{cases} y_1 = \\alpha_1 x_{1,1} + \\alpha_2 x_{1,2} + ... + \\alpha_n x_{1,n} + \\beta + \\epsilon_1\\\\ y_2 = \\alpha_1 x_{2,1} + \\alpha_2 x_{2,2} + ... + \\alpha_n x_{2,n} + \\beta + \\epsilon_2\\\\ ...\\\\ y_p = \\alpha_1 x_{p,1} + \\alpha_2 x_{p,2} + ... + \\alpha_n x_{p,n} + \\beta + \\epsilon_p\\\\ \\end{cases}\\) Un syst\u00e8me d'\u00e9quations lin\u00e9aires que l'on peut mettre sous la forme matricielle suivante : \\(Y = X A + E\\) avec \\(Y = \\begin{pmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_p \\end{pmatrix}\\) \\(X = \\begin{pmatrix} 1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\ 1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{p,1} & x_{p,2} &\\cdots & x_{p,n} \\end{pmatrix}\\) \\(A = \\begin{pmatrix} \\beta\\\\ \\alpha_1\\\\ \\alpha_2\\\\ \\vdots\\\\ \\alpha_n \\end{pmatrix}\\) \\(E = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_p \\end{pmatrix}\\) Avec les m\u00eames hypoth\u00e8ses sur \\(\\epsilon\\) que pour la r\u00e9gression lin\u00e9aire simple, on peut appliquer les MCO pour trouver le meilleur estimateur lin\u00e9aire non-biais\u00e9 de \\(A\\) . Cette fois-ci, il s'agit de la matrice \\(\\hat{A}\\) minimisant l' erreur quadratique moyenne (ou \"MSE\" en anglais) : \\(MSE = \\frac{1}{p} \\sum_{i=1}^{p} (y_i - \\sum_{j=1}^{n} \\alpha_j x_{i,j} - \\beta)^2\\) On peut montrer que ce minimum est obtenu pour \\(\\hat{A}\\) v\u00e9rifiant l' \u00e9quation normale suivante : \\(\\hat{A} = (X^T X)^{-1} X^T Y\\) En pratique, il est rare que l'on r\u00e9solve directement l'\u00e9quation normale : (1) sa r\u00e9solution est complexe, (2) la matrice \\(X^T X\\) peut ne pas \u00eatre inversible (si \\(p<n\\) ou si certaines equations sont redondantes). C'est pourquoi la plupart des impl\u00e9mentations des MCO pour de la r\u00e9gression lin\u00e9aire multiple calculent plut\u00f4t : \\(\\hat{A} = X^{+} Y\\) avec \\(X^{+}\\) le pseudo-inverse de \\(X\\) . Celui-ci est calcul\u00e9 en utilisant la d\u00e9composition en valeurs singuli\u00e8res (SVD) de \\(X\\) . Cette m\u00e9thode \u00e0 l'avantage d'\u00eatre plus rapide que de r\u00e9soudre l'\u00e9quation normale directement, et que le pseudo-inverse de \\(X\\) existe toujours. On peut \u00e9galement g\u00e9n\u00e9raliser les formules de d\u00e9termination des intervalles de confiance et de pr\u00e9diction vues pr\u00e9c\u00e9demment. Tout d'abord, dans le cas multiple l'estimateur de l'\u00e9cart-type de \\(\\epsilon\\) devient : \\(s = \\sqrt{\\frac{\\sum_{i=1}^{p} (y_i-\\hat{y_i})^2}{p-n-1}} = \\sqrt{\\frac{\\sum_{i=1}^{p} \\epsilon_i^2}{p-n-1}}\\) Soit une r\u00e9alisation donn\u00e9e des variables d'entr\u00e9e : \\(\\begin{pmatrix} x_0 & x_1 & \\cdots & x_n \\end{pmatrix} = \\begin{pmatrix} u_0 & u_1 & \\cdots & u_n \\end{pmatrix} = U\\) L'intervalle de confiance \u00e0 \\(1-\\gamma\\) sur la moyenne des \\(y\\) sachant que les variables d'entr\u00e9e sont \u00e0 \\(U\\) est alors : \\(U A \\in [U \\hat{A} - t_{\\gamma/2}^{p-n-1} s(\\hat{y}(U)) ; U \\hat{A} + t_{\\gamma/2}^{p-n-1} s(\\hat{y}(U))]\\) avec \\(s(\\hat{y}(U)) = s \\sqrt{U(X^T X)^{-1}U^T}\\) Soit une nouvelle r\u00e9alisation des variables d'entr\u00e9es : \\(\\begin{pmatrix} x_{p+1,0} & x_{p+1,1} & \\cdots & x_{p+1,n} \\end{pmatrix} = V\\) L'intervalle de pr\u00e9diction \u00e0 \\(1-\\gamma\\) de \\(V\\) est : \\(y_{p+1} \\in [V \\hat{A} - t_{\\gamma/2}^{p-n-1} s(y_{p+1}) ; V \\hat{A} + t_{\\gamma/2}^{p-n-1} s(y_{p+1})]\\) avec \\(s(y_{p+1}) = s \\sqrt{1+V(X^T X)^{-1}V^T}\\)","title":"G\u00e9n\u00e9ralisation \u00e0 la r\u00e9gression lin\u00e9aire multiple"},{"location":"Chap3_Regression/#implementation-scikit-learn","text":"Il existe une impl\u00e9mentation Scikit-Learn des MCO, qui permet la r\u00e9gression lin\u00e9aire multiple (et donc la r\u00e9gression polynomiale). Elle peut \u00eatre import\u00e9e avec : from sklearn.linear_model import LinearRegression On peut ensuite initialiser un mod\u00e8le de r\u00e9gression lin\u00e9aire mco avec un objet \"LinearRegression\" : mco = LinearRegression() Pour donner le jeu d'entrainement (matrice des variables d'entr\u00e9e X et vecteur de la variable de sortie y ) \u00e0 ce mod\u00e8le, on utilise la m\u00e9thode : mco.fit(X,y) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions y_pred \u00e0 partir d'une matrice X_pred : y_pred = mco.predict(X_pred) Pour obtenir le \\(R^2\\) de notre mod\u00e8le sur ses donn\u00e9es d'entrainement X et y , il suffit d'utiliser la m\u00e9thode suivante : r_2 = mco.score(X,y) On peut de m\u00eame le calculer sur des donn\u00e9es de test. Malheureusement, contrairement \u00e0 Scipy, Scikit-Learn ne permet pas de faire l'inf\u00e9rence statistique avec les MCO : il n'y a pas de fonctionnalit\u00e9 pour d\u00e9terminer des intervalles de confiance ou de pr\u00e9diction. On doit donc calculer nous m\u00eame ces intervalles, \u00e0 partir de la loi de Student impl\u00e9ment\u00e9e par Scipy, et des formules g\u00e9n\u00e9ralis\u00e9es.","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap3_Regression/#application-a-notre-exemple","text":"Nous allons \u00e0 pr\u00e9sent appliquer la r\u00e9gression lin\u00e9aire avec les Moindres Carr\u00e9s Ordinaires \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, puis nous s\u00e9lectionnons les variables d'entr\u00e9e et de sortie sous la forme de matrices Numpy : df_dataset = pd.read_csv(input_path) df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) x_train = df_train['sunspots'].to_numpy() y_train = df_train['tsi'].to_numpy() x_test = df_test['sunspots'].to_numpy() y_test = df_test['tsi'].to_numpy() Si les hypoth\u00e8ses associ\u00e9es sont bien respect\u00e9es, les MCO permettent en th\u00e9orie d'obtenir des intervalles de confiance et de pr\u00e9diction fiables. Dans la r\u00e9alit\u00e9, il est compliqu\u00e9 d'avoir ces hypoth\u00e8ses exactement vraies. D'o\u00f9 le fait que l'on applique ici une strat\u00e9gie de s\u00e9paration entre ensemble d'entrainement et de test (80% / 20%), afin de v\u00e9rifier les performances en g\u00e9n\u00e9ralisation . On d\u00e9termine notre mod\u00e8le de r\u00e9gression lin\u00e9aire mco par les MCO en utilisant la biblioth\u00e8que Scipy : from scipy.stats import linregress mco = linregress(x_train,y_train) Nous pouvons \u00e0 pr\u00e9sent estimer des TSI \u00e0 partir de nombres de t\u00e2ches solaires. Voici par exemple pour 321 nombres de t\u00e2ches solaires entre 0 et 320 : x_mco = np.linspace(0,320,321) y_mco = mco.intercept+mco.slope*x_mco Nous avons ainsi les pr\u00e9dictions de notre mod\u00e8le pour chaque nombre entiers de t\u00e2ches solaires sur l'intervalle possible. Pour obtenir les intervalles de confiance et de pr\u00e9diction de notre mod\u00e8le, il nous faut d'abord estimer l'\u00e9cart-type des r\u00e9sidus : res_train = y_train-(mco.intercept+mco.slope*x_train) s = np.sqrt(np.sum(res_train**2)/(len(x_train)-2)) Ensuite, il nous faut calculer \\(sc_{xx}\\) : x_mean_train = np.mean(x_train) sc_xx_train = np.sum((x_train-x_mean_train)**2) Nous pouvons alors en d\u00e9duire l'estimation de l'\u00e9cart-type sur la moyenne conditionnelle de \\(y\\) , et l'estimation de l'\u00e9cart-type sur les pr\u00e9dictions : s_y_conf = s*np.sqrt((1/len(x_train))+(((x_mco-x_mean_train)**2)/sc_xx_train)) s_y_pred = s*np.sqrt(1+(1/len(x_train))+(((x_mco-x_mean_train)**2)/sc_xx_train)) Nous avons \u00e0 pr\u00e9sent tous les \u00e9lements pour calculer les intervalles de confiance et de pr\u00e9diction \u00e0 95% pour chaque nombre de t\u00e2ches solaires entre 0 et 320. Il suffit d'utiliser le t de Student adapt\u00e9 : from scipy.stats import t t_student = t.ppf(1-(0.05/2),len(x_train)-2) y_conf_inf = y_mco - (t_student*s_y_conf) y_conf_sup = y_mco + (t_student*s_y_conf) y_pred_inf = y_mco - (t_student*s_y_pred) y_pred_sup = y_mco + (t_student*s_y_pred) On peut tracer le mod\u00e8le lin\u00e9aire obtenu par les MCO par-dessus le nuage de points des donn\u00e9es d'entrainement, avec les intervalles de confiance et de pr\u00e9diction : Pour \u00e9valuer les performances du mod\u00e8le en entrainement, on peut simplement calculer le \\(R^2\\) sur les donn\u00e9es d'entrainement : print(mco.rvalue**2) On obtient \\(R^2 \\approx 0.792\\) , ce qui veut dire que 79.2% des \u00e9carts sont expliqu\u00e9s par le mod\u00e8le. Un score relativement bon, mais pas excellent puisque plus de 1/5 des \u00e9carts restent inexpliqu\u00e9s par le mod\u00e8le. Pour \u00e9valuer les performances du mod\u00e8le en test, on a pas d'autre choix que de calculer le \\(R^2\\) sur les donn\u00e9es de test \"\u00e0 la main\" : y_mean_test = np.mean(y_test) sct_test = np.sum((y_test-y_mean_test)**2) scr_test = np.sum((y_test-(mco.intercept+mco.slope*x_test))**2) r2_test = 1-(scr_test/sct_test) print(r2_test) On obtient alors \\(R^2 \\approx 0.795\\) soit un score tr\u00e8s similaire aux performances en entrainement. Des performances similaires peuvent donc \u00eatre attendues en g\u00e9n\u00e9ralisation. Pour valider ou invalider notre mod\u00e8le, et comprendre d'o\u00f9 viennent ses limites en terme de performances, affichons les r\u00e9sidus en fonction de la variable d'entr\u00e9e : On observe qu'il y a clairement une tendance des r\u00e9sidus en fonction de la variable d'entr\u00e9e. Les hypoth\u00e8ses sur les r\u00e9sidus ne sont donc pas respect\u00e9es, ce qui peut expliquer les performances modestes du mod\u00e8le. Pour d'obtenir un mod\u00e8le plus performant, nous proposons donc d'essayer une r\u00e9gression polynomiale de degr\u00e9 2. Cette r\u00e9gression polynomiale de degr\u00e9 2 va revenir \u00e0 faire une r\u00e9gression lin\u00e9aire multiple, avec pour variables d'entr\u00e9e le nombre de t\u00e2ches solaires et son carr\u00e9. Cette fois-ci, on va donc rassembler les variables d'entr\u00e9e au sein d'une matrice : x_train = np.ones((len(df_train),3)) x_train[:,1] = df_train['sunspots'].to_numpy() x_train[:,2] = x_train[:,1]**2 x_test = np.ones((len(df_test),3)) x_test[:,1] = df_test['sunspots'].to_numpy() x_test[:,2] = x_test[:,1]**2 La 1\u00e8re colonne correspond \u00e0 l'ordonn\u00e9e \u00e0 l'origine et ne contient que des 1, la 2nde colonne correspond au nombre de t\u00e2ches solaires, la 3\u00e8me au carr\u00e9 du nombre de t\u00e2ches solaires. La variable de sortie reste inchang\u00e9e par rapport \u00e0 la r\u00e9gression lin\u00e9aire simple. On d\u00e9termine notre mod\u00e8le de r\u00e9gression lin\u00e9aire multiple mco par les MCO en utilisant la biblioth\u00e8que Scikit-Learn : from sklearn.linear_model import LinearRegression mco = LinearRegression() mco.fit(x_train[:,1:],y_train) Attention , la m\u00e9thode fit des objets LinearRegression Scikit-Learn ne prend pas en entr\u00e9e la 1\u00e8re colonne de la matrice des variables d'entr\u00e9e (correspondant \u00e0 l'ordonn\u00e9e \u00e0 l'origine) ! Cependant, nous aurons bien besoin de la matrice compl\u00e8te pour calculer les intervalles de confiance et de pr\u00e9diction. Voici comment estimer les TSI correspondant \u00e0 321 de nombres de t\u00e2ches solaires entre 0 et 320 : x_mco = np.ones((100,3)) x_mco[:,1] = np.linspace(0,320,100) x_mco[:,2] = np.linspace(0,320,100)**2 y_mco = mco.predict(x_mco[:,1:]) Comme pour fit , la m\u00e9thode predict ne prend pas en entr\u00e9e la 1\u00e8re colonne de la matrice des variables d'entr\u00e9e. On calcule l'estimation de l'\u00e9cart-type des r\u00e9sidus, en faisant attention au fait qu'il y a maintenant une variable d'entr\u00e9e de plus : res_train = y_train-mco.predict(x_train[:,1:]) s = np.sqrt(np.sum(res_train**2)/(len(x_train)-3)) On peut alors estimer les \u00e9cart-types sur la moyenne conditionnelle de \\(y\\) , et sur les pr\u00e9dictions. Ce calcul se fait en s\u00e9lectionnant chaque ligne de la matrice x_mco cr\u00e9e pr\u00e9c\u00e9demment, de mani\u00e8re it\u00e9rative : s_y_conf = np.zeros(len(x_mco)) s_y_pred = np.zeros(len(x_mco)) for idx in range(len(x_mco)): x_mco_idx = x_mco[idx,:] s_y_conf[idx] = s*np.sqrt(x_mco_idx@np.linalg.inv(x_train.T@x_train)@x_mco_idx.T) s_y_pred[idx] = s*np.sqrt(1+x_mco_idx@np.linalg.inv(x_train.T@x_train)@x_mco_idx.T) On peut \u00e0 pr\u00e9sent d\u00e9terminer les intervalles de confiance et de pr\u00e9diction de notre mod\u00e8le lin\u00e9aire multiple : from scipy.stats import t t_student = t.ppf(1-(0.05/2),len(x_train)-3) conf_inf = y_mco - (t_student*s_y_conf) conf_sup = y_mco + (t_student*s_y_conf) pred_inf = y_mco - (t_student*s_y_pred) pred_sup = y_mco + (t_student*s_y_pred) Il est maintenant possible d'afficher par dessus le nuage de points des donn\u00e9es d'entrainement le mod\u00e8le polynomial, ainsi que ses intervalles de confiance et de pr\u00e9diction : Pour \u00e9valuer les performances du mod\u00e8le en entrainement et en test, on calcule le \\(R^2\\) : print(mco.score(x_train[:,1:],y_train)) On obtient \\(R^2 \\approx 0.896\\) sur les donn\u00e9es d'entrainement, ce qui veut dire que 89.6% des \u00e9carts sont expliqu\u00e9s par le mod\u00e8le. Un tr\u00e8s bon score ! Maintenant seul 1/10 des \u00e9carts restent inexpliqu\u00e9s par le mod\u00e8le. En test, on obtient un score tr\u00e8s similaire de \\(R^2 \\approx 0.904\\) , ce qui laisse pr\u00e9sager ce niveau de performance en g\u00e9n\u00e9ralisation. Pour valider ou invalider notre mod\u00e8le, v\u00e9rifions que les r\u00e9sidus en fonction de la variable d'entr\u00e9e se comportent conform\u00e9ment \u00e0 nos hypoth\u00e8ses : Cette fois-ci, on n'observe aucune tendance claire se d\u00e9gager des r\u00e9sidus en fonction de la variable d'entr\u00e9e : ce qui est attendu. Par contre, on peut noter que l'\u00e9cart-type des r\u00e9sidus a l'air de varier l\u00e9g\u00e8rement avec le nombre de t\u00e2ches solaires, et l'hypoth\u00e8se de normalit\u00e9 est assez difficile \u00e0 confirmer. C'est pourquoi il faudrait en toute rigueur appliquer des tests statistiques de nos hypoth\u00e8ses.","title":"Application \u00e0 notre exemple"},{"location":"Chap3_Regression/#remarques","text":"La m\u00e9thode des Moindres Carr\u00e9s Ordinaire a les avantages suivants : Elle est relativement simple \u00e0 mettre en place, avec peu de param\u00e8tres , et aucun hyperparam\u00e8tre . Les pr\u00e9dictions qu'elle r\u00e9alise sont compl\u00e8tement expliqu\u00e9es et interpr\u00e9tables : un humain peut les comprendre. On peut \u00e9tablir des intervalles de confiance sur les pr\u00e9dictions. Une fois le mod\u00e8le entrain\u00e9, le temps de calcul des pr\u00e9dictions est rapide (lin\u00e9aire par rapport au nombre de pr\u00e9dictions). Mais cette m\u00e9thode a aussi les limites suivantes : Le temps de calcul de la SVD augmente quadratiquement avec le nombre de variables explicatives ! Elle demande beaucoup de m\u00e9moire pour manipuler la matrice \\(X\\) . Ces 2 d\u00e9savantages sont les raisons pour lesquelles on utilise tr\u00e8s peu les MCO dans les cas o\u00f9 le nombre de variables explicatives est grand . M\u00eame s'il est possible d'utiliser les MCO pour de la r\u00e9gression non-lin\u00e9aire, avec l'astuce de la r\u00e9gression polynomiale, plus l'ordre du polyn\u00f4me est grand et plus le nombre de variables explicatives est grand. On utilise donc rarement les MCO pour des mod\u00e8les non-lin\u00e9aires complexes .","title":"Remarques"},{"location":"Chap3_Regression/#perceptron-multicouche","text":"","title":"Perceptron multicouche"},{"location":"Chap3_Regression/#perceptron-multicouche-pour-la-regression","text":"Nous avons vu lors du chapitre pr\u00e9c\u00e9dent que le perceptron multicouche (PMC) est un r\u00e9seau de neurones initialement invent\u00e9 pour r\u00e9soudre des probl\u00e8mes de classification binaire, qui peut \u00eatre adapt\u00e9 pour r\u00e9soudre n'importe quel probl\u00e8me de classification. Et bien le PMC peut \u00e9galement \u00eatre utilis\u00e9 pour r\u00e9soudre des probl\u00e8mes de r\u00e9gression non-lin\u00e9aire , \u00e0 condition de choisir des hyperparam\u00e8tres adapt\u00e9s. Si on applique le th\u00e9or\u00e8me de l'approximation universelle \u00e0 la r\u00e9gression, il en ressort que le PMC est capable de tracer n'importe quelle relation , \u00e0 condition d'avoir assez de neurones en couche cach\u00e9e. D'o\u00f9 l'int\u00e9r\u00eat de ce type de mod\u00e8le. Pour adapter le PMC \u00e0 la r\u00e9gression, il faut tout d'abord adapter l' architecture : Le nombre d'entr\u00e9es correspond au nombre de variables d'entr\u00e9e \\(n\\) du probl\u00e8me. Le nombre de sorties correspond au nombre de variables de sortie du probl\u00e8me (1 dans notre cas, mais on peut imaginer un mod\u00e8le multi-sorties). Ensuite, il faut choisir une fonction de co\u00fbt pertinente pour l'entrainement. La fonction la plus couramment utilis\u00e9e est l' erreur quadratique moyenne (not\u00e9e \"MSE\" en anglais) : \\(MSE = \\frac{1}{p} \\sum_{i=1}^{p} (y_i-\\hat{y_i})^2\\) On reconnait une g\u00e9n\u00e9ralisation de la formule des \"moindres carr\u00e9s\" utilis\u00e9e pr\u00e9c\u00e9demment. D'autres fonctions donnant plus ou moins de poids aux outliers existent. Autre particularit\u00e9 du PMC pour la r\u00e9gression : nous ne voulons pas en sortie un seuil entre 0 et 1 (ou -1 et 1), mais une valeur continue correspondant \u00e0 la variable de sortie pr\u00e9dite. Il faut donc que le neurone de sortie soit sans fonction de seuil . Et voil\u00e0, nous avons un PMC adapt\u00e9 \u00e0 la r\u00e9gression ! De la m\u00eame mani\u00e8re que pour la classification, pour un nombre de neurones par couche cach\u00e9e donn\u00e9, plus on aura de couches et plus complexes les fronti\u00e8res de d\u00e9cisions pourront \u00eatre. Et d\u00e8s que l'on a plus d'une couche cach\u00e9e, on parle d' apprentissage profond (\"Deep Learning\"). Nota Bene Pour que l'apprentissage d'un PMC pour de la r\u00e9gression se d\u00e9roule correctement, il est recommand\u00e9 d'effectuer une transformation des donn\u00e9es d'entr\u00e9e et de sortie (voir Chapitre 1).","title":"Perceptron multicouche pour la r\u00e9gression"},{"location":"Chap3_Regression/#implementation-scikit-learn_1","text":"Il existe une impl\u00e9mentation Scikit-Learn du PMC pour la r\u00e9gression. Elle peut \u00eatre import\u00e9e avec : from sklearn.neural_network import MLPRegressor On peut ensuite initialiser un PMC pour de la r\u00e9gression mlp avec les hyperparam\u00e8tres par d\u00e9faut en utilisant la commande : mlp = MLPRegressor() Nous verrons plus loin quels sont ces hyperparam\u00e8tres. Pour donner le jeu d'entrainement (variables d'entr\u00e9e avec x_train et variables de sortie avec y_train ) \u00e0 ce mod\u00e8le, on utilise la m\u00e9thode : mlp.fit(x_train,y_train) On peut \u00e0 pr\u00e9sent r\u00e9aliser des pr\u00e9dictions y_test \u00e0 partir de variables d'entr\u00e9e de test x_test : y_test = mlp.predict(x_test) Si on veut effectuer un test de notre mod\u00e8le de r\u00e9gression sur un jeu de donn\u00e9es, on peut obtenir un \\(R^2\\) avec la commande : mlp.score(x_test,y_test) Voici les hyperparam\u00e8tres par d\u00e9faut de l'impl\u00e9mentation Scikit-Learn du PMC : Nombre de couches cach\u00e9es : 1 Nombre de neurones par couche cach\u00e9e : 100 La fen\u00eatre d'activation pour les couches cach\u00e9es : ReLU La fen\u00eatre d'activation pour la couche de sortie : Aucune. Le taux d'apprentissage pour la descente de gradient : 0.001 Le nombre maximum d'\u00e9poques d'apprentissage : 200 (par d\u00e9faut sans arr\u00eat pr\u00e9matur\u00e9, mais on peut l'activer) La fonction de co\u00fbt : erreur quadratique moyenne Ces hyperparam\u00e8tres sont presque tous modifiables par l'utilisateur.","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap3_Regression/#application-a-notre-exemple_1","text":"Nous allons \u00e0 pr\u00e9sent entrainer un PMC \u00e0 r\u00e9soudre notre probl\u00e8me de r\u00e9gression. Comme pr\u00e9c\u00e9demment, nous importons le fichier CSV depuis son chemin input_path sous la forme d'un DataFrame, puis nous s\u00e9lectionnons les variables d'entr\u00e9e et de sortie sous la forme de matrices Numpy : df_dataset = pd.read_csv(input_path) df_train=df_dataset.sample(frac=0.8,random_state=0) df_test=df_dataset.drop(df_train.index) x_train = df_train[['sunspots']].to_numpy().reshape(-1, 1) y_train = df_train[['tsi']].to_numpy().reshape(-1, 1) x_test = df_test[['sunspots']].to_numpy().reshape(-1, 1) y_test = df_test[['tsi']].to_numpy().reshape(-1, 1) Afin d'aider le PMC \u00e0 converger, nous allons effectuer une transformation centrage-r\u00e9duction des entr\u00e9es et des sorties (voir Chapitre 1). C'est pourquoi nous avons fait attention \u00e0 ce que les dimensions des variables d'entr\u00e9e et de sortie soient celles attendues par StandardScaler . Attention ! Il faut calibrer la transformation sur les donn\u00e9es d'entrainement, puis l'appliquer aux jeux d'entrainement et de test ! from sklearn.preprocessing import StandardScaler x_scaler = StandardScaler() x_scaler.fit(x_train) x_train = x_scaler.transform(x_train) x_test = x_scaler.transform(x_test) y_scaler = StandardScaler() y_scaler.fit(y_train) y_train = y_scaler.transform(y_train) y_test = y_scaler.transform(y_test) Maintenant que les donn\u00e9es sont pr\u00eates, nous pouvons cr\u00e9er notre mod\u00e8le de r\u00e9gression. Voici comment initialiser un PMC pour de la r\u00e9gression, avec les param\u00e8tres par d\u00e9faut : from sklearn.neural_network import MLPRegressor mlp = MLPRegressor() Pour l'entrainer, il nous suffit ensuite d'utiliser la commande suivante : mlp.fit(x_train,y_train.ravel()) On a fait attention ici \u00e0 ce que les dimensions de la sortie soient celles attendues par MLPRegressor . Et pour obtenir le \\(R^2\\) de notre mod\u00e8le en entrainement et en test : print(mlp.score(x_train,y_train)) print(mlp.score(x_test,y_test)) Pour r\u00e9aliser des pr\u00e9dictions y_predict \u00e0 partir d'entr\u00e9es x_predict , il ne faudra pas oublier d'effectuer une transformation centrage-r\u00e9duction inverse des sorties : x_scaler.fit(x_predict) y_predict = mlp.predict(x_predict) y_scaler.inverse_transform(y_predict) Mais comme nous l'avons fait remarquer dans le chapitre pr\u00e9c\u00e9dent, le PMC est sensible au sur-apprentissage. Pour cette raison, on peut vouloir appliquer la m\u00e9thode de r\u00e9gularisation par \"arr\u00eat pr\u00e9matur\u00e9\" (voir Chapitre 1). Si on active le param\u00e8tre early_stopping du classifieur PMC de Scikit-Learn, au moment de l'apprentissage il va automatiquement mettre de c\u00f4t\u00e9 une partie du jeu d'entrainement pour faire un jeu de validation. On peut m\u00eame choisir la fraction du jeu d'entrainement \u00e0 utiliser pour la validation, avec le param\u00e8tre validation_fraction . Voici un exemple de d\u00e9finition d'un classifieur, avec de l'arr\u00eat pr\u00e9matur\u00e9 et 20% des donn\u00e9es d'entrainement utilis\u00e9es pour la validation : mlp = MLPRegressor(early_stopping=True,validation_fraction=0.2) Il y a aussi une astuce pour afficher l'\u00e9valuation de la fonction de co\u00fbt au cours des \u00e9poques, pour l'ensemble d'entrainement et de validation. Elle se base sur : Subdiviser le jeu d'entrainement en un nouveau jeu d'entrainement et un jeu de validation. Utiliser la m\u00e9thode partial_fit de notre PMC, qui permet de r\u00e9aliser une it\u00e9ration \u00e0 la fois. R\u00e9cup\u00e9rer la valeur de la fonction de co\u00fbt sur le jeu d'entrainement, avec l'attribut loss_ de notre classifieur. Evaluer la valeur de la fonction de co\u00fbt sur le jeu de validation, en utilisant la fonction mean_squared_error de Scikit-Learn. Voici l'affichage des 2 courbes Matplotlib obtenues sur notre base de donn\u00e9es, pour 50 \u00e9poques : import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error df_train2=df_train.sample(frac=0.8,random_state=0) df_validation=df_dataset.drop(df_train2.index) x_train = df_train2[['sunspots']].to_numpy().reshape(-1, 1) y_train = df_train2[['tsi']].to_numpy().reshape(-1, 1) x_validation = df_validation[['sunspots']].to_numpy().reshape(-1, 1) y_validation = df_validation[['tsi']].to_numpy().reshape(-1, 1) x_scaler = StandardScaler() x_scaler.fit(x_train) x_train = x_scaler.transform(x_train) x_validation = x_scaler.transform(x_validation) y_scaler = StandardScaler() y_scaler.fit(y_train) y_train = y_scaler.transform(y_train) y_validation = y_scaler.transform(y_validation) mlp = MLPRegressor() loss_train = [] loss_validation = [] for idx in range(50): mlp.partial_fit(x_train,y_train.ravel()) loss_train.append(mlp.loss_) loss_validation.append(mean_squared_error(y_validation,mlp.predict(x_validation))) plt.plot(loss_train, label=\"train loss\",c='r') plt.plot(loss_validation, label=\"validation loss\",c='g') plt.xlabel('Epoques') plt.ylabel('Fonction de co\u00fbt') plt.legend() Voici un exemple de mod\u00e8le de r\u00e9gression obtenu avec un PMC ayant les hyperparam\u00e8tres par d\u00e9faut : On voit bien que le PMC a \u00e9t\u00e9 capable d'apprendre une fonction non-lin\u00e9aire entre les variables d'entr\u00e9e et de sortie. Avec ce mod\u00e8le, on obtient \\(R^2 \\approx 0.900\\) en entrainement, et \\(R^2 \\approx 0.907\\) en test. On s'attend donc \u00e0 de tr\u00e8s bonnes performances en g\u00e9n\u00e9ralisation pour ce mod\u00e8le, mais on peut probablement obtenir mieux avec d'autres hyperparam\u00e8tres. Il faudrait donc \u00e0 pr\u00e9sent se baser sur ces codes pour effectuer une optimisation des hyperparam\u00e8tres.","title":"Application \u00e0 notre exemple"},{"location":"Chap3_Regression/#remarques_1","text":"La m\u00e9thode du Perceptron Multi-Couche a les avantages suivants pour la r\u00e9gression : Elle permet de dessiner de d\u00e9terminer des relations complexes entre les variables d'entr\u00e9e et de sortie sans faire de grosses hypoth\u00e8ses statistiques au pr\u00e9alable. Pour des nombres de variables d'entr\u00e9es importants , et des nombres d'\u00e9chantillons importants , l'entrainement d'un PMC devient plus int\u00e9ressant en termes de temps de calcul que les MCO. Une fois le mod\u00e8le entrain\u00e9, l' espace m\u00e9moire pour contenir le mod\u00e8le est plus faible que celui n\u00e9cessaire pour manipuler le \\(X\\) des MCO Mais cette m\u00e9thode a aussi les limites suivantes, les m\u00eames que pour la classification : Elle a de nombreux param\u00e8tres et hyperparam\u00e8tres \u00e0 optimiser. Elle est sensible au sur-apprentissage . Les d\u00e9cisions qu'elle prend sont difficilement expliqu\u00e9es et interpr\u00e9tables : on a aucun intervalle de confiance ou de pr\u00e9diction sur les sorties. On parle encore une fois de \"bo\u00eete noire\". L'initialisation de la m\u00e9thode se faisant de mani\u00e8re al\u00e9atoire , 2 apprentissages ne donneront pas exactement le m\u00eame mod\u00e8le. Comme nous l'avons d\u00e9j\u00e0 \u00e9voqu\u00e9, les r\u00e9seaux de neurones sont \u00e0 la base des mod\u00e8les d'apprentissage modernes, fondant ainsi une nouvelle sous-discipline : l' apprentissage profond .","title":"Remarques"},{"location":"Chap4_Partitionnement/","text":"Chapitre IV : Partitionnement Ce chapitre est une introduction au partitionnement (ou \"classification non-supervis\u00e9e\") : principe, mesures de performances et m\u00e9thodes de base. Probl\u00e8me de partitionnement Comme mentionn\u00e9 lors du Chapitre I, par \" partitionnement \" on entend diviser des individus non-lab\u00e9lis\u00e9s en groupes suivant leur proximit\u00e9 dans l'espace des features. On essayera d'assigner des labels \u00e0 ces groupes par la suite. L'id\u00e9e est de mieux comprendre un jeu de donn\u00e9es, et d'essayer de classifier de nouvelles donn\u00e9es . Diff\u00e9rents types de partitionnement Suivant le probl\u00e8me \u00e0 r\u00e9soudre, il existe 2 grands types de partitionnement : par partition et hi\u00e9rarchique . On parle de partitionnement \" par partition \" lorsque l'on cherche \u00e0 diviser les individus d'une base de donn\u00e9e en un nombre fini de groupes \\(k\\) , sans tisser de lien entre eux. On a alors aucune information sur la proximit\u00e9 des classes entre elles. Par opposition, on parle de partitionnement \" hi\u00e9rarchique \" lorsque l'on va diviser les individus d'une base de donn\u00e9es en \\(k\\) groupes, hi\u00e9rarchis\u00e9s selon leur similarit\u00e9. Cette hi\u00e9rarchisation sera dite \" descendante \" ou \" ascendante \" suivant si on part de 1 classe vers \\(k\\) classes, ou inversement. On repr\u00e9sente souvent la hi\u00e9rarchisation des classes sous la forme d'un diagramme appel\u00e9 dendrogramme : un arbre repr\u00e9sentant les liens entre classes (en abscisse) et leurs distances (en ordonn\u00e9e). La lab\u00e9lisation Une fois les donn\u00e9es s\u00e9par\u00e9es en \\(k\\) groupes, l'\u00e9tape suivante est souvent d'essayer d'attribuer des labels aux classes ainsi d\u00e9termin\u00e9es. Ceci permettra de donner une interpr\u00e9tation \u00e0 notre partition, et \u00e0 nos futures pr\u00e9dictions. On appelle ce processus \" lab\u00e9lisation \". En l'absence de v\u00e9rit\u00e9 terrain pour chacun des individus, la lab\u00e9lisation est d\u00e9licate. On peut n\u00e9anmoins proposer la m\u00e9thode suivante : Essayer de caract\u00e9riser chaque groupe avec les outils de statistiques descriptives vus au Chapitre 1 (moyenne, \u00e9cart-type, etc.). Si une partition hi\u00e9rarchique a \u00e9t\u00e9 r\u00e9alis\u00e9e, \u00e9tudier aussi les liens entre les groupes . Sinon, analyser les distances entre groupes. Comparer les caract\u00e9ristiques de chaque groupe, ainsi que les liens entre groupes, aux connaissances \u00e9tablies sur le domaine d'application, ou \u00e0 un petit \u00e9chantillon de donn\u00e9es lab\u00e9lis\u00e9es si disponible. Attribuer un label \u00e0 chaque groupe en se basant sur ces \u00e9l\u00e9ments. La lab\u00e9lisation implique donc une certaine expertise dans le domaine o\u00f9 on cherche \u00e0 appliquer de la classification non-supervis\u00e9e. Exemple de probl\u00e8me Comment recenser les esp\u00e8ces de chauves-souris pr\u00e9sentes sur le site de l'OVSQ ? Les chiropt\u00e9rologues (sp\u00e9cialistes des chauves-souris), utilisent souvent l'identification acoustique pour faire un relev\u00e9 des esp\u00e8ces de chauves-souris pr\u00e9sentes sur un site donn\u00e9. En effet, les ultrasons \u00e9mis par les chauves-souris sont caract\u00e9ristiques de leur esp\u00e8ce, et contrairement \u00e0 la capture, cette m\u00e9thode n'a aucun impact sur ces animaux, qui sont prot\u00e9g\u00e9s en France. Situ\u00e9 en bordure de la for\u00eat des Sources de la Bi\u00e8vre \u00e0 Guyancourt, l'Observatoire de Versailles Saint-Quentin-en-Yvelines (OVSQ) est travers\u00e9 toutes les nuits d'\u00e9t\u00e9 par des chauves-souris. C'est pourquoi depuis 2024, l'OVSQ installe un enregistreur d'ultrasons sur son site, afin de recenser les diff\u00e9rentes esp\u00e8ces pr\u00e9sentes. Voici quelques exemples de \"sonogrammes\" obtenus \u00e0 partir des enregistrements de l'OVSQ : Chaque image repr\u00e9sente un cri de chauve-souris : en abscisse le temps, en ordonn\u00e9e la fr\u00e9quence, et en nuance de gris l'amplitude. On a l'impression que ces 5 cris proviennent de 5 esp\u00e8ces diff\u00e9rentes : fr\u00e9quences moyennes diff\u00e9rentes, plage de fr\u00e9quences diff\u00e9rentes, dur\u00e9es diff\u00e9rentes, formes diff\u00e9rentes et nombre d'harmoniques diff\u00e9rentes. Pour les besoins de ce cours, ont \u00e9t\u00e9 s\u00e9lectionn\u00e9s 474 enregistrements de cris de chauves-souris provenant de l'OVSQ. Nous aimerions entrainer un mod\u00e8le \u00e0 reconnaitre les esp\u00e8ces de chauves-souris enregistr\u00e9es, mais nous n'avons pas de v\u00e9rit\u00e9 terrain pour v\u00e9rifier ses pr\u00e9dictions. Est-il tout de m\u00eame possible de diviser ces enregistrements en plusieurs classes selon leurs similarit\u00e9s, et d'identifier par la suite l'esp\u00e8ce correspondant \u00e0 chaque classe ? Dans ce but, les 2 features suivantes ont \u00e9t\u00e9 retenues pour chaque enregistrement de cri de chauve-souris : la fr\u00e9quence moyenne du fondamental (kHz) et la dur\u00e9e du cri (ms). Voici le jeu de donn\u00e9es complet, au format CSV : Chap4_bats_dataset Le tableau de donn\u00e9es qu'il contient est de la forme : freq_mean time_len 31.000 7.500 30.340 6.250 28.921 4.750 27.218 9.750 29.574 6.750 ... ... 26.605 4.750 23.630 5.750 26.000 4.500 Notre probl\u00e8me de partitionnement sera le suivant : Identifier les diff\u00e9rentes esp\u00e8ces de chauves-souris dans les enregistrements de l'OVSQ, \u00e0 partir de la fr\u00e9quence moyenne du fondamental et de la dur\u00e9e du cri . Assurons-nous d'abord qu'une telle partition est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) Il est possible avec Seaborn d'afficher ces donn\u00e9es sous la forme d'un histogramme 2D , avec une r\u00e9solution de 30 intervalles par axes : import seaborn as sns sns.histplot(data=df_dataset, x='time_len', y='freq_mean',bins=30,cbar=True) Voici le r\u00e9sultat, \u00e0 c\u00f4t\u00e9 du nuage de points correspondant : On observe que ces 2 features font apparaitre diff\u00e9rents regroupements d'enregistrements : les distributions sont clairement multimodales. Si les fronti\u00e8res entre groupes, ainsi que le nombre exact de groupes restent difficiles \u00e0 \u00e9tablir, il n'y a aucun doute sur la pr\u00e9sence de plusieurs groupes. Et ces classes sont probablement li\u00e9es \u00e0 l'esp\u00e8ce de chauves-souris. Essayer de partitionner notre base de donn\u00e9es \u00e0 partir de ces features a donc du sens. Restera alors \u00e0 lab\u00e9liser les classes ainsi d\u00e9limit\u00e9es. Cependant, on peut noter que certains groupes visibles ont l'air moins denses que d'autres. Ceci est plausible : on imagine bien que certaines esp\u00e8ces sont plus communes sur le site que d'autres. Un tel d\u00e9s\u00e9quilibre pourrait \u00eatre probl\u00e9matique pour entrainer notre mod\u00e8le. Aussi, les diff\u00e9rents groupes visibles ont l'air d'avoir des formes et des densit\u00e9s diff\u00e9rentes. On observe m\u00eame quelques outliers qui pourraient d\u00e9ranger l'entrainement. Tout ceci sera \u00e0 prendre en compte dans notre interpr\u00e9tation des r\u00e9sultats de nos mod\u00e8les. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP. Mesures de performances Pour mettre au point des m\u00e9thodes d'apprentissage automatique d'une partition de donn\u00e9es, nous avons besoin de crit\u00e8res pour juger de la qualit\u00e9 de notre partition . Un des grands probl\u00e8mes en classification non-supervis\u00e9e est que le nombre de classes est une entr\u00e9e de la plupart des m\u00e9thodes de r\u00e9solution. Mais comment connaitre le nombre de classes pertinentes pour un jeu de donn\u00e9es ? Il faut tester diff\u00e9rents nombres de classes plausibles, et \u00e9valuer les performances du mod\u00e8le obtenu pour chacun. Probl\u00e8me : les donn\u00e9es auxquelles on veut appliquer une m\u00e9thode de partitionnement \u00e9tant par d\u00e9finition non-lab\u00e9lis\u00e9es, on ne peut pas calculer une erreur par rapport \u00e0 une v\u00e9rit\u00e9 terrain . Il existe n\u00e9anmoins des crit\u00e8res pour \u00e9valuer la pertinence d'un partitionnement. Nous allons voir dans cette section diff\u00e9rents crit\u00e8res pour \u00e9valuer un partitionnement , et diff\u00e9rentes m\u00e9thodes pour d\u00e9terminer un nombre de classes optimal pour un jeu de donn\u00e9es. Inertie intra-classe et internie inter-classe Un bon partitionnement a les 2 caract\u00e9ristiques suivantes : Les individus au sein d'un groupe sont les plus similaires possibles (leurs distances dans l'espace des features sont les plus faibles possibles). Les diff\u00e9rents groupes sont les plus diff\u00e9rents possibles (leurs distances dans l'espace des features sont les plus grandes possibles). On utilise souvent comme indicateurs de ces 2 caract\u00e9ristiques l' inertie intra-classe et inter-classe. L' inertie d'une classe \\(i\\) contenant \\(n_i\\) individus est d\u00e9finie comme la somme des distances au centre de gravit\u00e9 \\(g_i\\) de la classe : \\(I_i = \\sum_{j=1}^{n_i} d(x_{i,j},g_i)^2\\) o\u00f9 chaque \\(x_{i,j}\\) est un vecteur contenant les r\u00e9alisations des diff\u00e9rentes features pour un individu de la classe \\(i\\) . Nota Bene Il s'agit d'une analogie avec la notion de moment d'inertie en Physique : la r\u00e9partition de la masse dans un objet autour de son centre de gravit\u00e9 va rendre plus ou moins difficile sa mise en mouvement. D'une mani\u00e8re analogue, la r\u00e9partition des individus dans un groupe va rendre plus ou moins co\u00fbteuse en termes de performances un changement de centre de gravit\u00e9 des groupes (idem pour les groupes vis-\u00e0-vis du centre de gravit\u00e9 du jeu de donn\u00e9es total). Cette formule d\u00e9pend bien \u00e9videmment de la d\u00e9finition du centre de gravit\u00e9 \\(g_i\\) de la classe \\(i\\) , et de la mesure de distance \\(d\\) choisie. Pour le centre de gravit\u00e9, on va souvent consid\u00e9rer le barycentre : \\(g_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} x_{i,j}\\) Pour les mesures de distances, reportez-vous \u00e0 la section \"K plus proches voisins\" du Chapitre 2. Dans le cas o\u00f9 on se servirait des inerties pour entrainer un mod\u00e8le de partitionnement, la mesure de distance sera un hyperparam\u00e8tre \u00e0 optimiser . On d\u00e9finit alors l' inertie intra-classe comme \u00e9tant la somme des inerties des \\(k\\) classes : \\(I = \\sum_{i=1}^{k} I_i = \\sum{i=1}^{k} \\sum_{j=1}^{n_i} d(x_{i,j},g_i)^2\\) Il s'agit d'un indicateur de la similarit\u00e9 des individus au sein de chaque classe . L' inertie inter-classe est quant \u00e0 elle d\u00e9finie comme : \\(J = \\sum_{i=1}^{k} n_i d(g_i,g)^2\\) avec \\(g = \\frac{1}{\\sum_{i=1}^{k} n_i} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} x_{i,j}\\) le barycentre du jeu de donn\u00e9es complet. Il s'agit d'un indicateur de la s\u00e9parabilit\u00e9 des diff\u00e9rentes classes . Th\u00e9or\u00e8me de Huygens On note \\(T\\) l' inertie totale d'un jeu de donn\u00e9es : \\(T = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} d(x_{i,j},g)^2\\) De mani\u00e8re analogue au th\u00e9or\u00e8me de Huygens en Physique, on a conservation de l'inertie totale d'un jeu de donn\u00e9es : \\(T = I + J\\) Cette somme est ind\u00e9pendante de la partition choisie. Une bonne partition minimise l'inertie intra-classe , ce qui d'apr\u00e8s le th\u00e9or\u00e8me de Huygens revient \u00e0 maximiser l'inertie inter-classe . Les inerties peuvent \u00eatre utilis\u00e9es pour comparer les performances de 2 partitions d'un m\u00eame jeu de donn\u00e9es , \u00e0 \\(k\\) constant , mais elles ne permettent pas de comparer 2 partitions de donn\u00e9es diff\u00e9rentes, ou de \\(k\\) diff\u00e9rents. De plus, l'inertie intra-classe diminue toujours \u00e0 mesure que l'on augmente le nombre classes d'une partition de donn\u00e9es optimis\u00e9e pour chaque valeur de \\(k\\) : alors comment choisir un nombre de classe pertinent ? Nous allons voir 2 crit\u00e8res pour r\u00e9soudre ces probl\u00e8mes. La m\u00e9thode du coude Comme nous venons de le mentionner, l'inertie intra-classe diminue toujours \u00e0 mesure que l'on augmente le nombre de classe \\(k\\) de notre partition de donn\u00e9es (optimis\u00e9e pour chaque \\(k\\) ). Mais cette diminution a tendance \u00e0 ralentir \u00e0 mesure que \\(k\\) augmente : plus \\(k\\) est \u00e9lev\u00e9, moins on gagne en inertie intra-classe en augmentant \\(k\\) . Plus pr\u00e9cis\u00e9ment, si on trace l'inertie intra-classe en fonction de \\(k\\) , on observe typiquement une courbe en forme de coude : au d\u00e9but l'inertie diminue fortement, puis soudainement elle diminue de mani\u00e8re marginale. On va alors consid\u00e9rer que la valeur de \\(k\\) optimale est celle qui correspond \u00e0 l' angle du coude , c'est-\u00e0-dire quand augmenter \\(k\\) n'apporte plus grand chose en termes d'inertie intra-classe. C'est ce que l'on appelle assez logiquement la \" m\u00e9thode du coude \". Nota Bene La m\u00e9thode du coude n'est pas sp\u00e9cifique aux probl\u00e8mes de partitionnement. On la retrouve dans la d\u00e9termination d'un param\u00e8tre optimal pour de nombreux types de mod\u00e8les. En pratique, cette m\u00e9thode n'est pas toujours simple \u00e0 appliquer \"visuellement\". En effet, on peut parfois obtenir une courbe de l'inertie intra-classe en fonction de \\(k\\) sans coude \u00e9vident. Dans un tel cas, le choix de \\(k\\) peut s'av\u00e9rer arbitraire. Pour obtenir un r\u00e9sultat plus objectif, on peut baser sa d\u00e9cision sur un crit\u00e8re statistique . Le plus connu est le Crit\u00e8re d'Information d'Akaike (AIC) : \\(AIC(k) = 2 p - 2 log(L(k))\\) avec \\(p\\) le nombre de param\u00e8tres \u00e0 estimer, et \\(L(k)\\) la vraisemblance du mod\u00e8le pour \\(k\\) classes. L'id\u00e9e est ici que le nombre de classe optimal est celui qui minimise l'AIC : la formule cherche un mod\u00e8le vraisemblable, tout en p\u00e9nalisant le choix d'un nombre de classes trop grand. Si la m\u00e9thode de partitionnement choisie ne se base pas sur un mod\u00e8le statistique, il faudra faire des hypoth\u00e8ses statistiques afin de d\u00e9terminer une fonction de vraisemblance. Coefficient de silhouette Nous avons vu que la m\u00e9thode du coude peux aider \u00e0 trouver le nombre de classes optimal dans certains cas, mais ce choix peut s'av\u00e9rer difficile en pratique. Et les crit\u00e8res statistiques tels que l'AIC impliquent de faire des hypoth\u00e8ses statistiques plus ou moins justes sur la m\u00e9thode de partitionnement choisie. Dans l'id\u00e9al, nous aimerions un crit\u00e8re qui permette de comparer des mod\u00e8les de partitionnement obtenus pour diff\u00e9rents jeux de donn\u00e9es, pour diff\u00e9rents nombre de classes, et sans hypoth\u00e8ses sur la m\u00e9thode choisie. C'est pourquoi le \" coefficient de silhouette \" est un des crit\u00e8res les plus utilis\u00e9s pour \u00e9valuer une partition de donn\u00e9es. Pour chaque individu de la base de donn\u00e9es, il est d\u00e9finit comme : \\(s(x_{i,j}) = \\frac{D_2(x_{i,j})-D_1(x_{i,j})}{max(D_1(x_{i,j}),D_2(x_{i,j}))}\\) Avec \\(D_1\\) la distance moyenne intra-classe : \\(D_1(x_{i,j}) = \\frac{1}{n_i-1} \\sum_{m=1,m \\neq j}^{n_i} d(x_{i,m},x_{i,j})\\) Il s'agit d'un indicateur de la similarit\u00e9 d'un individu au reste de sa classe : plus il est faible, plus l'individu est proche du reste de sa classe. Et \\(D_2\\) la distance moyenne \u00e0 la classe la plus proche : \\(D_2(x_{i,j}) = min_{1 \\leq l \\leq k, l \\neq i}(\\frac{1}{n_l} \\sum_{m=1}^{n_l} d(x_{l,m},x_{i,j}))\\) Il s'agit d'un indicateur de s\u00e9parabilit\u00e9 d'un individus par rapport \u00e0 la classe la plus proche de la sienne : plus il est \u00e9lev\u00e9, plus l'individu est s\u00e9parable des autres classes Le coefficient de silhouette est un score compris entre -1 et 1. Si pour un individu : \\(s(x_{i,j}) \\approx 1\\) alors l'individu est correctement identifi\u00e9 \u00e0 sa classe. \\(s(x_{i,j}) = 0\\) alors l'individu est \u00e0 la fronti\u00e8re entre 2 classes. \\(s(x_{i,j}) < 0\\) alors l'individu est mal identifi\u00e9 \u00e0 sa classe. On peut alors utiliser le coefficient de silhouette moyen \\(S = \\frac{1}{\\sum_{i=1}^{k} n_i} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} s(x_{i,j})\\) comme mesure de la qualit\u00e9 d'une partition de donn\u00e9es : il doit \u00eatre le plus proche possible de 1. Dans le but de choisir un nombre de classes optimal pour une partition, on peut simplement tracer la courbe de \\(S\\) obtenu pour les mod\u00e8les optimis\u00e9s par chaque \\(k\\) , et choisir la valeur de \\(k\\) maximisant \\(S\\) . Si on veut essayer de comprendre pourquoi une partition a de mauvaises performances, on peut analyser les valeurs de \\(s(x_{i,j})\\) pour chaque individu d'un jeu de donn\u00e9es. On affiche en g\u00e9n\u00e9ral les coefficients de silhouette sous la forme d'un diagramme en barres , avec en abscisses \\(s\\) et en ordonn\u00e9es les individus (rang\u00e9s par classe). On peut alors facilement identifier quels individus ont \u00e9t\u00e9 correctement associ\u00e9s \u00e0 la bonne classe ou non. Nota Bene Les coefficients de silhouette obtenus d\u00e9pendent \u00e9videmment de la mesure de distance choisie. M\u00e9thodes de base K Moyennes Principe La m\u00e9thode de base en partitionnement \"par partition\" est celle des K-moyennes . Il s'agit d'un algorithme it\u00e9ratif , cherchant \u00e0 r\u00e9duire \u00e0 chaque it\u00e9ration l'inertie intra-classe, \u00e0 partir d'une partition initiale al\u00e9atoire. L'id\u00e9e ici est donc d'essayer de faire converger le mod\u00e8le vers la partition minimisant l'inertie intra-classe . Le nombre de classes \\(k\\) est un param\u00e8tre d'entr\u00e9e de l'algorithme. Voici l'algorithme d\u00e9taill\u00e9 : Algorithme des K-moyennes On initialise al\u00e9atoirement \\(k\\) points \\(h_i\\) dans l'espace des features choisies, chacun correspondant \u00e0 une classe. Jusqu'\u00e0 ce qu'une crit\u00e8re d'arr\u00eat soit atteint, on va it\u00e9rer les actions suivantes : - On assigne \u00e0 chaque individu la classe \\(i\\) de point \\(h_i\\) le plus proche selon une mesure de distance. - On calcule le barycentre \\(g_i\\) de chacune des classes. - On assigne \u00e0 \\(h_i\\) le point \\(g_i\\) pour chacune des classes. Le but est de faire converger les \\(h_i\\) vers les barycentres des centres de gravit\u00e9 des classes id\u00e9ales. Il est \u00e0 noter que dans la plupart des impl\u00e9mentations des K-moyennes, la distance choisie est la distance euclidienne . N\u00e9anmoins, d'autres mesures de distance peuvent \u00eatre envisag\u00e9es. Il est \u00e9galement essentiel d'avoir en t\u00eate que lorsque l'on utilise les K-moyennes, on fait des hypoth\u00e8ses implicites : \\(k\\) est bien le nombre de classes optimal. Les diff\u00e9rentes classes sont \"sph\u00e9riques\" au sens de la distance choisie : on parle \"d' isotropie \" des classes. Chaque classe a la m\u00eame variance . Chaque classe a le m\u00eame nombre d'individus : elles sont \" \u00e9quilibr\u00e9es \". Plusieurs approches peuvent \u00eatre propos\u00e9es en cas de non-respect de ces hypoth\u00e8se : Le nombre de classes optimal peut \u00eatre estim\u00e9 avec une des m\u00e9thodes vues pr\u00e9c\u00e9demment. Si les classes sont anisotropes , c'est peut-\u00eatre parce que les features \u00e9voluent sur des ordres de grandeur diff\u00e9rents. Une normalisation des features peut alors aider. Si les classes sont d\u00e9s\u00e9quilibr\u00e9es , il est recommand\u00e9 de tester plusieurs initialisations des K-moyennes, pour \u00e9viter de rester bloqu\u00e9 dans un minimum local d'inertie intra-classe. Dans le cas o\u00f9 aucune de ces approches n'est efficace, il faut tout simplement envisager un autre mod\u00e8le de partitionnement que les K-moyennes. Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode des K-moyennes. Elle peut \u00eatre import\u00e9e avec : from sklearn.cluster import KMeans On peut ensuite initialiser un mod\u00e8le de partition km avec un objet \"KMeans\" de param\u00e8tre k correspondant au nombre de classes \u00e0 d\u00e9terminer : km = KMeans(n_clusters=k) Pour diviser le jeu de donn\u00e9es en \\(k\\) classes clusters \u00e0 partir des features choisies features , on utilise la m\u00e9thode : clusters = km.fit_predict(features) Si on veut obtenir le coefficient de silhouette moyen de notre partition, on peut utiliser la commande : from sklearn.metrics import silhouette_score silhouette_score(features,clusters) Affichage des coefficients de silhouette Dans les biblioth\u00e8ques s\u00e9lectionn\u00e9es dans le cadre de ce cours, il n'existe pas d'impl\u00e9mentation de l'affichage des coefficients de silhouette sous la forme d'un diagramme en barres. Cependant, dans ses tutoriels en ligne, Scikit-Learn propose un code pour r\u00e9aliser ce type d'affichage \"manuellement\". En partant du principe que l'on a les features choisies dans features , et les classes obtenues par partitionnement en k dans clusters , on peut utiliser la m\u00e9thode \"silhouette_samples\" pour r\u00e9cup\u00e9rer les coefficients de silhouette, et r\u00e9aliser un affichage avec Matplotlib : from sklearn.metrics import silhouette_score,silhouette_samples import numpy as np import matplotlib.pyplot as plt sample_scores = silhouette_samples(features,clusters) mean_score = silhouette_score(features,clusters) fig, ax = plt.subplots() y_lower = 10 for idx in range(k): sample_scores_idx = sample_scores[clusters == idx] sample_scores_idx.sort() size_cluster_idx = sample_scores_idx.shape[0] y_upper = y_lower + size_cluster_idx color = plt.cm.tab10(idx) ax.fill_betweenx( np.arange(y_lower, y_upper), 0, sample_scores_idx, facecolor=color, edgecolor=color, alpha=0.7 ) ax.text(-0.05, y_lower + 0.5 * size_cluster_idx, str(idx)) y_lower = y_upper + 10 ax.axvline(x=mean_score,color=\"red\",linestyle=\"--\") ax.set_yticks([]) ax.set_xlim([-0.1, 1]) ax.set_xlabel(\"Coefficient de silhouette\",fontsize=12) ax.set_ylabel(\"Classes\",fontsize=12) Vous pouvez r\u00e9utiliser ce code tel quel pour vos propres affichages. Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer les K-moyennes \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Afin de s'assurer que les 2 features \u00e9voluent sur des intervalles comparables, nous leur appliquons une transformation de centrage-r\u00e9duction (voir Chapitre 1) : from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_dataset) df_dataset[['freq_mean','time_len']] = pd.DataFrame(scaler.transform(df_dataset)) Nous allons dans un premier temps essayer de trouver le nombre de classes optimal pour notre partition. Utilisons le coefficient de silhouette moyen pour chaque nombre de classes entre 2 et 15, et affichons les scores obtenus : from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt silhouette = [] for k in range(2,15): km = KMeans(n_clusters=k,random_state=0) clusters = km.fit_predict(df_dataset) score = silhouette_score(df_dataset,clusters) silhouette.append(score) plt.plot(np.arange(2,15),silhouette,'ro-') plt.grid() plt.xlabel('Nombre de classes',fontsize=12) plt.ylabel('Coefficient de silhouette moyen',fontsize=12) Voici la courbe obtenue : On observe que le nombre de classes maximisant le coefficient de silhouette moyen est \\(k=3\\) . Nous choisirons donc ce param\u00e8tre pour la suite. La valeur du coefficient moyen obtenu sera de 0,76 environ, ce qui est consid\u00e9r\u00e9 comme un bon score pour des donn\u00e9es r\u00e9elles. Nota Bene Il est \u00e0 noter que nous avons mis le param\u00e8tre \"random_state\" \u00e0 0 car le mod\u00e8le renvoy\u00e9 par les K-moyennes d\u00e9pend de l'initialisation. Nous nous assurons ainsi que 2 executions de ce code Python donnerons le m\u00eame r\u00e9sultat. Il faudrait en toute rigueur v\u00e9rifier que d'autres initialisation donnent le m\u00eame \\(k\\) optimal. Nous pouvons ajouter les 3 classes identifi\u00e9es au DataFrame d'entr\u00e9e, puis inverser le centrage-r\u00e9duction des features : km = KMeans(n_clusters=3,random_state=0) clusters = km.fit_predict(df_dataset) df_dataset_clustured = df_dataset.copy() df_dataset_clustured['clusters'] = clusters df_dataset_clustured[['freq_mean','time_len']] = pd.DataFrame(scaler.inverse_transform(df_dataset_clustured[['freq_mean','time_len']])) Et afficher sous la forme d'un nuages de points la partition obtenue, en utilisant Seaborn : import seaborn as sns sns.scatterplot(data=df_dataset_clustured,x='time_len',y='freq_mean',hue='clusters',palette='tab10') Voici le graphique obtenu : Les classes identifi\u00e9es ont l'air coh\u00e9rentes avec les pics que nous avions observ\u00e9s dans les donn\u00e9es en d\u00e9but de chapitre. Pour v\u00e9rifier la qualit\u00e9 de cette partition, nous proposons d'afficher le coefficient de silhouette pour chaque individu, sous la forme d'un diagramme en barres . Voici le graphique obtenu avec le code donn\u00e9 pr\u00e9c\u00e9demment : Tout d'abord, on observe qu'aucun individu n'a un coefficient de silhouette n\u00e9gatif, ce qui est le signe d'un plut\u00f4t bon partitionnement. Ensuite, on voit que pour le classe 0, la quasi-int\u00e9gralit\u00e9 des individus a un coefficient sup\u00e9rieur \u00e0 0,5. Ceci est coh\u00e9rent avec la matrice de corr\u00e9lation que nous avons affich\u00e9e pr\u00e9c\u00e9demment : la classe 0 a l'air d'\u00eatre la mieux s\u00e9par\u00e9e des 3. Les classes 1 et 2 ont quelques individus avec des coefficients entre 0 et 0,5. Ces individus sont donc proches de la fronti\u00e8re avec la classe la plus proche. Ce qui une fois de plus colle au r\u00e9sultat pr\u00e9c\u00e9dent : les classes 1 et 2 ont l'air plus difficilement s\u00e9parables avec les features retenues. Il est \u00e0 noter que, comme beaucoup de cas pratiques, notre probl\u00e8me ne partitionnement ne respecte pas exactement les hypoth\u00e8ses implicites des K-moyennes : Comme nous l'avions d\u00e9j\u00e0 mentionn\u00e9, nos classes sont d\u00e9s\u00e9quilibr\u00e9es. Nos classes ne sont pas isotropes. La variance de nos classes n'est clairement pas la m\u00eame. Il y a des outliers dans nos donn\u00e9es. De meilleurs r\u00e9sultats pourraient donc potentiellement \u00eatre obtenus avec plus d'observations pour \u00e9quilibrer les classes, en supprimant les outliers, ou avec une m\u00e9thode aux hypoth\u00e8ses diff\u00e9rentes. Remarques La m\u00e9thode des K-moyennes a les avantages suivants : Elle est simple \u00e0 impl\u00e9menter, et son r\u00e9sultat est tout \u00e0 fait interpr\u00e9table par un humain : on cherche \u00e0 obtenir les centres de gravit\u00e9 des classes. Elle converge assez rapidement , et on peut donc l'utiliser sur de grands jeux de donn\u00e9es . Mais cette m\u00e9thode a aussi les limites suivantes : Les performances de la m\u00e9thode sont mauvaises si les hypoth\u00e8ses implicites sur les donn\u00e9es ne sont pas respect\u00e9es. L'initialisation \u00e9tant al\u00e9atoire , 2 executions de l'algorithme ne donneront pas exactement la m\u00eame partition . Il est m\u00eame possible que la m\u00e9thode tombe dans un minimum local de l'inertie intra-classe. La distance euclidienne \u00e9tant sensible aux outliers , la m\u00e9thode des K-moyennes l'est aussi. Les K-moyennes n'\u00e9tant pas une m\u00e9thode de partitionnement hi\u00e9rarchique, elle ne trace aucun lien entre les classes qu'elle d\u00e9termine. Classification Ascendante Hi\u00e9rarchique Principe Comme son nom l'indique, la m\u00e9thode de la Classification Ascendante Hi\u00e9rarchique (CAH) est une m\u00e9thode de partitionnement hi\u00e9rarchique , qui agr\u00e8ge les classes de mani\u00e8re ascendante . Il s'agit d'une m\u00e9thode it\u00e9rative , fusionnant 2 classes \u00e0 chaque it\u00e9ration, ce qui correspond \u00e0 fusionner 2 branches d'un dendrogramme. Elle s'initialise en consid\u00e9rant chaque individu comme unique repr\u00e9sentant de sa propre classe : on a autant de classes que d'individu, l'inertie intra-classe est nulle . Elle se termine une fois que toutes les classes ont \u00e9t\u00e9 fusionn\u00e9es : on a une unique classe, l'inertie inter-classe est nulle . On peut d\u00e9couper le dendrogramme obtenu au niveau du nombre de classe \\(k\\) d\u00e9sir\u00e9, ou alors arr\u00eater les it\u00e9rations de la m\u00e9thode pour ce \\(k\\) si nous ne sommes pas int\u00e9ress\u00e9s par le dendrogramme. Voici l'algorithme d\u00e9taill\u00e9 : Algorithme de la CAH On initialise \\(n\\) classes : une par individu dans le jeu de donn\u00e9es. Jusqu'\u00e0 ce que l'on atteigne un nombre de classe \u00e9gal \u00e0 1 (ou \u00e9gal au nombre de classes \\(k\\) d\u00e9sir\u00e9), on va it\u00e9rer les actions suivantes : - On mesure la similarit\u00e9 entre les diff\u00e9rentes classes. - Les 2 classes les plus similaires sont fusionn\u00e9es. On enregistre les classes obtenues \u00e0 chaque it\u00e9ration pour pouvoir tracer un dendrogramme. Cette m\u00e9thode implique de d\u00e9cider d'un crit\u00e8re de similarit\u00e9 entre 2 classes. Il s'agira d'un hyperparam\u00e8tre \u00e0 choisir. Voici les 4 principaux crit\u00e8res utilisables par la CAH pour mesurer la similarit\u00e9 entre 2 classes : Lien simple : la distance minimale entre 2 individus issus de ces 2 classes. Lien complet : la distance maximale entre 2 individus issus de ces 2 classes. Lien moyen : la moyenne des distances entre tous les couples d'individus issus des 2 classes possibles. Crit\u00e8re de Ward : l'augmentation de l'inertie intra-classe quand les 2 classes sont fusionn\u00e9es. Derri\u00e8re ces crit\u00e8res, se cache donc le choix d'une mesure de distance entre individus. Certains crit\u00e8res sont plus rapides \u00e0 calculer que d'autres, mais lorsque l'on choisis un crit\u00e8re plut\u00f4t qu'un autre, on fait un choix sur notre vision de la \"proximit\u00e9\" entre classes : Lien simple : on voit la notion de similarit\u00e9 \u00e0 l' \u00e9chelle des individus . Ce crit\u00e8re est adapt\u00e9 aux cas de classes tr\u00e8s anisotropes, mais aura tendance \u00e0 relier 2 classes si un outlier se trouve entre elles. On la consid\u00e8rera donc comme peu conservatrice . Lien complet : on voit la notion de similarit\u00e9 \u00e0 l' \u00e9chelle de la classe enti\u00e8re . Ce crit\u00e8re est adapt\u00e9 aux classes fortement s\u00e9par\u00e9es, mais aura aussi tendance \u00e0 \u00eatre sensible aux outliers, mais dans l'exc\u00e8s inverse : la m\u00e9thode sera tr\u00e8s conservatrice , et un outlier pourra \u00e0 lui tout seul emp\u00eacher de lier 2 classes pourtant proches. Lien moyen : on voit la notion de similarit\u00e9 du point de vue de la moyenne des distances des individus entre classes. On peut donc voir ce crit\u00e8re comme un compromis entre les 2 pr\u00e9c\u00e9dents . Il aura tendance \u00e0 favoriser des classes \"sph\u00e9riques\" au sens de la distance choisie. Crit\u00e8re de Ward : il s'agit du crit\u00e8re pour lequel les hypoth\u00e8ses sont les plus fortes . Elles sont similaires \u00e0 celles des K-moyennes : classes isotropes, de m\u00eame variance, et \u00e9quilibr\u00e9es. En pratique, on va souvent choisir par d\u00e9faut le crit\u00e8re de Ward . Impl\u00e9mentation Scikit-Learn Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode de la CAH. Elle peut \u00eatre import\u00e9e avec : from sklearn.cluster import AgglomerativeClustering On peut ensuite initialiser un mod\u00e8le de partition hca avec un objet \"AgglomerativeClustering\" de param\u00e8tre k correspondant au nombre de classes \u00e0 d\u00e9terminer : hca = AgglomerativeClustering(n_clusters=k) Par d\u00e9faut, l'impl\u00e9mentation Scikit-Learn utilise le crit\u00e8re de Ward comme mesure de similarit\u00e9. Il est possible de changer cet hyperparam\u00e8tre avec le param\u00e8tre \"linkage\" de l'objet \"AgglomerativeClustering\" : 'ward', 'complete', 'average' ou 'single'. Pour diviser le jeu de donn\u00e9es en \\(k\\) classes clusters \u00e0 partir des features choisies features , on utilise la m\u00e9thode : clusters = hca.fit_predict(features) Comme pour les K-moyennes, si on veut obtenir le coefficient de silhouette moyen de notre partition, on peut utiliser la commande : from sklearn.metrics import silhouette_score silhouette_score(features,clusters) Affichage d'un dendrogramme avec Scipy Dans les biblioth\u00e8ques s\u00e9lectionn\u00e9es dans le cadre de ce cours, il n'existe pas d'impl\u00e9mentation pour afficher le dendrogramme d'une CAH. Cependant, dans ses tutoriels en ligne, Scikit-Learn propose une fonction pour r\u00e9aliser ce type d'affichage \"manuellement\", en s'appuyant sur la fonction \"dendrogramme\" de la biblioth\u00e8que Scipy. La voici, pour un mod\u00e8le d\u00e9termin\u00e9 par CAH model : from scipy.cluster.hierarchy import dendrogram import numpy as np def plot_dendrogram(model, **kwargs): counts = np.zeros(model.children_.shape[0]) n_samples = len(model.labels_) for i, merge in enumerate(model.children_): current_count = 0 for child_idx in merge: if child_idx < n_samples: current_count += 1 else: current_count += counts[child_idx - n_samples] counts[i] = current_count linkage_matrix = np.column_stack([ model.children_, model.distances_, counts ]).astype(float) dendrogram(linkage_matrix, **kwargs) Vous pouvez r\u00e9utiliser ce code tel quel pour vos propres affichages. Pour pouvoir utiliser cette fonction, il faut cr\u00e9er une partition des features choisies features , avec les param\u00e8tres suivants : hca = AgglomerativeClustering(distance_threshold=0,n_clusters=None) hca.fit(features) Avec d'autres param\u00e8tres, il ne sera pas possible de r\u00e9cup\u00e9rer le dendrogramme associ\u00e9 au mod\u00e8le. Pour afficher le dendrogramme complet (de 1 classe par individu \u00e0 1 classe unique) : import maplotlib.pyplot as plt plt.figure() plot_dendrogram(hca,truncate_mode=\"level\") plt.xlabel(\"Nombre d'individus par classe\",fontsize=12) plt.ylabel(\"Distance entre classes\",fontsize=12) plt.xticks([]) plt.show() Mais en g\u00e9n\u00e9ral, on va pr\u00e9f\u00e9rer afficher le dendrogramme jusqu'au nombre de classes voulu k : import matplotlib.pyplot as plt plt.figure() plot_dendrogram(hca,truncate_mode=\"lastp\",p=k) plt.xlabel(\"Nombre d'individus par classe\",fontsize=12) plt.ylabel(\"Distance entre classes\",fontsize=12) plt.show() Application \u00e0 notre exemple Nous allons \u00e0 pr\u00e9sent appliquer la CAH \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Afin de s'assurer que les 2 features \u00e9voluent sur des intervalles comparables, nous leur appliquons une transformation de centrage-r\u00e9duction (voir Chapitre 1) : from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_dataset) df_dataset[['freq_mean','time_len']] = pd.DataFrame(scaler.transform(df_dataset)) Comme pour les K-moyennes, nous allons dans un premier temps essayer de trouver le nombre de classes optimal pour notre partition. Notre exemple \u00e9tant relativement simple, on s'attend \u00e0 ce que le r\u00e9sultat soit tr\u00e8s similaire. Nous utilisons une fois encore le coefficient de silhouette moyen pour chaque nombre de classes entre 2 et 15, et nous affichons les scores obtenus : from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt silhouette = [] for k in range(2,15): hca = AgglomerativeClustering(n_clusters=k) clusters = hca.fit_predict(df_dataset) score = silhouette_score(df_dataset,clusters) silhouette.append(score) plt.plot(np.arange(2,15),silhouette,'ro-') plt.grid() plt.xlabel('Nombre de classes',fontsize=12) plt.ylabel('Coefficient de silhouette moyen',fontsize=12) La courbe obtenue est en effet tr\u00e8s similaire \u00e0 celle obtenue avec les K-moyennes : C'est donc sans surprise que nous choisissons \u00e0 nouveau \\(k=3\\) . Le coefficient de silhouette moyen sera alors de 0,76. Ce score est environ le m\u00eame que pour les K-moyenne : de mani\u00e8re g\u00e9n\u00e9rale les 2 partitions sont aussi bonnes aux yeux du coefficient de silhouette. Nous pouvons ajouter les 3 classes identifi\u00e9es au DataFrame d'entr\u00e9e, puis inverser le centrage-r\u00e9duction des features : hca = AgglomerativeClustering(n_clusters=3) clusters = hca.fit_predict(df_dataset) df_dataset_clustured = df_dataset.copy() df_dataset_clustured['clusters'] = clusters df_dataset_clustured[['freq_mean','time_len']] = pd.DataFrame(scaler.inverse_transform(df_dataset_clustured[['freq_mean','time_len']])) Et comme pour les K-moyennes, nous pouvons afficher sous la forme d'un nuage de points la partition obtenue, en utilisant Seaborn : import seaborn as sns sns.scatterplot(data=df_dataset_clustured,x='time_len',y='freq_mean',hue='clusters',palette='tab10') Voici le graphique obtenu : Il est similaire \u00e0 celui obtenu par les K-moyennes, \u00e0 l'exception de quelques individus entre les classe (Attention, les classes n'ont pas re\u00e7u le m\u00eame num\u00e9ro). Le coefficient de silhouette de chaque individu peut aussi \u00eatre affich\u00e9 sous la forme d'un diagramme en barres , et sans surprise il est assez similaire \u00e0 celui obtenu par les K-moyennes : On remarque tout de m\u00eame que 2 individus sont attribu\u00e9s \u00e0 tort \u00e0 la classe 0 selon le coefficient de silhouette. Il s'agit de d'individus situ\u00e9s entre la classe 0 et la classe 1. Nota Bene Les K-moyennes et la CAH ne donneront pas toujours des partitions aussi similaires pour un m\u00eame jeu de donn\u00e9es. Nous sommes ici dans un cas particulier. La CAH \u00e9tant une m\u00e9thode de partitionnement hi\u00e9rarchique, elle permet de tisser des liens entre les diff\u00e9rentes classes. Nous pouvons tracer ces liens sous la forme d'un dendrogramme , en utilisant la fonction pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Voici le dendrogramme total, ainsi que le dendrogramme tronqu\u00e9 pour 3 classes : Selon ce dendrogramme, les classes 1 et 2 (104 et 210 individus) sont plus proches entre elles que de la classe 0 (160 individus). Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9d\u00e9mment, l'impl\u00e9mentation Scikit-Learn de la CAH utilise par d\u00e9faut le crit\u00e8re de Ward comme mesure de similarit\u00e9 entre classes. C'est souvent le compromis choisi pour la CAH. Selon la d\u00e9finition de la \"similarit\u00e9\" du crit\u00e8re de Ward, cela signifie que fusionner les classes 1 et 2 fait moins monter l'inertie intra-classe que fusionner les classe 0 et 1 ou 0 et 2. Il est \u00e9vident que si nous d\u00e9finissons autrement la \"similarit\u00e9\" entre classes, le dendrogramme que nous obtiendrons sera diff\u00e9rent : 2 groupes peuvent \u00eatre plus proche selon une mesure de similarit\u00e9 qu'une autre. Voici les dendrogrammes obtenus pour les 4 mesures de similarit\u00e9 impl\u00e9ment\u00e9es dans Scikit-Learn : On remarque directement que les crit\u00e8res de similarit\u00e9 du \"lien simple\" et du \"lien moyen\" donnent de mauvais r\u00e9sultats : une des classes n'a qu'un seul individu... Pour les autres crit\u00e8res, difficile de dire quelle partition est la meilleure \u00e0 partir des dendrogrammes seuls. On peut v\u00e9rifier les partitions obtenues avec des nuages de points selon les diff\u00e9rentes features. Voici un exemple pour la fr\u00e9quence moyenne du fondamental et la dur\u00e9e du cri : On comprend alors l'origine de la sous-performance des liens \"simple\" et \"moyen\": un outlier. Dans les 2 cas, la CAH a isol\u00e9 cet outlier dans une classe dont il est l'unique individu. Ceci peut se comprendre intuitivement : un outlier est par d\u00e9finition loin de tous les autres individus, ainsi que de la moyenne des individus, et ceci pour toutes les classes. Nous avons ici une illustration de la sensibilit\u00e9 aux outliers de ces 2 types de liens. Faire le m\u00e9nage dans notre base de donn\u00e9e en retirant les outliers pourrait donc grandement am\u00e9liorer les performances de la CAH dans ces 2 cas. On peut noter que l'outlier est plac\u00e9 comme \u00e9loign\u00e9 des 2 autre classes dans le dendrogramme pour le lien \"simple\", et comme proche de la classe 1. Encore une fois, ceci ce comprend facilement : du point de vue de la distance minimale entre individus, l'outlier est plus \u00e9loign\u00e9 des 2 autres classes, alors que du point de vue de la moyenne il est visiblement plus proche de la classe 1. D'apr\u00e8s les dendrogrammes obtenus, le lien \"complet\" et le crit\u00e8re de Ward sont en accord sur les liens entre classes. On observe aussi que les 2 types de liens d\u00e9limitent de la m\u00eame fa\u00e7on la classe 2. Par contre ils ne fixent clairement pas la m\u00eame fronti\u00e8re entre les classes 0 et 1 : le lien \"complet\" attribut plus d'individus \u00e0 la classe 1 que le crit\u00e8re de Ward. Ce r\u00e9sultat n'est pas surprenant : le lien \"complet\" se basant sur la distance maximale entre individus de 2 classes, l'outlier va avoir tendance \u00e0 \"tirer\" la fronti\u00e8re entre les classes 0 et 1 de sont c\u00f4t\u00e9. Ce n'est pas le cas pour le crit\u00e8re de Ward, qui ne regardera que l'inertie intra-classe. Nous avons ici une illustration de la sensibilit\u00e9 aux outlier du lien \"complet\". On aurait tendance \u00e0 choisir la partition obtenue avec le crit\u00e8re de Ward. Pour faire un choix d\u00e9finitif, on peut \u00e9valuer le coefficient de silhouette moyen de la partition obtenue pour chaque crit\u00e8re de similarit\u00e9 : Crit\u00e8re de similarit\u00e9 Coefficient de silhouette moyen Simple 0.55 Complet 0.72 Moyen 0.63 Ward 0.76 Selon le coefficient de silhouette moyen, sans surprise la meilleure partition est celle obtenue avec le crit\u00e8re de Ward. Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9demment, le crit\u00e8re de Ward est souvent le choix par d\u00e9faut. Cependant, suivant le probl\u00e8me auquel on est confront\u00e9, une autre mesure de similarit\u00e9 peut \u00eatre plus pertinente. Dans le cas de notre exemple, nettoyer notre base de donn\u00e9es des outliers permettrait probablement d'am\u00e9liorer les performances obtenues pour tous les types de liens. Comme pour les K-moyennes, les performances que nous obtenons ici avec le crit\u00e8re de Ward sont limit\u00e9es, car celui-ci implique des hypoth\u00e8ses fortes, qui ne sont pas respect\u00e9es ici (isotropie, variance constante, classes \u00e9quilibr\u00e9es). Remarques La m\u00e9thode de la CAH a les avantages suivants : Elle permet de tisser des liens entre les classes d\u00e9termin\u00e9es, ce qui rend le mod\u00e8le tr\u00e8s interpr\u00e9table . On peut d\u00e9cider du nombre optimal de classes \\(k\\) a posterio , en coupant le dendrogramme. Pour chaque \\(k\\) , le mod\u00e8le renvoy\u00e9 par le CAH est d\u00e9terministe : il n'y a pas ici d'initialisation al\u00e9atoire comme pour les K-moyennes. Il est possible d' adapter le crit\u00e8re de similarit\u00e9 selon notre probl\u00e8me. Mais cette m\u00e9thode a aussi les limites suivantes : Elle est relativement gourmande en temps de calcul , ce qui la rend difficile \u00e0 appliquer \u00e0 des grands jeux de donn\u00e9es. Suivant le crit\u00e8re de similarit\u00e9 choisie, elle peut \u00eatre plus ou moins sensible aux outliers . Lab\u00e9lisation de l'exemple El\u00e9ments pour la lab\u00e9lisation Nous allons \u00e0 pr\u00e9sent essayer de lab\u00e9liser les 3 classes obtenues avec les k-moyennes (sachant que les classes obtenues par CAH sont identiques). Commen\u00e7ons par caract\u00e9riser avec les statistiques descriptives les diff\u00e9rents groupes. Voici les moyennes des classes identifi\u00e9es selon les diff\u00e9rentes features : Classe 0 Classe 1 Classe 2 freq_mean 48.85 19.75 26.07 time_len 4.09 22.08 5.87 Pour attribuer une esp\u00e8ce \u00e0 chaque groupe, nous allons nous appuyer sur la cl\u00e9 d'identification acoustique du chercheur du Museum d'Histoire Naturelle Yves Bas. Commen\u00e7ons par la classe 0. Nous voyons que la dur\u00e9e des cris est assez courte, de 4.09 ms, et la fr\u00e9quence moyenne du fondamental est plut\u00f4t \u00e9lev\u00e9e, de 48.85 kHz. Voici la table d'identification pour ce type de cris, \u00e0 fr\u00e9quence \u00e9lev\u00e9e : Esp\u00e8ce Fr\u00e9quences (kHz) Pipistrelle de Kuhl 34-40 Pipistrelle de Nathusius 38-42 Pipistrelle commune 43-50 Pipistrelle pygm\u00e9e 53-60 Il est alors \u00e9vident que la seule esp\u00e8ce plausible est la Pipistrelle commune Ensuite, nous voyons que la classe 1 a des longs cris de 22.08 ms. La fr\u00e9quence moyenne du cri est de 19.75 kHz, ce qui est plut\u00f4t faible (\u00e0 la limite de l'audition d'un humain). Voici la table d'identification pour des cris de longue dur\u00e9e, \u00e0 fr\u00e9quence basse : Esp\u00e8ce Fr\u00e9quences (kHz) Molosse de Cestoni 9-12 Grande Noctule 13-15 Noctule commune 17-20 Noctule de Leisler 22-26 Il est alors \u00e9vident que la seule esp\u00e8ce plausible est la Noctule commune . Enfin, nous voyons que la classe 2 a une fr\u00e9quence moyenne interm\u00e9diaire de 26.07 kHz, avec une dur\u00e9e de 5.87 ms. Ce type de cris est g\u00e9n\u00e9ralement associ\u00e9 \u00e0 des chauves-souris de la famille des S\u00e9rotines ou des Oreillards . D'apr\u00e8s le document de Yves Bas, il est tr\u00e8s difficile de diff\u00e9rencier les cris des diff\u00e9rentes esp\u00e8ces d'Oreillards. Et sans informations sur la forme des cris, il nous est impossible d'attribuer avec certitude ces cris aux Oreillards ou aux S\u00e9rotines. Nous ne pouvons donc pas aller plus loin dans la lab\u00e9lisation de cette classe. D'o\u00f9 les labels que nous avons choisis pour les 3 classes : Noctule commune, Pipistrelle commune et Oreillards / S\u00e9rotine. On trouve facilement dans la litt\u00e9rature sp\u00e9cialis\u00e9e que ces 3 esp\u00e8ces sont cr\u00e9dibles pour une lisi\u00e8re de for\u00eat en Ile-de-France. De plus, cette lab\u00e9lisation est coh\u00e9rente avec les lien trac\u00e9s entre les classes par la CAH : les cris de noctules \u00e9tant beaucoup plus longs en moyenne que ceux des pipistrelles communes et des oreillards / s\u00e9rotines, on comprend pourquoi ils seraient consid\u00e9r\u00e9s comme \"moins similaires\". Comme nous l'avions anticip\u00e9, la lab\u00e9lisation a n\u00e9cessit\u00e9 ici des recherches bibliographiques sur l'identification acoustique des chauves-souris fran\u00e7aise. De mani\u00e8re g\u00e9n\u00e9rale, une lab\u00e9lisation correcte n\u00e9cessite souvent une expertise dans le domaine d'\u00e9tude. V\u00e9rit\u00e9 terrain En 2006, le Mus\u00e9um d'Histoire Naturelle lance le programme de sciences participatives \"Vigie-Chiro\". L'id\u00e9e est de collecter des enregistrements de chauves-souris r\u00e9alis\u00e9s par des amateurs dans toute la France, afin de r\u00e9aliser un suivi des diff\u00e9rentes esp\u00e8ces d'une ann\u00e9e sur l'autre, par r\u00e9gion. Dans le cadre de ce programme, l'OVSQ partage tous les ans l'int\u00e9gralit\u00e9 de ses enregistrements avec le Mus\u00e9um. Sur sa plateforme en ligne, \"Vigie-Chiro\" propose un logiciel d'identification automatique des enregistrements de chauves-souris : Tadarida (Bas et al., 2017). Nous avons lab\u00e9lis\u00e9 nos 474 enregistrements \u00e0 l'aide de Tadarida, et 5 esp\u00e8ces ont \u00e9t\u00e9 identifi\u00e9es : La s\u00e9rotine commune ( Eptesicus serotinus ), de label : Eptser . Le murin de Daubenton ( Myotis daubentonii ), de label : Myodau . La noctule commune ( Nyctalus noctula ), de label : Nycnoc . La pipistrelle commune ( Pipistrellus pipistrellus ), de label : Pippip . L'Oreillard roux ( Plecotus auritus ), de label : Pleaur . Voici l'identification Tadarida des diff\u00e9rents enregistrements, sous la forme d'un nuage de points : Et pour informations, les 5 exemples de sonogrammes pr\u00e9sent\u00e9s en d\u00e9but de chapitre correspondent chacun \u00e0 une esp\u00e8ce diff\u00e9rente parmi celles identifi\u00e9es par Tadarida : On remarque que les noctules communes, les pipistrelles communes, ainsi que les oreillard roux ont \u00e9t\u00e9 assez bien identifi\u00e9es par nos m\u00e9thodes de partitions. Par contre, la s\u00e9rotine commune et le murin de Daubenton n'ont pas \u00e9t\u00e9 clairement identifi\u00e9es. On peut avancer plusieurs explications : La s\u00e9rotine commune et le murin de Daubenton sont sous repr\u00e9sent\u00e9s dans les observations (26 et 41, contre 159 pour la noctule commune, 169 pour la pipistrelle commune, et 79 pour l'oreillard roux). La s\u00e9rotine commune a l'air difficilement s\u00e9parable de l'oreillard roux, et le murin de Daubenton difficilement s\u00e9parable de la pipistrelle commune. Il faudrait donc potentiellement ajouter de nouvelles features, ou appliquer une transformation aux features choisies afin de s\u00e9parer ces esp\u00e8ces. Par d\u00e9faut, nos m\u00e9thodes de partitionnement utilisent la distance euclidienne. Une autre mesure de distance donnerait peut-\u00eatre de meilleurs r\u00e9sultats. Nota Bene En g\u00e9n\u00e9ral, une v\u00e9rit\u00e9 terrain n'est pas disponible dans un cas de partitionnement, ou alors pour un \u00e9chantillon restreint. Dans les cas o\u00f9 des labels sont accessibles ulterieurement, on peut utiliser les m\u00eames mesures de performances que pour la classification supervis\u00e9e.","title":"IV. Partitionnement"},{"location":"Chap4_Partitionnement/#chapitre-iv-partitionnement","text":"Ce chapitre est une introduction au partitionnement (ou \"classification non-supervis\u00e9e\") : principe, mesures de performances et m\u00e9thodes de base.","title":"Chapitre IV : Partitionnement"},{"location":"Chap4_Partitionnement/#probleme-de-partitionnement","text":"Comme mentionn\u00e9 lors du Chapitre I, par \" partitionnement \" on entend diviser des individus non-lab\u00e9lis\u00e9s en groupes suivant leur proximit\u00e9 dans l'espace des features. On essayera d'assigner des labels \u00e0 ces groupes par la suite. L'id\u00e9e est de mieux comprendre un jeu de donn\u00e9es, et d'essayer de classifier de nouvelles donn\u00e9es .","title":"Probl\u00e8me de partitionnement"},{"location":"Chap4_Partitionnement/#differents-types-de-partitionnement","text":"Suivant le probl\u00e8me \u00e0 r\u00e9soudre, il existe 2 grands types de partitionnement : par partition et hi\u00e9rarchique . On parle de partitionnement \" par partition \" lorsque l'on cherche \u00e0 diviser les individus d'une base de donn\u00e9e en un nombre fini de groupes \\(k\\) , sans tisser de lien entre eux. On a alors aucune information sur la proximit\u00e9 des classes entre elles. Par opposition, on parle de partitionnement \" hi\u00e9rarchique \" lorsque l'on va diviser les individus d'une base de donn\u00e9es en \\(k\\) groupes, hi\u00e9rarchis\u00e9s selon leur similarit\u00e9. Cette hi\u00e9rarchisation sera dite \" descendante \" ou \" ascendante \" suivant si on part de 1 classe vers \\(k\\) classes, ou inversement. On repr\u00e9sente souvent la hi\u00e9rarchisation des classes sous la forme d'un diagramme appel\u00e9 dendrogramme : un arbre repr\u00e9sentant les liens entre classes (en abscisse) et leurs distances (en ordonn\u00e9e).","title":"Diff\u00e9rents types de partitionnement"},{"location":"Chap4_Partitionnement/#la-labelisation","text":"Une fois les donn\u00e9es s\u00e9par\u00e9es en \\(k\\) groupes, l'\u00e9tape suivante est souvent d'essayer d'attribuer des labels aux classes ainsi d\u00e9termin\u00e9es. Ceci permettra de donner une interpr\u00e9tation \u00e0 notre partition, et \u00e0 nos futures pr\u00e9dictions. On appelle ce processus \" lab\u00e9lisation \". En l'absence de v\u00e9rit\u00e9 terrain pour chacun des individus, la lab\u00e9lisation est d\u00e9licate. On peut n\u00e9anmoins proposer la m\u00e9thode suivante : Essayer de caract\u00e9riser chaque groupe avec les outils de statistiques descriptives vus au Chapitre 1 (moyenne, \u00e9cart-type, etc.). Si une partition hi\u00e9rarchique a \u00e9t\u00e9 r\u00e9alis\u00e9e, \u00e9tudier aussi les liens entre les groupes . Sinon, analyser les distances entre groupes. Comparer les caract\u00e9ristiques de chaque groupe, ainsi que les liens entre groupes, aux connaissances \u00e9tablies sur le domaine d'application, ou \u00e0 un petit \u00e9chantillon de donn\u00e9es lab\u00e9lis\u00e9es si disponible. Attribuer un label \u00e0 chaque groupe en se basant sur ces \u00e9l\u00e9ments. La lab\u00e9lisation implique donc une certaine expertise dans le domaine o\u00f9 on cherche \u00e0 appliquer de la classification non-supervis\u00e9e.","title":"La lab\u00e9lisation"},{"location":"Chap4_Partitionnement/#exemple-de-probleme","text":"Comment recenser les esp\u00e8ces de chauves-souris pr\u00e9sentes sur le site de l'OVSQ ? Les chiropt\u00e9rologues (sp\u00e9cialistes des chauves-souris), utilisent souvent l'identification acoustique pour faire un relev\u00e9 des esp\u00e8ces de chauves-souris pr\u00e9sentes sur un site donn\u00e9. En effet, les ultrasons \u00e9mis par les chauves-souris sont caract\u00e9ristiques de leur esp\u00e8ce, et contrairement \u00e0 la capture, cette m\u00e9thode n'a aucun impact sur ces animaux, qui sont prot\u00e9g\u00e9s en France. Situ\u00e9 en bordure de la for\u00eat des Sources de la Bi\u00e8vre \u00e0 Guyancourt, l'Observatoire de Versailles Saint-Quentin-en-Yvelines (OVSQ) est travers\u00e9 toutes les nuits d'\u00e9t\u00e9 par des chauves-souris. C'est pourquoi depuis 2024, l'OVSQ installe un enregistreur d'ultrasons sur son site, afin de recenser les diff\u00e9rentes esp\u00e8ces pr\u00e9sentes. Voici quelques exemples de \"sonogrammes\" obtenus \u00e0 partir des enregistrements de l'OVSQ : Chaque image repr\u00e9sente un cri de chauve-souris : en abscisse le temps, en ordonn\u00e9e la fr\u00e9quence, et en nuance de gris l'amplitude. On a l'impression que ces 5 cris proviennent de 5 esp\u00e8ces diff\u00e9rentes : fr\u00e9quences moyennes diff\u00e9rentes, plage de fr\u00e9quences diff\u00e9rentes, dur\u00e9es diff\u00e9rentes, formes diff\u00e9rentes et nombre d'harmoniques diff\u00e9rentes. Pour les besoins de ce cours, ont \u00e9t\u00e9 s\u00e9lectionn\u00e9s 474 enregistrements de cris de chauves-souris provenant de l'OVSQ. Nous aimerions entrainer un mod\u00e8le \u00e0 reconnaitre les esp\u00e8ces de chauves-souris enregistr\u00e9es, mais nous n'avons pas de v\u00e9rit\u00e9 terrain pour v\u00e9rifier ses pr\u00e9dictions. Est-il tout de m\u00eame possible de diviser ces enregistrements en plusieurs classes selon leurs similarit\u00e9s, et d'identifier par la suite l'esp\u00e8ce correspondant \u00e0 chaque classe ? Dans ce but, les 2 features suivantes ont \u00e9t\u00e9 retenues pour chaque enregistrement de cri de chauve-souris : la fr\u00e9quence moyenne du fondamental (kHz) et la dur\u00e9e du cri (ms). Voici le jeu de donn\u00e9es complet, au format CSV : Chap4_bats_dataset Le tableau de donn\u00e9es qu'il contient est de la forme : freq_mean time_len 31.000 7.500 30.340 6.250 28.921 4.750 27.218 9.750 29.574 6.750 ... ... 26.605 4.750 23.630 5.750 26.000 4.500 Notre probl\u00e8me de partitionnement sera le suivant : Identifier les diff\u00e9rentes esp\u00e8ces de chauves-souris dans les enregistrements de l'OVSQ, \u00e0 partir de la fr\u00e9quence moyenne du fondamental et de la dur\u00e9e du cri . Assurons-nous d'abord qu'une telle partition est possible \u00e0 partir de ces donn\u00e9es. Une fois le fichier CSV t\u00e9l\u00e9charg\u00e9, il peut \u00eatre import\u00e9 sous Python en tant que DataFrame Pandas \u00e0 partir de son chemin d'acc\u00e8s \"input_path\" : import pandas as pd df_dataset = pd.read_csv(input_path) Il est possible avec Seaborn d'afficher ces donn\u00e9es sous la forme d'un histogramme 2D , avec une r\u00e9solution de 30 intervalles par axes : import seaborn as sns sns.histplot(data=df_dataset, x='time_len', y='freq_mean',bins=30,cbar=True) Voici le r\u00e9sultat, \u00e0 c\u00f4t\u00e9 du nuage de points correspondant : On observe que ces 2 features font apparaitre diff\u00e9rents regroupements d'enregistrements : les distributions sont clairement multimodales. Si les fronti\u00e8res entre groupes, ainsi que le nombre exact de groupes restent difficiles \u00e0 \u00e9tablir, il n'y a aucun doute sur la pr\u00e9sence de plusieurs groupes. Et ces classes sont probablement li\u00e9es \u00e0 l'esp\u00e8ce de chauves-souris. Essayer de partitionner notre base de donn\u00e9es \u00e0 partir de ces features a donc du sens. Restera alors \u00e0 lab\u00e9liser les classes ainsi d\u00e9limit\u00e9es. Cependant, on peut noter que certains groupes visibles ont l'air moins denses que d'autres. Ceci est plausible : on imagine bien que certaines esp\u00e8ces sont plus communes sur le site que d'autres. Un tel d\u00e9s\u00e9quilibre pourrait \u00eatre probl\u00e9matique pour entrainer notre mod\u00e8le. Aussi, les diff\u00e9rents groupes visibles ont l'air d'avoir des formes et des densit\u00e9s diff\u00e9rentes. On observe m\u00eame quelques outliers qui pourraient d\u00e9ranger l'entrainement. Tout ceci sera \u00e0 prendre en compte dans notre interpr\u00e9tation des r\u00e9sultats de nos mod\u00e8les. Il est \u00e0 noter que nous avons ici grandement simplifi\u00e9 le probl\u00e8me et sa r\u00e9solution pour les besoins de ce cours. Une vraie strat\u00e9gie de validation pour optimiser les hyperparam\u00e8tres et \u00e9viter le sur-apprentissage ne sera pas appliqu\u00e9e . L'id\u00e9e est que nous verrons un exemple plus en d\u00e9tails en TP.","title":"Exemple de probl\u00e8me"},{"location":"Chap4_Partitionnement/#mesures-de-performances","text":"Pour mettre au point des m\u00e9thodes d'apprentissage automatique d'une partition de donn\u00e9es, nous avons besoin de crit\u00e8res pour juger de la qualit\u00e9 de notre partition . Un des grands probl\u00e8mes en classification non-supervis\u00e9e est que le nombre de classes est une entr\u00e9e de la plupart des m\u00e9thodes de r\u00e9solution. Mais comment connaitre le nombre de classes pertinentes pour un jeu de donn\u00e9es ? Il faut tester diff\u00e9rents nombres de classes plausibles, et \u00e9valuer les performances du mod\u00e8le obtenu pour chacun. Probl\u00e8me : les donn\u00e9es auxquelles on veut appliquer une m\u00e9thode de partitionnement \u00e9tant par d\u00e9finition non-lab\u00e9lis\u00e9es, on ne peut pas calculer une erreur par rapport \u00e0 une v\u00e9rit\u00e9 terrain . Il existe n\u00e9anmoins des crit\u00e8res pour \u00e9valuer la pertinence d'un partitionnement. Nous allons voir dans cette section diff\u00e9rents crit\u00e8res pour \u00e9valuer un partitionnement , et diff\u00e9rentes m\u00e9thodes pour d\u00e9terminer un nombre de classes optimal pour un jeu de donn\u00e9es.","title":"Mesures de performances"},{"location":"Chap4_Partitionnement/#inertie-intra-classe-et-internie-inter-classe","text":"Un bon partitionnement a les 2 caract\u00e9ristiques suivantes : Les individus au sein d'un groupe sont les plus similaires possibles (leurs distances dans l'espace des features sont les plus faibles possibles). Les diff\u00e9rents groupes sont les plus diff\u00e9rents possibles (leurs distances dans l'espace des features sont les plus grandes possibles). On utilise souvent comme indicateurs de ces 2 caract\u00e9ristiques l' inertie intra-classe et inter-classe. L' inertie d'une classe \\(i\\) contenant \\(n_i\\) individus est d\u00e9finie comme la somme des distances au centre de gravit\u00e9 \\(g_i\\) de la classe : \\(I_i = \\sum_{j=1}^{n_i} d(x_{i,j},g_i)^2\\) o\u00f9 chaque \\(x_{i,j}\\) est un vecteur contenant les r\u00e9alisations des diff\u00e9rentes features pour un individu de la classe \\(i\\) . Nota Bene Il s'agit d'une analogie avec la notion de moment d'inertie en Physique : la r\u00e9partition de la masse dans un objet autour de son centre de gravit\u00e9 va rendre plus ou moins difficile sa mise en mouvement. D'une mani\u00e8re analogue, la r\u00e9partition des individus dans un groupe va rendre plus ou moins co\u00fbteuse en termes de performances un changement de centre de gravit\u00e9 des groupes (idem pour les groupes vis-\u00e0-vis du centre de gravit\u00e9 du jeu de donn\u00e9es total). Cette formule d\u00e9pend bien \u00e9videmment de la d\u00e9finition du centre de gravit\u00e9 \\(g_i\\) de la classe \\(i\\) , et de la mesure de distance \\(d\\) choisie. Pour le centre de gravit\u00e9, on va souvent consid\u00e9rer le barycentre : \\(g_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} x_{i,j}\\) Pour les mesures de distances, reportez-vous \u00e0 la section \"K plus proches voisins\" du Chapitre 2. Dans le cas o\u00f9 on se servirait des inerties pour entrainer un mod\u00e8le de partitionnement, la mesure de distance sera un hyperparam\u00e8tre \u00e0 optimiser . On d\u00e9finit alors l' inertie intra-classe comme \u00e9tant la somme des inerties des \\(k\\) classes : \\(I = \\sum_{i=1}^{k} I_i = \\sum{i=1}^{k} \\sum_{j=1}^{n_i} d(x_{i,j},g_i)^2\\) Il s'agit d'un indicateur de la similarit\u00e9 des individus au sein de chaque classe . L' inertie inter-classe est quant \u00e0 elle d\u00e9finie comme : \\(J = \\sum_{i=1}^{k} n_i d(g_i,g)^2\\) avec \\(g = \\frac{1}{\\sum_{i=1}^{k} n_i} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} x_{i,j}\\) le barycentre du jeu de donn\u00e9es complet. Il s'agit d'un indicateur de la s\u00e9parabilit\u00e9 des diff\u00e9rentes classes . Th\u00e9or\u00e8me de Huygens On note \\(T\\) l' inertie totale d'un jeu de donn\u00e9es : \\(T = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} d(x_{i,j},g)^2\\) De mani\u00e8re analogue au th\u00e9or\u00e8me de Huygens en Physique, on a conservation de l'inertie totale d'un jeu de donn\u00e9es : \\(T = I + J\\) Cette somme est ind\u00e9pendante de la partition choisie. Une bonne partition minimise l'inertie intra-classe , ce qui d'apr\u00e8s le th\u00e9or\u00e8me de Huygens revient \u00e0 maximiser l'inertie inter-classe . Les inerties peuvent \u00eatre utilis\u00e9es pour comparer les performances de 2 partitions d'un m\u00eame jeu de donn\u00e9es , \u00e0 \\(k\\) constant , mais elles ne permettent pas de comparer 2 partitions de donn\u00e9es diff\u00e9rentes, ou de \\(k\\) diff\u00e9rents. De plus, l'inertie intra-classe diminue toujours \u00e0 mesure que l'on augmente le nombre classes d'une partition de donn\u00e9es optimis\u00e9e pour chaque valeur de \\(k\\) : alors comment choisir un nombre de classe pertinent ? Nous allons voir 2 crit\u00e8res pour r\u00e9soudre ces probl\u00e8mes.","title":"Inertie intra-classe et internie inter-classe"},{"location":"Chap4_Partitionnement/#la-methode-du-coude","text":"Comme nous venons de le mentionner, l'inertie intra-classe diminue toujours \u00e0 mesure que l'on augmente le nombre de classe \\(k\\) de notre partition de donn\u00e9es (optimis\u00e9e pour chaque \\(k\\) ). Mais cette diminution a tendance \u00e0 ralentir \u00e0 mesure que \\(k\\) augmente : plus \\(k\\) est \u00e9lev\u00e9, moins on gagne en inertie intra-classe en augmentant \\(k\\) . Plus pr\u00e9cis\u00e9ment, si on trace l'inertie intra-classe en fonction de \\(k\\) , on observe typiquement une courbe en forme de coude : au d\u00e9but l'inertie diminue fortement, puis soudainement elle diminue de mani\u00e8re marginale. On va alors consid\u00e9rer que la valeur de \\(k\\) optimale est celle qui correspond \u00e0 l' angle du coude , c'est-\u00e0-dire quand augmenter \\(k\\) n'apporte plus grand chose en termes d'inertie intra-classe. C'est ce que l'on appelle assez logiquement la \" m\u00e9thode du coude \". Nota Bene La m\u00e9thode du coude n'est pas sp\u00e9cifique aux probl\u00e8mes de partitionnement. On la retrouve dans la d\u00e9termination d'un param\u00e8tre optimal pour de nombreux types de mod\u00e8les. En pratique, cette m\u00e9thode n'est pas toujours simple \u00e0 appliquer \"visuellement\". En effet, on peut parfois obtenir une courbe de l'inertie intra-classe en fonction de \\(k\\) sans coude \u00e9vident. Dans un tel cas, le choix de \\(k\\) peut s'av\u00e9rer arbitraire. Pour obtenir un r\u00e9sultat plus objectif, on peut baser sa d\u00e9cision sur un crit\u00e8re statistique . Le plus connu est le Crit\u00e8re d'Information d'Akaike (AIC) : \\(AIC(k) = 2 p - 2 log(L(k))\\) avec \\(p\\) le nombre de param\u00e8tres \u00e0 estimer, et \\(L(k)\\) la vraisemblance du mod\u00e8le pour \\(k\\) classes. L'id\u00e9e est ici que le nombre de classe optimal est celui qui minimise l'AIC : la formule cherche un mod\u00e8le vraisemblable, tout en p\u00e9nalisant le choix d'un nombre de classes trop grand. Si la m\u00e9thode de partitionnement choisie ne se base pas sur un mod\u00e8le statistique, il faudra faire des hypoth\u00e8ses statistiques afin de d\u00e9terminer une fonction de vraisemblance.","title":"La m\u00e9thode du coude"},{"location":"Chap4_Partitionnement/#coefficient-de-silhouette","text":"Nous avons vu que la m\u00e9thode du coude peux aider \u00e0 trouver le nombre de classes optimal dans certains cas, mais ce choix peut s'av\u00e9rer difficile en pratique. Et les crit\u00e8res statistiques tels que l'AIC impliquent de faire des hypoth\u00e8ses statistiques plus ou moins justes sur la m\u00e9thode de partitionnement choisie. Dans l'id\u00e9al, nous aimerions un crit\u00e8re qui permette de comparer des mod\u00e8les de partitionnement obtenus pour diff\u00e9rents jeux de donn\u00e9es, pour diff\u00e9rents nombre de classes, et sans hypoth\u00e8ses sur la m\u00e9thode choisie. C'est pourquoi le \" coefficient de silhouette \" est un des crit\u00e8res les plus utilis\u00e9s pour \u00e9valuer une partition de donn\u00e9es. Pour chaque individu de la base de donn\u00e9es, il est d\u00e9finit comme : \\(s(x_{i,j}) = \\frac{D_2(x_{i,j})-D_1(x_{i,j})}{max(D_1(x_{i,j}),D_2(x_{i,j}))}\\) Avec \\(D_1\\) la distance moyenne intra-classe : \\(D_1(x_{i,j}) = \\frac{1}{n_i-1} \\sum_{m=1,m \\neq j}^{n_i} d(x_{i,m},x_{i,j})\\) Il s'agit d'un indicateur de la similarit\u00e9 d'un individu au reste de sa classe : plus il est faible, plus l'individu est proche du reste de sa classe. Et \\(D_2\\) la distance moyenne \u00e0 la classe la plus proche : \\(D_2(x_{i,j}) = min_{1 \\leq l \\leq k, l \\neq i}(\\frac{1}{n_l} \\sum_{m=1}^{n_l} d(x_{l,m},x_{i,j}))\\) Il s'agit d'un indicateur de s\u00e9parabilit\u00e9 d'un individus par rapport \u00e0 la classe la plus proche de la sienne : plus il est \u00e9lev\u00e9, plus l'individu est s\u00e9parable des autres classes Le coefficient de silhouette est un score compris entre -1 et 1. Si pour un individu : \\(s(x_{i,j}) \\approx 1\\) alors l'individu est correctement identifi\u00e9 \u00e0 sa classe. \\(s(x_{i,j}) = 0\\) alors l'individu est \u00e0 la fronti\u00e8re entre 2 classes. \\(s(x_{i,j}) < 0\\) alors l'individu est mal identifi\u00e9 \u00e0 sa classe. On peut alors utiliser le coefficient de silhouette moyen \\(S = \\frac{1}{\\sum_{i=1}^{k} n_i} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} s(x_{i,j})\\) comme mesure de la qualit\u00e9 d'une partition de donn\u00e9es : il doit \u00eatre le plus proche possible de 1. Dans le but de choisir un nombre de classes optimal pour une partition, on peut simplement tracer la courbe de \\(S\\) obtenu pour les mod\u00e8les optimis\u00e9s par chaque \\(k\\) , et choisir la valeur de \\(k\\) maximisant \\(S\\) . Si on veut essayer de comprendre pourquoi une partition a de mauvaises performances, on peut analyser les valeurs de \\(s(x_{i,j})\\) pour chaque individu d'un jeu de donn\u00e9es. On affiche en g\u00e9n\u00e9ral les coefficients de silhouette sous la forme d'un diagramme en barres , avec en abscisses \\(s\\) et en ordonn\u00e9es les individus (rang\u00e9s par classe). On peut alors facilement identifier quels individus ont \u00e9t\u00e9 correctement associ\u00e9s \u00e0 la bonne classe ou non. Nota Bene Les coefficients de silhouette obtenus d\u00e9pendent \u00e9videmment de la mesure de distance choisie.","title":"Coefficient de silhouette"},{"location":"Chap4_Partitionnement/#methodes-de-base","text":"","title":"M\u00e9thodes de base"},{"location":"Chap4_Partitionnement/#k-moyennes","text":"","title":"K Moyennes"},{"location":"Chap4_Partitionnement/#principe","text":"La m\u00e9thode de base en partitionnement \"par partition\" est celle des K-moyennes . Il s'agit d'un algorithme it\u00e9ratif , cherchant \u00e0 r\u00e9duire \u00e0 chaque it\u00e9ration l'inertie intra-classe, \u00e0 partir d'une partition initiale al\u00e9atoire. L'id\u00e9e ici est donc d'essayer de faire converger le mod\u00e8le vers la partition minimisant l'inertie intra-classe . Le nombre de classes \\(k\\) est un param\u00e8tre d'entr\u00e9e de l'algorithme. Voici l'algorithme d\u00e9taill\u00e9 : Algorithme des K-moyennes On initialise al\u00e9atoirement \\(k\\) points \\(h_i\\) dans l'espace des features choisies, chacun correspondant \u00e0 une classe. Jusqu'\u00e0 ce qu'une crit\u00e8re d'arr\u00eat soit atteint, on va it\u00e9rer les actions suivantes : - On assigne \u00e0 chaque individu la classe \\(i\\) de point \\(h_i\\) le plus proche selon une mesure de distance. - On calcule le barycentre \\(g_i\\) de chacune des classes. - On assigne \u00e0 \\(h_i\\) le point \\(g_i\\) pour chacune des classes. Le but est de faire converger les \\(h_i\\) vers les barycentres des centres de gravit\u00e9 des classes id\u00e9ales. Il est \u00e0 noter que dans la plupart des impl\u00e9mentations des K-moyennes, la distance choisie est la distance euclidienne . N\u00e9anmoins, d'autres mesures de distance peuvent \u00eatre envisag\u00e9es. Il est \u00e9galement essentiel d'avoir en t\u00eate que lorsque l'on utilise les K-moyennes, on fait des hypoth\u00e8ses implicites : \\(k\\) est bien le nombre de classes optimal. Les diff\u00e9rentes classes sont \"sph\u00e9riques\" au sens de la distance choisie : on parle \"d' isotropie \" des classes. Chaque classe a la m\u00eame variance . Chaque classe a le m\u00eame nombre d'individus : elles sont \" \u00e9quilibr\u00e9es \". Plusieurs approches peuvent \u00eatre propos\u00e9es en cas de non-respect de ces hypoth\u00e8se : Le nombre de classes optimal peut \u00eatre estim\u00e9 avec une des m\u00e9thodes vues pr\u00e9c\u00e9demment. Si les classes sont anisotropes , c'est peut-\u00eatre parce que les features \u00e9voluent sur des ordres de grandeur diff\u00e9rents. Une normalisation des features peut alors aider. Si les classes sont d\u00e9s\u00e9quilibr\u00e9es , il est recommand\u00e9 de tester plusieurs initialisations des K-moyennes, pour \u00e9viter de rester bloqu\u00e9 dans un minimum local d'inertie intra-classe. Dans le cas o\u00f9 aucune de ces approches n'est efficace, il faut tout simplement envisager un autre mod\u00e8le de partitionnement que les K-moyennes.","title":"Principe"},{"location":"Chap4_Partitionnement/#implementation-scikit-learn","text":"Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode des K-moyennes. Elle peut \u00eatre import\u00e9e avec : from sklearn.cluster import KMeans On peut ensuite initialiser un mod\u00e8le de partition km avec un objet \"KMeans\" de param\u00e8tre k correspondant au nombre de classes \u00e0 d\u00e9terminer : km = KMeans(n_clusters=k) Pour diviser le jeu de donn\u00e9es en \\(k\\) classes clusters \u00e0 partir des features choisies features , on utilise la m\u00e9thode : clusters = km.fit_predict(features) Si on veut obtenir le coefficient de silhouette moyen de notre partition, on peut utiliser la commande : from sklearn.metrics import silhouette_score silhouette_score(features,clusters)","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap4_Partitionnement/#affichage-des-coefficients-de-silhouette","text":"Dans les biblioth\u00e8ques s\u00e9lectionn\u00e9es dans le cadre de ce cours, il n'existe pas d'impl\u00e9mentation de l'affichage des coefficients de silhouette sous la forme d'un diagramme en barres. Cependant, dans ses tutoriels en ligne, Scikit-Learn propose un code pour r\u00e9aliser ce type d'affichage \"manuellement\". En partant du principe que l'on a les features choisies dans features , et les classes obtenues par partitionnement en k dans clusters , on peut utiliser la m\u00e9thode \"silhouette_samples\" pour r\u00e9cup\u00e9rer les coefficients de silhouette, et r\u00e9aliser un affichage avec Matplotlib : from sklearn.metrics import silhouette_score,silhouette_samples import numpy as np import matplotlib.pyplot as plt sample_scores = silhouette_samples(features,clusters) mean_score = silhouette_score(features,clusters) fig, ax = plt.subplots() y_lower = 10 for idx in range(k): sample_scores_idx = sample_scores[clusters == idx] sample_scores_idx.sort() size_cluster_idx = sample_scores_idx.shape[0] y_upper = y_lower + size_cluster_idx color = plt.cm.tab10(idx) ax.fill_betweenx( np.arange(y_lower, y_upper), 0, sample_scores_idx, facecolor=color, edgecolor=color, alpha=0.7 ) ax.text(-0.05, y_lower + 0.5 * size_cluster_idx, str(idx)) y_lower = y_upper + 10 ax.axvline(x=mean_score,color=\"red\",linestyle=\"--\") ax.set_yticks([]) ax.set_xlim([-0.1, 1]) ax.set_xlabel(\"Coefficient de silhouette\",fontsize=12) ax.set_ylabel(\"Classes\",fontsize=12) Vous pouvez r\u00e9utiliser ce code tel quel pour vos propres affichages.","title":"Affichage des coefficients de silhouette"},{"location":"Chap4_Partitionnement/#application-a-notre-exemple","text":"Nous allons \u00e0 pr\u00e9sent appliquer les K-moyennes \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame, depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Afin de s'assurer que les 2 features \u00e9voluent sur des intervalles comparables, nous leur appliquons une transformation de centrage-r\u00e9duction (voir Chapitre 1) : from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_dataset) df_dataset[['freq_mean','time_len']] = pd.DataFrame(scaler.transform(df_dataset)) Nous allons dans un premier temps essayer de trouver le nombre de classes optimal pour notre partition. Utilisons le coefficient de silhouette moyen pour chaque nombre de classes entre 2 et 15, et affichons les scores obtenus : from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt silhouette = [] for k in range(2,15): km = KMeans(n_clusters=k,random_state=0) clusters = km.fit_predict(df_dataset) score = silhouette_score(df_dataset,clusters) silhouette.append(score) plt.plot(np.arange(2,15),silhouette,'ro-') plt.grid() plt.xlabel('Nombre de classes',fontsize=12) plt.ylabel('Coefficient de silhouette moyen',fontsize=12) Voici la courbe obtenue : On observe que le nombre de classes maximisant le coefficient de silhouette moyen est \\(k=3\\) . Nous choisirons donc ce param\u00e8tre pour la suite. La valeur du coefficient moyen obtenu sera de 0,76 environ, ce qui est consid\u00e9r\u00e9 comme un bon score pour des donn\u00e9es r\u00e9elles. Nota Bene Il est \u00e0 noter que nous avons mis le param\u00e8tre \"random_state\" \u00e0 0 car le mod\u00e8le renvoy\u00e9 par les K-moyennes d\u00e9pend de l'initialisation. Nous nous assurons ainsi que 2 executions de ce code Python donnerons le m\u00eame r\u00e9sultat. Il faudrait en toute rigueur v\u00e9rifier que d'autres initialisation donnent le m\u00eame \\(k\\) optimal. Nous pouvons ajouter les 3 classes identifi\u00e9es au DataFrame d'entr\u00e9e, puis inverser le centrage-r\u00e9duction des features : km = KMeans(n_clusters=3,random_state=0) clusters = km.fit_predict(df_dataset) df_dataset_clustured = df_dataset.copy() df_dataset_clustured['clusters'] = clusters df_dataset_clustured[['freq_mean','time_len']] = pd.DataFrame(scaler.inverse_transform(df_dataset_clustured[['freq_mean','time_len']])) Et afficher sous la forme d'un nuages de points la partition obtenue, en utilisant Seaborn : import seaborn as sns sns.scatterplot(data=df_dataset_clustured,x='time_len',y='freq_mean',hue='clusters',palette='tab10') Voici le graphique obtenu : Les classes identifi\u00e9es ont l'air coh\u00e9rentes avec les pics que nous avions observ\u00e9s dans les donn\u00e9es en d\u00e9but de chapitre. Pour v\u00e9rifier la qualit\u00e9 de cette partition, nous proposons d'afficher le coefficient de silhouette pour chaque individu, sous la forme d'un diagramme en barres . Voici le graphique obtenu avec le code donn\u00e9 pr\u00e9c\u00e9demment : Tout d'abord, on observe qu'aucun individu n'a un coefficient de silhouette n\u00e9gatif, ce qui est le signe d'un plut\u00f4t bon partitionnement. Ensuite, on voit que pour le classe 0, la quasi-int\u00e9gralit\u00e9 des individus a un coefficient sup\u00e9rieur \u00e0 0,5. Ceci est coh\u00e9rent avec la matrice de corr\u00e9lation que nous avons affich\u00e9e pr\u00e9c\u00e9demment : la classe 0 a l'air d'\u00eatre la mieux s\u00e9par\u00e9e des 3. Les classes 1 et 2 ont quelques individus avec des coefficients entre 0 et 0,5. Ces individus sont donc proches de la fronti\u00e8re avec la classe la plus proche. Ce qui une fois de plus colle au r\u00e9sultat pr\u00e9c\u00e9dent : les classes 1 et 2 ont l'air plus difficilement s\u00e9parables avec les features retenues. Il est \u00e0 noter que, comme beaucoup de cas pratiques, notre probl\u00e8me ne partitionnement ne respecte pas exactement les hypoth\u00e8ses implicites des K-moyennes : Comme nous l'avions d\u00e9j\u00e0 mentionn\u00e9, nos classes sont d\u00e9s\u00e9quilibr\u00e9es. Nos classes ne sont pas isotropes. La variance de nos classes n'est clairement pas la m\u00eame. Il y a des outliers dans nos donn\u00e9es. De meilleurs r\u00e9sultats pourraient donc potentiellement \u00eatre obtenus avec plus d'observations pour \u00e9quilibrer les classes, en supprimant les outliers, ou avec une m\u00e9thode aux hypoth\u00e8ses diff\u00e9rentes.","title":"Application \u00e0 notre exemple"},{"location":"Chap4_Partitionnement/#remarques","text":"La m\u00e9thode des K-moyennes a les avantages suivants : Elle est simple \u00e0 impl\u00e9menter, et son r\u00e9sultat est tout \u00e0 fait interpr\u00e9table par un humain : on cherche \u00e0 obtenir les centres de gravit\u00e9 des classes. Elle converge assez rapidement , et on peut donc l'utiliser sur de grands jeux de donn\u00e9es . Mais cette m\u00e9thode a aussi les limites suivantes : Les performances de la m\u00e9thode sont mauvaises si les hypoth\u00e8ses implicites sur les donn\u00e9es ne sont pas respect\u00e9es. L'initialisation \u00e9tant al\u00e9atoire , 2 executions de l'algorithme ne donneront pas exactement la m\u00eame partition . Il est m\u00eame possible que la m\u00e9thode tombe dans un minimum local de l'inertie intra-classe. La distance euclidienne \u00e9tant sensible aux outliers , la m\u00e9thode des K-moyennes l'est aussi. Les K-moyennes n'\u00e9tant pas une m\u00e9thode de partitionnement hi\u00e9rarchique, elle ne trace aucun lien entre les classes qu'elle d\u00e9termine.","title":"Remarques"},{"location":"Chap4_Partitionnement/#classification-ascendante-hierarchique","text":"","title":"Classification Ascendante Hi\u00e9rarchique"},{"location":"Chap4_Partitionnement/#principe_1","text":"Comme son nom l'indique, la m\u00e9thode de la Classification Ascendante Hi\u00e9rarchique (CAH) est une m\u00e9thode de partitionnement hi\u00e9rarchique , qui agr\u00e8ge les classes de mani\u00e8re ascendante . Il s'agit d'une m\u00e9thode it\u00e9rative , fusionnant 2 classes \u00e0 chaque it\u00e9ration, ce qui correspond \u00e0 fusionner 2 branches d'un dendrogramme. Elle s'initialise en consid\u00e9rant chaque individu comme unique repr\u00e9sentant de sa propre classe : on a autant de classes que d'individu, l'inertie intra-classe est nulle . Elle se termine une fois que toutes les classes ont \u00e9t\u00e9 fusionn\u00e9es : on a une unique classe, l'inertie inter-classe est nulle . On peut d\u00e9couper le dendrogramme obtenu au niveau du nombre de classe \\(k\\) d\u00e9sir\u00e9, ou alors arr\u00eater les it\u00e9rations de la m\u00e9thode pour ce \\(k\\) si nous ne sommes pas int\u00e9ress\u00e9s par le dendrogramme. Voici l'algorithme d\u00e9taill\u00e9 : Algorithme de la CAH On initialise \\(n\\) classes : une par individu dans le jeu de donn\u00e9es. Jusqu'\u00e0 ce que l'on atteigne un nombre de classe \u00e9gal \u00e0 1 (ou \u00e9gal au nombre de classes \\(k\\) d\u00e9sir\u00e9), on va it\u00e9rer les actions suivantes : - On mesure la similarit\u00e9 entre les diff\u00e9rentes classes. - Les 2 classes les plus similaires sont fusionn\u00e9es. On enregistre les classes obtenues \u00e0 chaque it\u00e9ration pour pouvoir tracer un dendrogramme. Cette m\u00e9thode implique de d\u00e9cider d'un crit\u00e8re de similarit\u00e9 entre 2 classes. Il s'agira d'un hyperparam\u00e8tre \u00e0 choisir. Voici les 4 principaux crit\u00e8res utilisables par la CAH pour mesurer la similarit\u00e9 entre 2 classes : Lien simple : la distance minimale entre 2 individus issus de ces 2 classes. Lien complet : la distance maximale entre 2 individus issus de ces 2 classes. Lien moyen : la moyenne des distances entre tous les couples d'individus issus des 2 classes possibles. Crit\u00e8re de Ward : l'augmentation de l'inertie intra-classe quand les 2 classes sont fusionn\u00e9es. Derri\u00e8re ces crit\u00e8res, se cache donc le choix d'une mesure de distance entre individus. Certains crit\u00e8res sont plus rapides \u00e0 calculer que d'autres, mais lorsque l'on choisis un crit\u00e8re plut\u00f4t qu'un autre, on fait un choix sur notre vision de la \"proximit\u00e9\" entre classes : Lien simple : on voit la notion de similarit\u00e9 \u00e0 l' \u00e9chelle des individus . Ce crit\u00e8re est adapt\u00e9 aux cas de classes tr\u00e8s anisotropes, mais aura tendance \u00e0 relier 2 classes si un outlier se trouve entre elles. On la consid\u00e8rera donc comme peu conservatrice . Lien complet : on voit la notion de similarit\u00e9 \u00e0 l' \u00e9chelle de la classe enti\u00e8re . Ce crit\u00e8re est adapt\u00e9 aux classes fortement s\u00e9par\u00e9es, mais aura aussi tendance \u00e0 \u00eatre sensible aux outliers, mais dans l'exc\u00e8s inverse : la m\u00e9thode sera tr\u00e8s conservatrice , et un outlier pourra \u00e0 lui tout seul emp\u00eacher de lier 2 classes pourtant proches. Lien moyen : on voit la notion de similarit\u00e9 du point de vue de la moyenne des distances des individus entre classes. On peut donc voir ce crit\u00e8re comme un compromis entre les 2 pr\u00e9c\u00e9dents . Il aura tendance \u00e0 favoriser des classes \"sph\u00e9riques\" au sens de la distance choisie. Crit\u00e8re de Ward : il s'agit du crit\u00e8re pour lequel les hypoth\u00e8ses sont les plus fortes . Elles sont similaires \u00e0 celles des K-moyennes : classes isotropes, de m\u00eame variance, et \u00e9quilibr\u00e9es. En pratique, on va souvent choisir par d\u00e9faut le crit\u00e8re de Ward .","title":"Principe"},{"location":"Chap4_Partitionnement/#implementation-scikit-learn_1","text":"Il existe une impl\u00e9mentation Scikit-Learn de la m\u00e9thode de la CAH. Elle peut \u00eatre import\u00e9e avec : from sklearn.cluster import AgglomerativeClustering On peut ensuite initialiser un mod\u00e8le de partition hca avec un objet \"AgglomerativeClustering\" de param\u00e8tre k correspondant au nombre de classes \u00e0 d\u00e9terminer : hca = AgglomerativeClustering(n_clusters=k) Par d\u00e9faut, l'impl\u00e9mentation Scikit-Learn utilise le crit\u00e8re de Ward comme mesure de similarit\u00e9. Il est possible de changer cet hyperparam\u00e8tre avec le param\u00e8tre \"linkage\" de l'objet \"AgglomerativeClustering\" : 'ward', 'complete', 'average' ou 'single'. Pour diviser le jeu de donn\u00e9es en \\(k\\) classes clusters \u00e0 partir des features choisies features , on utilise la m\u00e9thode : clusters = hca.fit_predict(features) Comme pour les K-moyennes, si on veut obtenir le coefficient de silhouette moyen de notre partition, on peut utiliser la commande : from sklearn.metrics import silhouette_score silhouette_score(features,clusters)","title":"Impl\u00e9mentation Scikit-Learn"},{"location":"Chap4_Partitionnement/#affichage-dun-dendrogramme-avec-scipy","text":"Dans les biblioth\u00e8ques s\u00e9lectionn\u00e9es dans le cadre de ce cours, il n'existe pas d'impl\u00e9mentation pour afficher le dendrogramme d'une CAH. Cependant, dans ses tutoriels en ligne, Scikit-Learn propose une fonction pour r\u00e9aliser ce type d'affichage \"manuellement\", en s'appuyant sur la fonction \"dendrogramme\" de la biblioth\u00e8que Scipy. La voici, pour un mod\u00e8le d\u00e9termin\u00e9 par CAH model : from scipy.cluster.hierarchy import dendrogram import numpy as np def plot_dendrogram(model, **kwargs): counts = np.zeros(model.children_.shape[0]) n_samples = len(model.labels_) for i, merge in enumerate(model.children_): current_count = 0 for child_idx in merge: if child_idx < n_samples: current_count += 1 else: current_count += counts[child_idx - n_samples] counts[i] = current_count linkage_matrix = np.column_stack([ model.children_, model.distances_, counts ]).astype(float) dendrogram(linkage_matrix, **kwargs) Vous pouvez r\u00e9utiliser ce code tel quel pour vos propres affichages. Pour pouvoir utiliser cette fonction, il faut cr\u00e9er une partition des features choisies features , avec les param\u00e8tres suivants : hca = AgglomerativeClustering(distance_threshold=0,n_clusters=None) hca.fit(features) Avec d'autres param\u00e8tres, il ne sera pas possible de r\u00e9cup\u00e9rer le dendrogramme associ\u00e9 au mod\u00e8le. Pour afficher le dendrogramme complet (de 1 classe par individu \u00e0 1 classe unique) : import maplotlib.pyplot as plt plt.figure() plot_dendrogram(hca,truncate_mode=\"level\") plt.xlabel(\"Nombre d'individus par classe\",fontsize=12) plt.ylabel(\"Distance entre classes\",fontsize=12) plt.xticks([]) plt.show() Mais en g\u00e9n\u00e9ral, on va pr\u00e9f\u00e9rer afficher le dendrogramme jusqu'au nombre de classes voulu k : import matplotlib.pyplot as plt plt.figure() plot_dendrogram(hca,truncate_mode=\"lastp\",p=k) plt.xlabel(\"Nombre d'individus par classe\",fontsize=12) plt.ylabel(\"Distance entre classes\",fontsize=12) plt.show()","title":"Affichage d'un dendrogramme avec Scipy"},{"location":"Chap4_Partitionnement/#application-a-notre-exemple_1","text":"Nous allons \u00e0 pr\u00e9sent appliquer la CAH \u00e0 notre probl\u00e8me exemple. Tout d'abord, nous importons notre fichier CSV sous la forme d'un DataFrame depuis le chemin input_path : df_dataset = pd.read_csv(input_path) Afin de s'assurer que les 2 features \u00e9voluent sur des intervalles comparables, nous leur appliquons une transformation de centrage-r\u00e9duction (voir Chapitre 1) : from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(df_dataset) df_dataset[['freq_mean','time_len']] = pd.DataFrame(scaler.transform(df_dataset)) Comme pour les K-moyennes, nous allons dans un premier temps essayer de trouver le nombre de classes optimal pour notre partition. Notre exemple \u00e9tant relativement simple, on s'attend \u00e0 ce que le r\u00e9sultat soit tr\u00e8s similaire. Nous utilisons une fois encore le coefficient de silhouette moyen pour chaque nombre de classes entre 2 et 15, et nous affichons les scores obtenus : from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt silhouette = [] for k in range(2,15): hca = AgglomerativeClustering(n_clusters=k) clusters = hca.fit_predict(df_dataset) score = silhouette_score(df_dataset,clusters) silhouette.append(score) plt.plot(np.arange(2,15),silhouette,'ro-') plt.grid() plt.xlabel('Nombre de classes',fontsize=12) plt.ylabel('Coefficient de silhouette moyen',fontsize=12) La courbe obtenue est en effet tr\u00e8s similaire \u00e0 celle obtenue avec les K-moyennes : C'est donc sans surprise que nous choisissons \u00e0 nouveau \\(k=3\\) . Le coefficient de silhouette moyen sera alors de 0,76. Ce score est environ le m\u00eame que pour les K-moyenne : de mani\u00e8re g\u00e9n\u00e9rale les 2 partitions sont aussi bonnes aux yeux du coefficient de silhouette. Nous pouvons ajouter les 3 classes identifi\u00e9es au DataFrame d'entr\u00e9e, puis inverser le centrage-r\u00e9duction des features : hca = AgglomerativeClustering(n_clusters=3) clusters = hca.fit_predict(df_dataset) df_dataset_clustured = df_dataset.copy() df_dataset_clustured['clusters'] = clusters df_dataset_clustured[['freq_mean','time_len']] = pd.DataFrame(scaler.inverse_transform(df_dataset_clustured[['freq_mean','time_len']])) Et comme pour les K-moyennes, nous pouvons afficher sous la forme d'un nuage de points la partition obtenue, en utilisant Seaborn : import seaborn as sns sns.scatterplot(data=df_dataset_clustured,x='time_len',y='freq_mean',hue='clusters',palette='tab10') Voici le graphique obtenu : Il est similaire \u00e0 celui obtenu par les K-moyennes, \u00e0 l'exception de quelques individus entre les classe (Attention, les classes n'ont pas re\u00e7u le m\u00eame num\u00e9ro). Le coefficient de silhouette de chaque individu peut aussi \u00eatre affich\u00e9 sous la forme d'un diagramme en barres , et sans surprise il est assez similaire \u00e0 celui obtenu par les K-moyennes : On remarque tout de m\u00eame que 2 individus sont attribu\u00e9s \u00e0 tort \u00e0 la classe 0 selon le coefficient de silhouette. Il s'agit de d'individus situ\u00e9s entre la classe 0 et la classe 1. Nota Bene Les K-moyennes et la CAH ne donneront pas toujours des partitions aussi similaires pour un m\u00eame jeu de donn\u00e9es. Nous sommes ici dans un cas particulier. La CAH \u00e9tant une m\u00e9thode de partitionnement hi\u00e9rarchique, elle permet de tisser des liens entre les diff\u00e9rentes classes. Nous pouvons tracer ces liens sous la forme d'un dendrogramme , en utilisant la fonction pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Voici le dendrogramme total, ainsi que le dendrogramme tronqu\u00e9 pour 3 classes : Selon ce dendrogramme, les classes 1 et 2 (104 et 210 individus) sont plus proches entre elles que de la classe 0 (160 individus). Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9d\u00e9mment, l'impl\u00e9mentation Scikit-Learn de la CAH utilise par d\u00e9faut le crit\u00e8re de Ward comme mesure de similarit\u00e9 entre classes. C'est souvent le compromis choisi pour la CAH. Selon la d\u00e9finition de la \"similarit\u00e9\" du crit\u00e8re de Ward, cela signifie que fusionner les classes 1 et 2 fait moins monter l'inertie intra-classe que fusionner les classe 0 et 1 ou 0 et 2. Il est \u00e9vident que si nous d\u00e9finissons autrement la \"similarit\u00e9\" entre classes, le dendrogramme que nous obtiendrons sera diff\u00e9rent : 2 groupes peuvent \u00eatre plus proche selon une mesure de similarit\u00e9 qu'une autre. Voici les dendrogrammes obtenus pour les 4 mesures de similarit\u00e9 impl\u00e9ment\u00e9es dans Scikit-Learn : On remarque directement que les crit\u00e8res de similarit\u00e9 du \"lien simple\" et du \"lien moyen\" donnent de mauvais r\u00e9sultats : une des classes n'a qu'un seul individu... Pour les autres crit\u00e8res, difficile de dire quelle partition est la meilleure \u00e0 partir des dendrogrammes seuls. On peut v\u00e9rifier les partitions obtenues avec des nuages de points selon les diff\u00e9rentes features. Voici un exemple pour la fr\u00e9quence moyenne du fondamental et la dur\u00e9e du cri : On comprend alors l'origine de la sous-performance des liens \"simple\" et \"moyen\": un outlier. Dans les 2 cas, la CAH a isol\u00e9 cet outlier dans une classe dont il est l'unique individu. Ceci peut se comprendre intuitivement : un outlier est par d\u00e9finition loin de tous les autres individus, ainsi que de la moyenne des individus, et ceci pour toutes les classes. Nous avons ici une illustration de la sensibilit\u00e9 aux outliers de ces 2 types de liens. Faire le m\u00e9nage dans notre base de donn\u00e9e en retirant les outliers pourrait donc grandement am\u00e9liorer les performances de la CAH dans ces 2 cas. On peut noter que l'outlier est plac\u00e9 comme \u00e9loign\u00e9 des 2 autre classes dans le dendrogramme pour le lien \"simple\", et comme proche de la classe 1. Encore une fois, ceci ce comprend facilement : du point de vue de la distance minimale entre individus, l'outlier est plus \u00e9loign\u00e9 des 2 autres classes, alors que du point de vue de la moyenne il est visiblement plus proche de la classe 1. D'apr\u00e8s les dendrogrammes obtenus, le lien \"complet\" et le crit\u00e8re de Ward sont en accord sur les liens entre classes. On observe aussi que les 2 types de liens d\u00e9limitent de la m\u00eame fa\u00e7on la classe 2. Par contre ils ne fixent clairement pas la m\u00eame fronti\u00e8re entre les classes 0 et 1 : le lien \"complet\" attribut plus d'individus \u00e0 la classe 1 que le crit\u00e8re de Ward. Ce r\u00e9sultat n'est pas surprenant : le lien \"complet\" se basant sur la distance maximale entre individus de 2 classes, l'outlier va avoir tendance \u00e0 \"tirer\" la fronti\u00e8re entre les classes 0 et 1 de sont c\u00f4t\u00e9. Ce n'est pas le cas pour le crit\u00e8re de Ward, qui ne regardera que l'inertie intra-classe. Nous avons ici une illustration de la sensibilit\u00e9 aux outlier du lien \"complet\". On aurait tendance \u00e0 choisir la partition obtenue avec le crit\u00e8re de Ward. Pour faire un choix d\u00e9finitif, on peut \u00e9valuer le coefficient de silhouette moyen de la partition obtenue pour chaque crit\u00e8re de similarit\u00e9 : Crit\u00e8re de similarit\u00e9 Coefficient de silhouette moyen Simple 0.55 Complet 0.72 Moyen 0.63 Ward 0.76 Selon le coefficient de silhouette moyen, sans surprise la meilleure partition est celle obtenue avec le crit\u00e8re de Ward. Comme nous l'avons mentionn\u00e9 pr\u00e9c\u00e9demment, le crit\u00e8re de Ward est souvent le choix par d\u00e9faut. Cependant, suivant le probl\u00e8me auquel on est confront\u00e9, une autre mesure de similarit\u00e9 peut \u00eatre plus pertinente. Dans le cas de notre exemple, nettoyer notre base de donn\u00e9es des outliers permettrait probablement d'am\u00e9liorer les performances obtenues pour tous les types de liens. Comme pour les K-moyennes, les performances que nous obtenons ici avec le crit\u00e8re de Ward sont limit\u00e9es, car celui-ci implique des hypoth\u00e8ses fortes, qui ne sont pas respect\u00e9es ici (isotropie, variance constante, classes \u00e9quilibr\u00e9es).","title":"Application \u00e0 notre exemple"},{"location":"Chap4_Partitionnement/#remarques_1","text":"La m\u00e9thode de la CAH a les avantages suivants : Elle permet de tisser des liens entre les classes d\u00e9termin\u00e9es, ce qui rend le mod\u00e8le tr\u00e8s interpr\u00e9table . On peut d\u00e9cider du nombre optimal de classes \\(k\\) a posterio , en coupant le dendrogramme. Pour chaque \\(k\\) , le mod\u00e8le renvoy\u00e9 par le CAH est d\u00e9terministe : il n'y a pas ici d'initialisation al\u00e9atoire comme pour les K-moyennes. Il est possible d' adapter le crit\u00e8re de similarit\u00e9 selon notre probl\u00e8me. Mais cette m\u00e9thode a aussi les limites suivantes : Elle est relativement gourmande en temps de calcul , ce qui la rend difficile \u00e0 appliquer \u00e0 des grands jeux de donn\u00e9es. Suivant le crit\u00e8re de similarit\u00e9 choisie, elle peut \u00eatre plus ou moins sensible aux outliers .","title":"Remarques"},{"location":"Chap4_Partitionnement/#labelisation-de-lexemple","text":"","title":"Lab\u00e9lisation de l'exemple"},{"location":"Chap4_Partitionnement/#elements-pour-la-labelisation","text":"Nous allons \u00e0 pr\u00e9sent essayer de lab\u00e9liser les 3 classes obtenues avec les k-moyennes (sachant que les classes obtenues par CAH sont identiques). Commen\u00e7ons par caract\u00e9riser avec les statistiques descriptives les diff\u00e9rents groupes. Voici les moyennes des classes identifi\u00e9es selon les diff\u00e9rentes features : Classe 0 Classe 1 Classe 2 freq_mean 48.85 19.75 26.07 time_len 4.09 22.08 5.87 Pour attribuer une esp\u00e8ce \u00e0 chaque groupe, nous allons nous appuyer sur la cl\u00e9 d'identification acoustique du chercheur du Museum d'Histoire Naturelle Yves Bas. Commen\u00e7ons par la classe 0. Nous voyons que la dur\u00e9e des cris est assez courte, de 4.09 ms, et la fr\u00e9quence moyenne du fondamental est plut\u00f4t \u00e9lev\u00e9e, de 48.85 kHz. Voici la table d'identification pour ce type de cris, \u00e0 fr\u00e9quence \u00e9lev\u00e9e : Esp\u00e8ce Fr\u00e9quences (kHz) Pipistrelle de Kuhl 34-40 Pipistrelle de Nathusius 38-42 Pipistrelle commune 43-50 Pipistrelle pygm\u00e9e 53-60 Il est alors \u00e9vident que la seule esp\u00e8ce plausible est la Pipistrelle commune Ensuite, nous voyons que la classe 1 a des longs cris de 22.08 ms. La fr\u00e9quence moyenne du cri est de 19.75 kHz, ce qui est plut\u00f4t faible (\u00e0 la limite de l'audition d'un humain). Voici la table d'identification pour des cris de longue dur\u00e9e, \u00e0 fr\u00e9quence basse : Esp\u00e8ce Fr\u00e9quences (kHz) Molosse de Cestoni 9-12 Grande Noctule 13-15 Noctule commune 17-20 Noctule de Leisler 22-26 Il est alors \u00e9vident que la seule esp\u00e8ce plausible est la Noctule commune . Enfin, nous voyons que la classe 2 a une fr\u00e9quence moyenne interm\u00e9diaire de 26.07 kHz, avec une dur\u00e9e de 5.87 ms. Ce type de cris est g\u00e9n\u00e9ralement associ\u00e9 \u00e0 des chauves-souris de la famille des S\u00e9rotines ou des Oreillards . D'apr\u00e8s le document de Yves Bas, il est tr\u00e8s difficile de diff\u00e9rencier les cris des diff\u00e9rentes esp\u00e8ces d'Oreillards. Et sans informations sur la forme des cris, il nous est impossible d'attribuer avec certitude ces cris aux Oreillards ou aux S\u00e9rotines. Nous ne pouvons donc pas aller plus loin dans la lab\u00e9lisation de cette classe. D'o\u00f9 les labels que nous avons choisis pour les 3 classes : Noctule commune, Pipistrelle commune et Oreillards / S\u00e9rotine. On trouve facilement dans la litt\u00e9rature sp\u00e9cialis\u00e9e que ces 3 esp\u00e8ces sont cr\u00e9dibles pour une lisi\u00e8re de for\u00eat en Ile-de-France. De plus, cette lab\u00e9lisation est coh\u00e9rente avec les lien trac\u00e9s entre les classes par la CAH : les cris de noctules \u00e9tant beaucoup plus longs en moyenne que ceux des pipistrelles communes et des oreillards / s\u00e9rotines, on comprend pourquoi ils seraient consid\u00e9r\u00e9s comme \"moins similaires\". Comme nous l'avions anticip\u00e9, la lab\u00e9lisation a n\u00e9cessit\u00e9 ici des recherches bibliographiques sur l'identification acoustique des chauves-souris fran\u00e7aise. De mani\u00e8re g\u00e9n\u00e9rale, une lab\u00e9lisation correcte n\u00e9cessite souvent une expertise dans le domaine d'\u00e9tude.","title":"El\u00e9ments pour la lab\u00e9lisation"},{"location":"Chap4_Partitionnement/#verite-terrain","text":"En 2006, le Mus\u00e9um d'Histoire Naturelle lance le programme de sciences participatives \"Vigie-Chiro\". L'id\u00e9e est de collecter des enregistrements de chauves-souris r\u00e9alis\u00e9s par des amateurs dans toute la France, afin de r\u00e9aliser un suivi des diff\u00e9rentes esp\u00e8ces d'une ann\u00e9e sur l'autre, par r\u00e9gion. Dans le cadre de ce programme, l'OVSQ partage tous les ans l'int\u00e9gralit\u00e9 de ses enregistrements avec le Mus\u00e9um. Sur sa plateforme en ligne, \"Vigie-Chiro\" propose un logiciel d'identification automatique des enregistrements de chauves-souris : Tadarida (Bas et al., 2017). Nous avons lab\u00e9lis\u00e9 nos 474 enregistrements \u00e0 l'aide de Tadarida, et 5 esp\u00e8ces ont \u00e9t\u00e9 identifi\u00e9es : La s\u00e9rotine commune ( Eptesicus serotinus ), de label : Eptser . Le murin de Daubenton ( Myotis daubentonii ), de label : Myodau . La noctule commune ( Nyctalus noctula ), de label : Nycnoc . La pipistrelle commune ( Pipistrellus pipistrellus ), de label : Pippip . L'Oreillard roux ( Plecotus auritus ), de label : Pleaur . Voici l'identification Tadarida des diff\u00e9rents enregistrements, sous la forme d'un nuage de points : Et pour informations, les 5 exemples de sonogrammes pr\u00e9sent\u00e9s en d\u00e9but de chapitre correspondent chacun \u00e0 une esp\u00e8ce diff\u00e9rente parmi celles identifi\u00e9es par Tadarida : On remarque que les noctules communes, les pipistrelles communes, ainsi que les oreillard roux ont \u00e9t\u00e9 assez bien identifi\u00e9es par nos m\u00e9thodes de partitions. Par contre, la s\u00e9rotine commune et le murin de Daubenton n'ont pas \u00e9t\u00e9 clairement identifi\u00e9es. On peut avancer plusieurs explications : La s\u00e9rotine commune et le murin de Daubenton sont sous repr\u00e9sent\u00e9s dans les observations (26 et 41, contre 159 pour la noctule commune, 169 pour la pipistrelle commune, et 79 pour l'oreillard roux). La s\u00e9rotine commune a l'air difficilement s\u00e9parable de l'oreillard roux, et le murin de Daubenton difficilement s\u00e9parable de la pipistrelle commune. Il faudrait donc potentiellement ajouter de nouvelles features, ou appliquer une transformation aux features choisies afin de s\u00e9parer ces esp\u00e8ces. Par d\u00e9faut, nos m\u00e9thodes de partitionnement utilisent la distance euclidienne. Une autre mesure de distance donnerait peut-\u00eatre de meilleurs r\u00e9sultats. Nota Bene En g\u00e9n\u00e9ral, une v\u00e9rit\u00e9 terrain n'est pas disponible dans un cas de partitionnement, ou alors pour un \u00e9chantillon restreint. Dans les cas o\u00f9 des labels sont accessibles ulterieurement, on peut utiliser les m\u00eames mesures de performances que pour la classification supervis\u00e9e.","title":"V\u00e9rit\u00e9 terrain"}]}
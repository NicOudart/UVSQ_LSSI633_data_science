{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LSSI633 : Science des donn\u00e9es Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ) Pr\u00e9sentation du cours La science des donn\u00e9es ou \"data science\" est une discipline au croisement entre les math\u00e9matiques (surtout les statistiques ) et la programmation informatique . Si les contours de cette discipline sont encore mal d\u00e9finis, elle est en g\u00e9n\u00e9ral associ\u00e9e \u00e0 l' explosion des volumes de donn\u00e9es \u00e0 traiter (on parle aussi de \"big data\"), et \u00e0 la mise en place d'outils d' apprentissage automatique (ou \"Machine Learning\"). L'id\u00e9e est la suivante : manipuler des quantit\u00e9s massives de donn\u00e9es est devenu presque impossible pour un humain, et requiert donc des outils informatiques capables d' apprendre des donn\u00e9es de mani\u00e8re automatique. Vous trouverez dans ce cours une initiation aux sciences des donn\u00e9es, et plus particuli\u00e8rement aux 3 grandes m\u00e9thodes d'apprentissage automatique : Chapitre I : Introduction : une introduction aux concepts et enjeux de la science des donn\u00e9es. Chapitre II : Classification supervis\u00e9e : une initiation \u00e0 la classification par apprentissage supervis\u00e9. Chapitre III : R\u00e9gression : une initiation \u00e0 la r\u00e9gression par apprentissage supervis\u00e9. Chapitre IV : Clustering : une initiation au partitionnement, aussi connu sous le nom de classification par apprentissage non-supervis\u00e9. L'objectif est qu'\u00e0 la fin de ce cours vous soyez capable de : Prendre du recul sur un jeu de donn\u00e9es, et savoir l' analyser / le pr\u00e9parer en vue d'un apprentissage. Identifier \u00e0 quel type de probl\u00e8me (classification, regression, clustering) vous \u00eates confront\u00e9s. R\u00e9fl\u00e9chir aux enjeux li\u00e9s \u00e0 chacun de ces types de probl\u00e8mes, et savoir choisir une des m\u00e9thodes pertinente parmi celles vue en cours. Evaluer les performances de votre mod\u00e8le, et les interpr\u00e9ter. Ce cours vous donnera aussi des astuces d'impl\u00e9mentation en Python , bas\u00e9es sur la biblioth\u00e8que Scikit-Learn . Tous les exemples de ce cours sont imaginaires, et les donn\u00e9es synth\u00e9tiques. Leur int\u00e9r\u00eat est purement p\u00e9dagogique. Credits \u00a9 Nicolas OUDART Remerciements \u00e0 C\u00e9cile MALLET","title":"Accueil"},{"location":"#lssi633-science-des-donnees","text":"Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ)","title":"LSSI633 : Science des donn\u00e9es"},{"location":"#presentation-du-cours","text":"La science des donn\u00e9es ou \"data science\" est une discipline au croisement entre les math\u00e9matiques (surtout les statistiques ) et la programmation informatique . Si les contours de cette discipline sont encore mal d\u00e9finis, elle est en g\u00e9n\u00e9ral associ\u00e9e \u00e0 l' explosion des volumes de donn\u00e9es \u00e0 traiter (on parle aussi de \"big data\"), et \u00e0 la mise en place d'outils d' apprentissage automatique (ou \"Machine Learning\"). L'id\u00e9e est la suivante : manipuler des quantit\u00e9s massives de donn\u00e9es est devenu presque impossible pour un humain, et requiert donc des outils informatiques capables d' apprendre des donn\u00e9es de mani\u00e8re automatique. Vous trouverez dans ce cours une initiation aux sciences des donn\u00e9es, et plus particuli\u00e8rement aux 3 grandes m\u00e9thodes d'apprentissage automatique : Chapitre I : Introduction : une introduction aux concepts et enjeux de la science des donn\u00e9es. Chapitre II : Classification supervis\u00e9e : une initiation \u00e0 la classification par apprentissage supervis\u00e9. Chapitre III : R\u00e9gression : une initiation \u00e0 la r\u00e9gression par apprentissage supervis\u00e9. Chapitre IV : Clustering : une initiation au partitionnement, aussi connu sous le nom de classification par apprentissage non-supervis\u00e9. L'objectif est qu'\u00e0 la fin de ce cours vous soyez capable de : Prendre du recul sur un jeu de donn\u00e9es, et savoir l' analyser / le pr\u00e9parer en vue d'un apprentissage. Identifier \u00e0 quel type de probl\u00e8me (classification, regression, clustering) vous \u00eates confront\u00e9s. R\u00e9fl\u00e9chir aux enjeux li\u00e9s \u00e0 chacun de ces types de probl\u00e8mes, et savoir choisir une des m\u00e9thodes pertinente parmi celles vue en cours. Evaluer les performances de votre mod\u00e8le, et les interpr\u00e9ter. Ce cours vous donnera aussi des astuces d'impl\u00e9mentation en Python , bas\u00e9es sur la biblioth\u00e8que Scikit-Learn . Tous les exemples de ce cours sont imaginaires, et les donn\u00e9es synth\u00e9tiques. Leur int\u00e9r\u00eat est purement p\u00e9dagogique.","title":"Pr\u00e9sentation du cours"},{"location":"#credits","text":"\u00a9 Nicolas OUDART Remerciements \u00e0 C\u00e9cile MALLET","title":"Credits"},{"location":"Chap1_Introduction/","text":"Chapitre I : Introduction aux sciences des donn\u00e9es Ce chapitre porte sur les grands concepts et les enjeux des sciences des donn\u00e9es. Analyse de donn\u00e9es L'explosion des capacit\u00e9s de stockage de donn\u00e9es est \u00e0 l'origine d'une explosion de la taille des jeux de donn\u00e9es \u00e0 traiter. D'o\u00f9 la n\u00e9cessit\u00e9 de trouver de nouvelles mani\u00e8res de manipuler, traiter, analyser et interpr\u00e9ter nos donn\u00e9es. La 1\u00e8re \u00e9tape lorsque l'on est confront\u00e9 \u00e0 un vaste jeu de donn\u00e9es est toujours de l'analyser, afin d'essayer de le comprendre : Quels types de donn\u00e9es contient-il ? Ces donn\u00e9es sont-elles de qualit\u00e9 ? Comment ces donn\u00e9es sont-elles r\u00e9parties ? Peut-on tisser des liens entre les diff\u00e9rentes variables ? Peut-on regrouper les diff\u00e9rentes r\u00e9alisations de ces variables en groupes ? Cette \u00e9tape est essentielle si l'on veut par la suite entrainer un mod\u00e8le \u00e0 \"apprendre\" de nos donn\u00e9es. Nature et type des donn\u00e9es Une des difficult\u00e9s rencontr\u00e9es en sciences des donn\u00e9es provient de la grande vari\u00e9t\u00e9s des donn\u00e9es . Donn\u00e9es de diff\u00e9rentes natures Tout d'abord, les variables \u00e9tudi\u00e9es peuvent \u00eatre de nature diff\u00e9rente : Une donn\u00e9e quantitative continue peut prendre n'importe quelle valeur num\u00e9rique : par exemple, le prix d'un kilo de farine. Une donn\u00e9e quantitative discr\u00e8te ne peut prendre qu'un nombre fini de valeurs num\u00e9riques dans un intervalle : par exemple, le nombre de p\u00e9pites de chocolats dans une brioche. Une donn\u00e9e qualitative nominale est descriptive sans ordre hi\u00e9rarchique : par exemple, la r\u00e9gion d'origine d'une p\u00e2tisserie. Une donn\u00e9e qualitative ordinale est descriptive avec un ordre hi\u00e9rarchique : par exemple, le niveau de cuisson d'une baguette de pain (blanche, pas trop cuite, bien cuite). La plupart des mod\u00e8les d'apprentissage automatique ne prennent que des valeurs num\u00e9riques en entr\u00e9e. On va donc en g\u00e9n\u00e9ral encoder des donn\u00e9es qualitatives avec des valeurs num\u00e9riques . Par exemple : Cuisson du pain Encodage Blanc 1 Pas trop cuit 2 Bien cuit 3 Cette m\u00e9thode fonctionne bien pour des donn\u00e9es ordinales comme la cuisson du pain, mais pour des donn\u00e9es nominales le mod\u00e8le risque de croire qu'il y a un ordre hi\u00e9rarchique dans les donn\u00e9es qui n'existe pas. C'est pourquoi on utilise souvent l'encodage one-hot . L'id\u00e9e est de faire comme si chaque nom possible pour une variable qualitative \u00e9tait une variable en soit. On appelle parfois ces variables imaginaires des \"dummy variables\". Par exemple, pour la r\u00e9gion d'origine des p\u00e2tisseries, on passe de : P\u00e2tisserie R\u00e9gion Croissant Paris Merveilleux Nord Kouign-amann Bretagne Cannel\u00e9 Sud-Ouest Kougelhopf Est \u00e0 l'encodage one-hot suivant : P\u00e2tisserie Paris Nord Bretagne Sud-Ouest Est Croissant 1 0 0 0 0 Merveilleux 0 1 0 0 0 Kouign-amann 0 0 1 0 0 Cannel\u00e9 0 0 0 1 0 Kougelhopf 0 0 0 0 1 Pour le Merveilleux, on donnera donc en entr\u00e9e d'un mod\u00e8le le binaire 01000. On remarque ici que plus la variable a de noms possibles, et plus les binaires d'encodage one-hot seront longs, ce qui peut \u00eatre probl\u00e9matique. Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package preprocessing une fonction OrdinalEncoder , permettant d'assigner un entier \u00e0 des variables qualitatives ordinales. Dans ce m\u00eame package, vous trouverez \u00e9galement une fonction OneHotEncoder , permettant d'encoder en one-hot des variables qualitatives nominales. Dans les 2 cas, il vous faut cr\u00e9er une instance de OrdinalEncoder ou de OneHotEncoder , puis utiliser la m\u00e9thode fit_transform() avec vos donn\u00e9es en entr\u00e9e. Donn\u00e9es multidimensionnelles Les donn\u00e9es \u00e9tudi\u00e9es peuvent aussi \u00eatre multidimensionnelles . En effet, dans la pluplart des situations, notre jeu de donn\u00e9es peut se mettre sous la forme d'un tableau, dont Les colonnes correspondront aux \" variables \". Les lignes correspondront aux \" individus \" : les diff\u00e9rentes r\u00e9alisations de ces variables. L'ensemble des individus sera nomm\u00e9 \" population \", une s\u00e9lection des individus un \" \u00e9chantillon \". Voici un exemple de jeu de donn\u00e9es multidimensionnelles : Brioche n\u00b01 Poids (g) Nombre de p\u00e9pites de chocolat Prix (\u20ac) 1 70 13 3.5 2 80 17 3.6 3 85 15 3.7 4 83 16 3.4 5 76 18 3.3 6 78 13 3.5 Nous avons ici 6 individus, les brioches, pour lesquelles nous avons mesur\u00e9 3 variables, le poids, le nombre de p\u00e9pites de chocolat, et le prix. Astuce Python Pour stocker puis manipuler des donn\u00e9es multidimensionnelles, on utilise souvent en Python un type de conteneur de la biblioth\u00e8que Pandas : les \" DataFrames \". Les DataFrames se pr\u00e9sentent comme des tableaux pouvant contenir des variables de types diff\u00e9rents, avec un label associ\u00e9 \u00e0 chaque colonne du tableau (variable). Nous reparlerons de Pandas plus loin dans ce chapitre. Donn\u00e9es structur\u00e9es Enfin, les donn\u00e9es \u00e9tudi\u00e9es peuvent \u00eatre structur\u00e9es . On entend par l\u00e0 que des donn\u00e9es peuvent avoir un coh\u00e9rence chronologique (s\u00e9rie temporelle, un son) ou spatiale (une carte, une image, un texte, une vid\u00e9o). Par exemple, dans le cas d'une image : Chaque pixel de l'image doit \u00eatre compris dans le contexte global de l'image. Il est \u00e9vident que changer la position des pixels les uns par rapport aux autres change le jeu de donn\u00e9es : Dans certains cas, l' ordre des donn\u00e9es est donc en soit une information n\u00e9cessaire \u00e0 leur interpr\u00e9tation. Vous l'aurez compris, la nature des donn\u00e9es, leur dimensionnalit\u00e9, ainsi que leur structure, peuvent rendre leur compr\u00e9hension difficile . Nous allons dans la suite voir comment on peut essayer de tirer des informations pertinentes de nos donn\u00e9es. Visualisation graphique La 1\u00e8re \u00e9tape lorsque l'on cherche \u00e0 comprendre ses donn\u00e9es, c'est d'essayer de les visualiser de mani\u00e8re pertinente. Nous allons voir les types de repr\u00e9sentations graphiques les plus classiques pour visualiser un jeu de donn\u00e9es. Courbes et nuages de points : Diagrammes en barres et histogrammes : Bo\u00eetes \u00e0 moustaches : Kernel Density Estimation (KDE) : Graphique en aires : Diagramme circulaire (camembert) : En Python Astuce Python La biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, propose une m\u00e9thode \"plot\" qui permet des affichages graphiques \u00e0 partir de jeux de donn\u00e9es. Il suffit donner le bon param\u00e8tre \"kind\" en entr\u00e9e pour obtenir le type d'affichage voulu : - \"line\" : une courbe. - \"scatter\" : un nuage de points. - \"bar\" : un diagramme en barres vertical. - \"barh\" : un diagramme en barres horizontal. - \"hist\" : un histogramme. - \"box\" : des bo\u00eetes \u00e0 moustaches. - \"kde\" : une \"kernel density estimation\". - \"area\" : un graphique en aires. - \"pie\" : un diagramme circulaire. Statistiques descriptives Toujours dans l'objectif de comprendre notre jeu de donn\u00e9es, on peut essayer de d\u00e9crire chaque variable par des indicateurs statistiques . Nous allons voir ici les indicateurs les plus communs en statistiques descriptives. Il est important de savoir comment ces indicateurs sont d\u00e9finis afin de comprendre les informations qu'ils donnent ou ne donnent pas sur un jeu de donn\u00e9es. Moyenne, m\u00e9diane et mode Lorsque l'on veut connaitre l'ordre de grandeur des valeurs d'une variable, l\u00e0 o\u00f9 se rassemblent la plupart des valeurs, on va utiliser un indicateur de tendance centrale : moyenne, m\u00e9diane ou mode. Il existe plusieurs fa\u00e7on de d\u00e9finir la moyenne, mais la plus connue est la moyenne arithm\u00e9tique : \\(\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i\\) On note en effet souvent \\(\\overline{x}\\) la moyenne d'une variable \\(x\\) . La m\u00e9diane est la valeur s\u00e9parant les valeurs de la variable en 2 groupes de m\u00eame taille : la moiti\u00e9 des valeurs sont sup\u00e9rieures \u00e0 la m\u00e9diane, l'autre moiti\u00e9 lui sont inf\u00e9rieures. Le mode est la valeur la plus repr\u00e9sent\u00e9e dans l'ensemble des valeurs de la variable. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".mean()\": la moyenne. - \".median()\": la m\u00e9diane. - \".mode()\": le mode. Variance et \u00e9cart-type Lorsque l'on veut savoir \u00e0 quel point les valeurs d'une variable fluctuent autour de la valeur centrale, on va utiliser des indicateurs de dispersion . Les valeurs extr\u00eames de la variable, le min et le max , pour connaitre l'\u00e9tendue de la variable. La variance est d\u00e9finie par la moyenne des carr\u00e9es des \u00e9carts \u00e0 la moyenne : \\(\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\\) L' \u00e9cart-type (souvent not\u00e9 \\(\\sigma\\) ) est la racine carr\u00e9e de la variance, soit a moyenne quadratique de \u00e9carts \u00e0 la moyenne. Contrairement \u00e0 la variance, il a l'avantage d'\u00eatre homog\u00e8ne \u00e0 la variable \u00e9tudi\u00e9e . Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".min()\" et \".max()\": le minimum et le maximum. - \".var()\": la variance. - \".std()\": l'\u00e9cart-type. Quantiles Afin d'avoir plus d'informations sur la r\u00e9partition de valeurs d'une variable, on peut g\u00e9n\u00e9raliser la notion de m\u00e9diane en utilisant ce que l'on appelle les quantiles : La division des valeurs de la variables en groupes de tailles \u00e9gales. Quartiles : 3 indicateurs en divisant les valeurs de la variable en 4 groupes (25%,50% et 75%). D\u00e9ciles : 9 indicateurs en divisant les valeurs de la variable en 10 groupes (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%). Centiles : 99 indicateurs en divisant les valeurs de la variable en 100 groupes (1%, 2%, 3%, ..., 98%, 99%). Asym\u00e9trie et kurtosis Enfin, si l'on veut une information sur la r\u00e9partition des valeurs d'une variable, sous la forme d'un indicateur unique, on va utiliser un indicateur de forme . Le coefficient d' asym\u00e9trie (\"skewness\" en anglais, souvent not\u00e9 \\(\\gamma_1\\) ) permet de quantifier le d\u00e9sequilibre de la r\u00e9partition des valeurs de la variable de chaque c\u00f4t\u00e9 de sa valeur centrale. \\(\\gamma_1 = \\frac{1}{N \\sigma^3} \\sum_{i=1}^{N} (x_i - \\overline{x})^3\\) Un coefficient n\u00e9gatif indique un d\u00e9calage \u00e0 droite, un coefficient positif un d\u00e9calage \u00e0 gauche, et un coefficient nul une distribution sym\u00e9trique. NB : Pour la loi normale, on a \\(\\gamma_1 = 0\\) . Le kurtosis (souvent not\u00e9 \\(\\gamma_2\\) ) permet de quantifier l'acuit\u00e9 ou l'applatissement de la r\u00e9partition des valeurs de la variable autour de sa valeur centrale. \\(\\gamma_2 = \\frac{1}{N \\sigma^4} \\sum_{i=1}^{N} (x_i - \\overline{x})^4\\) Un kurtosis positif est un indicateur de valeurs anormales de la variable (aux extr\u00eamit\u00e9s) plus fr\u00e9quentes. Un kurtosis n\u00e9gatif est un indicateur d'une distribution tr\u00e8s applatie des valeurs de la variable. NB : Pour la loi normale, on a \\(\\gamma_2 = 0\\) . Recherche de corr\u00e9lation Une fois que l'on a d\u00e9crit statistiquement les diff\u00e9rentes variables de notre jeu de donn\u00e9es, on va souvent vouloir essayer des tisser des liens entre ces variables. Cette analyse exploratoire des donn\u00e9es a 2 principales utilit\u00e9s : Voir si une ou plusieurs variables pourraient servir \u00e0 en pr\u00e9dire une ou plusieurs autres. Essayer de r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me bas\u00e9 sur ces variables. En effet, comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es sont souvent multidimensionnels. Quand la dimension des donn\u00e9es devient tr\u00e8s grande, la quantit\u00e9 de donn\u00e9es devient peu dense en comparaison, ce qui rend difficile leur interpr\u00e9tation. On appelle commun\u00e9ment ce probl\u00e8me le \"Fl\u00e9au de la dimension\" (\"curse of dimensionality\" en anglais). Nous allons voir dans un 1er temps comment essayer de d\u00e9terminer ce que l'on appelle des \" corr\u00e9lations \" entre variables. Puis nous verrons une m\u00e9thode classique de r\u00e9duction de dimension appel\u00e9e \" Analyse en Composantes Principales \". Matrice de corr\u00e9lation Une 1\u00e8re approche pour essayer de tisser des liens ou \" corr\u00e9lations \" entre les variables et de tracer ce que l'on appelle une matrice de nuages de points, ou \" scatter-matrix \" en anglais. L'id\u00e9e est d'afficher une matrice de graphiques , repr\u00e9sentant chacun une variable en fonction d'une autre , sous la forme d'un nuage de points. La diagonale n'\u00e9tant pas tr\u00e8s utile (une variable en fonction d'elle-m\u00eame), on la remplace en g\u00e9n\u00e9ral par un histogramme de la variable en question. Ce type de repr\u00e9sentation permet de d\u00e9tecter visuellement des relations entre les variables . Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"plotting.scatter_matrix()\", qui permet d'afficher une \"scatter_matrix\". Pour quantifier la corr\u00e9lation entre 2 variables \\(x\\) et \\(y\\) , on va souvent se contenter de mesurer \u00e0 quel point une relation lin\u00e9aire \\(y = a x + b\\) peut \u00eatre tir\u00e9e de ces variables. Pour cela, on va calculer le coefficient de corr\u00e9lation de Pearson : \\(r = \\frac{\\sum_{i=1}^{N} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^{N} (x_i - \\overline{x})^2 \\sum_{i=1}^{N} (y_i - \\overline{y})^2}}\\) La valeur de ce coefficient est toujours compris entre -1 et 1 : Une valeur de 1 signifie une corr\u00e9lation parfaite entre les variables. Une valeur de -1 signifie une anti-corr\u00e9lation parfaite entre les variables. Une valeur de 0 signifie une d\u00e9corr\u00e9lation parfaite entre les variables : elles sont ind\u00e9pendantes . Il y a du sens \u00e0 vouloir pr\u00e9dire une variable \u00e0 partir d'une autre si elles sont corr\u00e9l\u00e9es / anti-corr\u00e9l\u00e9es. On peut aussi imaginer r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me s'il se base sur plusieurs variables qui ne sont pas ind\u00e9pendantes. NB : Attention ! Corr\u00e9lation entre variables n'implique pas causalit\u00e9 entre variables ! On affiche souvent les coefficients de corr\u00e9lation obtenus pour toutes les combinaisons de variables possibles sous la forme d'une matrice : la matrice de corr\u00e9lation de ces variables. La diagonale de la matrice ne contient bien \u00e9videmment que des 1. Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"corr()\" associ\u00e9e aux DataFrames. Elle retourne une matrice de corr\u00e9lation du jeu de donn\u00e9es. Analyse en Composantes Principales (ACP) Comme mentionn\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es que l'on rencontre sont souvent multidimensionels. Ceci rend difficile voir impossible un affichage graphique compr\u00e9hensible des individus d'une variable par rapport \u00e0 une autre (il faudrait un graphique 2D pour 2 variables, 3D pour 3 variables, 4D pour 4 variables, etc.). Afin de repr\u00e9senter des donn\u00e9es multidimensionnelles sous la forme d'un affichage graphique de dimension faible (en g\u00e9n\u00e9ral 1, 2 ou 3), on utilise souvent l' Analyse en Composantes Principales (ACP). L'id\u00e9e est la suivante. Soit un jeu de donn\u00e9es contenant \\(p\\) variables et \\(n\\) individus. On va chercher \\(q\\) nouvelles variables par projections lin\u00e9aires des \\(p\\) variables d'origine, avec \\(q < p\\) , de mani\u00e8re \u00e0 perdre le moins d'information possible sur le jeu de donn\u00e9es. Ces \\(q\\) nouvelles variables sont alors nomm\u00e9es composantes principales . Il existe plusieurs algorithmes pour obtenir ce r\u00e9sultat, celui impl\u00e9ment\u00e9 dans la biblioth\u00e8que Python Scikit-Learn se base sur la D\u00e9composition en Valeurs Singuli\u00e8res (SVD) de la matrice de donn\u00e9es : \\(X = \\begin{pmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} &\\cdots & x_{n,p} \\end{pmatrix}\\) o\u00f9 chaque colonne correspond \u00e0 une variable, et chaque ligne correspond \u00e0 un individu. On peut voir l'ACP comme le choix du sous-espace de dimension \\(q\\) tel que le nuage de points projet\u00e9s ait la variance la plus grande possible. Les r\u00e9sultats d'une ACP peuvent \u00eatre affich\u00e9s sous la forme d'un nuage de points 2D ou 3D ( \\(q = 2\\) ou \\(3\\) ) repr\u00e9sentant les diff\u00e9rents individus, avec pour axes les composantes principales. Voici un exemple : L'id\u00e9e est de voir si on peut s\u00e9parer les individus en diff\u00e9rents groupes \u00e0 partir des composantes principales. Pour juger de la qualit\u00e9 d'une ACP, on utilise un type de graphique appel\u00e9 \" cercle des corr\u00e9lations \". Ce graphique 2D repr\u00e9sente sur chaque axe la corr\u00e9lation des \\(p\\) variables d'origine avec les \\(q\\) composantes principales. Chacune des \\(p\\) variables correspond \u00e0 un vecteur sur ce graphique, et un cercle de rayon 1 est \u00e9galement affich\u00e9 pour comparaison. Voici un exemple : Un cercle des corr\u00e9lations permet donc de juger de la corr\u00e9lation des variables d'origines avec les composantes principales, et de la corr\u00e9lation des variables d'origine entre elles : Plus une variable d'origine est proche du cercle, plus elle est fid\u00e8lement repr\u00e9sent\u00e9e par l'ACP. Dans l'id\u00e9al, on voudrait donc que toutes les variables soient proches du cercle. Pour 2 variables d'origine proches du cercle, si l'angle entre 2 les variables est aigu elles sont corr\u00e9l\u00e9es, s'il est obtu elles sont anti-corr\u00e9l\u00e9es, et s'il est droit elles sont d\u00e9corr\u00e9l\u00e9es. On pourra utiliser la projection des donn\u00e9es renvoy\u00e9e par l'ACP pour entrainer des mod\u00e8les d'apprentissage. Astuce Python La classe \"sklearn.decomposition.PCA\" de la biblioth\u00e8que \"Scikit-Learn\" vous permet de r\u00e9aliser l'ACP d'une matrice de donn\u00e9es. Le nombre de composantes principales \u00e0 trouver est un des attributs de la classe \u00e0 initialiser (\"n_components\"). Pour obtenir les composantes principales d'une matrice de donn\u00e9es, il faut lui appliquer la m\u00e9thode \"fit_transform()\" de la classe. Pour aller plus loin : D'autres m\u00e9thodes de r\u00e9duction de dimensionnalit\u00e9 existent, on peut citer entre autres les \"auto-encodeurs\" et la \"t-SNE\". Pr\u00e9paration des donn\u00e9es Une fois les donn\u00e9es analys\u00e9es, on a normalement une bonne id\u00e9e de ce qu'un outil automatique pourra en \"apprendre\" ou non. Cependant, la plupart de ces outils (dont nous parlerons dans la section suivante), ont besoin que les donn\u00e9es soient \" transform\u00e9es \" d'une certaine mani\u00e8re. C'est pourquoi nous allons voir dans cette section quelques transformations classiques pour pr\u00e9parer nos donn\u00e9es . Tout d'abord, il est possible que le jeu de donn\u00e9es contienne des valeurs erron\u00e9es ou manquantes , souvent marqu\u00e9es par des NaN (\"Not a Number\"). Il convient alors de se d\u00e9barrasser de ces valeurs avant apprentissage, car la plupart des outils ne savent pas g\u00e9rer ce probl\u00e8me. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"dropna\" associ\u00e9e aux objets DataFrames. Cette m\u00e9thode permet de supprimer les NaN d'un DataFrame. Nous avons aussi vu pr\u00e9c\u00e9demment que les donn\u00e9es qualitatives doivent \u00eatre encod\u00e9es avant apprentissage, soit en \"ordinal\", soit en \"one-hot\". Enfin, les outils d'apprentissage sont affect\u00e9s par les diff\u00e9rences d'ordre de grandeur entre les variables . C'est pourquoi une remise \u00e0 l'\u00e9chelle des diff\u00e9rentes variables d'un jeu de donn\u00e9es est n\u00e9cessaire avant apprentissage. On appelle ce processus recalibration , ou \" feature scaling \" en anglais. Nous allons voir en particulier 2 types de transformation pour recalibrer des donn\u00e9es : la transformation min-max et le centrage-r\u00e9duction . Transformation min-max Certains types de mod\u00e8les d'apprentissage n\u00e9cessitent des valeurs d'entr\u00e9e entre 0 et 1. C'est pourquoi la transformation min-max (\"normalization\" en anglais) va recalibrer toutes les variables de mani\u00e8re \u00e0 ce que leurs valeurs restent entre 0 et 1 . Pour ce faire, on va appliquer la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x_i-min_j}{max_j-min_j}\\) avec \\(min_j\\) le minimum et \\(max_j\\) le maximum des individus de la j-\u00e8me variable. (Il est \u00e9galement possible d'adapter cette transformation pour les mod\u00e8les prenant des valeurs entre -1 et 1 en entr\u00e9e). Le probl\u00e8me majeur avec cette transformation est sa sensibilit\u00e9 aux valeurs aberrantes. En effet, il suffit qu'une variable ait une valeur aberrante pour qu'elle devienne le minimum ou le maximum, impactant ainsi la transformation. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"MinMaxScaler\". Centrage-r\u00e9duction La transformation centrage-reduction (\"standardization\" en anglais) applique la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x-\\overline{x_p}}{\\sigma_p}\\) avec \\(\\overline{x_p}\\) la moyenne et \\(\\sigma_p\\) l'\u00e9cart-type des individus de la j-\u00e8me variable. Cette transformation est beaucoup moins sensible aux valeurs aberrantes, mais elle ne garanti pas que les valeurs des diff\u00e9rentes variables seront entre 0 et 1 (ou -1 et 1). Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"StandardScaler\". Autres transformations Nous l'avons pr\u00e9c\u00e9demment, on peut d\u00e9couvrir que les individus d'une variable ont une distribution asym\u00e9trique. Par exemple, la distribution des individus peut avoir une longue tra\u00eene d'un c\u00f4t\u00e9 de la m\u00e9diane. On peut aussi avoir une distribution multimodale (c'est-\u00e0-dire avec plusieurs pics). Ceci peut perturber un apprentissage automatique. Dans ces situations, d'autres types de transformation pourrons alors \u00eatre envisag\u00e9es en addition des 2 pr\u00e9c\u00e9dentes : utiliser la racine carr\u00e9e ou le logarithme de la variable, utiliser les quantiles de la variable, utiliser un encodage de la variable, etc. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" permet de cr\u00e9er sa propre transformation, avec \"FunctionTransformer\". Les apprentissages Une fois que l'on a bien cern\u00e9 notre jeu de donn\u00e9es, et qu'on l'a transform\u00e9 de mani\u00e8re ad\u00e9quate, on va en g\u00e9n\u00e9ral vouloir le mod\u00e9liser . L'id\u00e9e du mod\u00e8le sera de prendre une d\u00e9cision sur \u00e0 partir de nouvelles donn\u00e9es, en se basant sur la connaissance des donn\u00e9es de la base d'origine. On parle alors d'\"apprendre\" des donn\u00e9es. L'apprentissage automatique Par \"mod\u00e9liser\", on entend trouver une fonction param\u00e9trique \\(M\\) qui permet de d\u00e9duire une sortie vectorielle \\(y\\) voulue \u00e0 partir d'une entr\u00e9e vectorielle de nouvelles donn\u00e9es \\(x\\) et de nos connaissances sur les donn\u00e9es d'origine : \\(y = M(x,\\theta)\\) avec \\(\\theta\\) les param\u00e8tres du mod\u00e8le, qui correspondent \u00e0 notre connaissance du jeu de donn\u00e9es initial. Il nous faut donc ajuster les param\u00e8tres \\(\\theta\\) pour obtenir la sortie \\(y\\) attendue en fonction de \\(x\\) qui colle le plus aux donn\u00e9es. C'est ce processus d' optimisation de \\(\\theta\\) que l'on appelle \" apprentissage \". Les jeux de donn\u00e9es dont on doit apprendre sont en g\u00e9n\u00e9ral \u00e9normes, ce qui rend souvent un ajustement manuel des param\u00e8tres impossible. C'est pourquoi on va en g\u00e9n\u00e9ral choisir un type de mod\u00e8le , et ajuster automatiquement les param\u00e8tres \u00e0 nos donn\u00e9es. D'o\u00f9 l'expression \" apprentissage automatique \". Suivant les applications, il existe diff\u00e9rents types d'apprentissage, avec pour chacun diff\u00e9rents types de mod\u00e8les possibles. Lors de ce cours, nous verrons 3 grands types d'apprentissage, et nous verrons pour chacun quelques exemples de mod\u00e8les classiques. Les 3 grands types d'apprentissages En apprentissage, on appelle souvent en anglais les entr\u00e9e d'un mod\u00e8le les \" features \", et les sortie des \" labels \". Lors du processus d' apprentissage (ajustement des param\u00e8tres), on va enseigner au mod\u00e8le comment d\u00e9terminer des \"labels\" correspondant \u00e0 des \"features\", en se basant sur ce qu'il a appris d'une base de donn\u00e9es d'\"entrainement\" de \"features\". Apprentissage supervis\u00e9 ou non-supervis\u00e9 Dans le cas o\u00f9 les donn\u00e9es d'apprentissage ont des \"labels\" d\u00e9finis, le mod\u00e8le va apprendre \u00e0 retrouver ces \"labels\" (connus) pour ces \"features\". On esp\u00e8re alors qu'apr\u00e8s apprentissage, le mod\u00e8le pourra retourner les \"labels\" corrects une fois confront\u00e9 \u00e0 des \"features\" issus de nouvelles donn\u00e9es. On parle alors de \" g\u00e9n\u00e9ralisation \". Comme on peut directement v\u00e9rifier les performances du mod\u00e8le \u00e0 pr\u00e9dire les \"labels\" du jeu de donn\u00e9es d'entrainement, on parle d' apprentissage supervis\u00e9 . Dans le cas o\u00f9 les donn\u00e9es d'apprentissage n'ont pas de \"labels\", on peut tout de m\u00eame essayer de diviser les individus des \"features\" en diff\u00e9rent groupes, auxquels on assignera des \"labels\" plus tard. Comme nous n'avons pas de \"labels\" d'entrainement comme r\u00e9f\u00e9rence, on parle d' apprentissage non-supervis\u00e9 ou \"clustering\" (\"partition de donn\u00e9es\"). Classification et r\u00e9gression On peut aussi diviser les apprentissages suivant le type de sortie attendu, et donc de mod\u00e8le \u00e0 entrainer. Si la sortie est un groupe d'individus auxquel on veut assigner un nouvel individu, on va parler de \" classification \". Si la sortie est un vecteur de valeurs de variables pour un nouvel individu, on va parler de \" r\u00e9gression \". On peut entrainer un mod\u00e8le de classification de mani\u00e8re supervis\u00e9e ou non-supervis\u00e9e . On ne peut entrainer un mod\u00e8le de r\u00e9gression que de mani\u00e8re supervis\u00e9e . Pour aller plus loin... Il existe un 3\u00e8me type d'apprentissage, que nous ne d\u00e9taillerons pas dans ce cours, qui s'appelle \"apprentissage par renforcement \". L'id\u00e9e est la suivante : Le mod\u00e8le est directement mis en place sur son cas d'application final. Le mod\u00e8le prend des d\u00e9cisions en fonction des situations, et re\u00e7oit un retour (\"feedback\") sur sa d\u00e9cision, positif ou n\u00e9gatif. Le mod\u00e8le se met \u00e0 jour en fonction du retour qu'il a re\u00e7u. Ce processus se r\u00e9p\u00e8te pour chaque nouvelle situation, et ainsi le mod\u00e8le apprend de ses exp\u00e9riences . Difficult\u00e9s de l'apprentissage Comme expliqu\u00e9 plus haut, l' apprentissage est un processus d' optimisation , qui consiste en l' ajustement des param\u00e8tres d'un mod\u00e8le en se basant sur les donn\u00e9es disponibles, dans le but de prendre des d\u00e9cisions correctes \u00e0 partir de donn\u00e9es futures ( g\u00e9n\u00e9ralisation ). La phase durant laquelle on ajuste les param\u00e8tres est appel\u00e9e entra\u00eenement , et les donn\u00e9es sur lesquelles cet ajustement est fait sont appel\u00e9es \" base de donn\u00e9es d'entra\u00eenement \". Dans la section qui suit, nous aurons un aper\u00e7u des grandes difficult\u00e9es que l'on peut rencontrer lors de l'entrainement d'un mod\u00e8le, tous types de mod\u00e8les confondus. Quantit\u00e9 et qualit\u00e9 des donn\u00e9es S'il n'y a pas de r\u00e8gle pr\u00e9cise pour d\u00e9terminer la quantit\u00e9 de donn\u00e9es n\u00e9cessaire \u00e0 un apprentissage, il y a 2 maximes \u00e0 retenir : Plus on a donn\u00e9es d'entrainement, meilleur sera l'apprentissage par le mod\u00e8le. Plus le probl\u00e8me complexe, plus il faudra de donn\u00e9es d'entrainement. Pour donner un ordre de grandeur, la quantit\u00e9 d'individus n\u00e9cessaires \u00e0 un apprentissage va en g\u00e9n\u00e9ral de quelques milliers \u00e0 des centaines de millions . Cependant, il n'est pas ais\u00e9 de constituer une base de donn\u00e9es aussi large, et de surcroit une base de donn\u00e9e de qualit\u00e9. En effet, comme on peut facilement le deviner, la qualit\u00e9 des donn\u00e9es aura un impact sur l'apprentissage. La qualit\u00e9 des donn\u00e9es peut par exemple \u00eatre d\u00e9grad\u00e9e par : La pr\u00e9sence d'individus ab\u00e9rrants (\"outliers\"), li\u00e9e \u00e0 des erreurs de mesures ou \u00e0 des cas exceptionnels. Des individus manquants, li\u00e9s \u00e0 notre \u00e9chantillonnage ou a des erreurs de mesures. La pr\u00e9sence de bruit dans les donn\u00e9es. D'o\u00f9 la n\u00e9cessit\u00e9 de proc\u00e9der \u00e0 un nettoyage des donn\u00e9es en amont de l'apprentissage : supprimer certaines donn\u00e9es, les combler, ou faire de nouvelles mesures. Repr\u00e9sentativit\u00e9 et \u00e9quilibre des donn\u00e9es Pertinence des variables Sur-apprentissage / sous-apprentissage Test, validation et hyperparam\u00e8tres Import de donn\u00e9es et fichiers CSV Outils Python pour l'apprentissage Pandas Scikit-Learn Keras-Tensorflow, Pytorch","title":"I. Introduction"},{"location":"Chap1_Introduction/#chapitre-i-introduction-aux-sciences-des-donnees","text":"Ce chapitre porte sur les grands concepts et les enjeux des sciences des donn\u00e9es.","title":"Chapitre I : Introduction aux sciences des donn\u00e9es"},{"location":"Chap1_Introduction/#analyse-de-donnees","text":"L'explosion des capacit\u00e9s de stockage de donn\u00e9es est \u00e0 l'origine d'une explosion de la taille des jeux de donn\u00e9es \u00e0 traiter. D'o\u00f9 la n\u00e9cessit\u00e9 de trouver de nouvelles mani\u00e8res de manipuler, traiter, analyser et interpr\u00e9ter nos donn\u00e9es. La 1\u00e8re \u00e9tape lorsque l'on est confront\u00e9 \u00e0 un vaste jeu de donn\u00e9es est toujours de l'analyser, afin d'essayer de le comprendre : Quels types de donn\u00e9es contient-il ? Ces donn\u00e9es sont-elles de qualit\u00e9 ? Comment ces donn\u00e9es sont-elles r\u00e9parties ? Peut-on tisser des liens entre les diff\u00e9rentes variables ? Peut-on regrouper les diff\u00e9rentes r\u00e9alisations de ces variables en groupes ? Cette \u00e9tape est essentielle si l'on veut par la suite entrainer un mod\u00e8le \u00e0 \"apprendre\" de nos donn\u00e9es.","title":"Analyse de donn\u00e9es"},{"location":"Chap1_Introduction/#nature-et-type-des-donnees","text":"Une des difficult\u00e9s rencontr\u00e9es en sciences des donn\u00e9es provient de la grande vari\u00e9t\u00e9s des donn\u00e9es .","title":"Nature et type des donn\u00e9es"},{"location":"Chap1_Introduction/#donnees-de-differentes-natures","text":"Tout d'abord, les variables \u00e9tudi\u00e9es peuvent \u00eatre de nature diff\u00e9rente : Une donn\u00e9e quantitative continue peut prendre n'importe quelle valeur num\u00e9rique : par exemple, le prix d'un kilo de farine. Une donn\u00e9e quantitative discr\u00e8te ne peut prendre qu'un nombre fini de valeurs num\u00e9riques dans un intervalle : par exemple, le nombre de p\u00e9pites de chocolats dans une brioche. Une donn\u00e9e qualitative nominale est descriptive sans ordre hi\u00e9rarchique : par exemple, la r\u00e9gion d'origine d'une p\u00e2tisserie. Une donn\u00e9e qualitative ordinale est descriptive avec un ordre hi\u00e9rarchique : par exemple, le niveau de cuisson d'une baguette de pain (blanche, pas trop cuite, bien cuite). La plupart des mod\u00e8les d'apprentissage automatique ne prennent que des valeurs num\u00e9riques en entr\u00e9e. On va donc en g\u00e9n\u00e9ral encoder des donn\u00e9es qualitatives avec des valeurs num\u00e9riques . Par exemple : Cuisson du pain Encodage Blanc 1 Pas trop cuit 2 Bien cuit 3 Cette m\u00e9thode fonctionne bien pour des donn\u00e9es ordinales comme la cuisson du pain, mais pour des donn\u00e9es nominales le mod\u00e8le risque de croire qu'il y a un ordre hi\u00e9rarchique dans les donn\u00e9es qui n'existe pas. C'est pourquoi on utilise souvent l'encodage one-hot . L'id\u00e9e est de faire comme si chaque nom possible pour une variable qualitative \u00e9tait une variable en soit. On appelle parfois ces variables imaginaires des \"dummy variables\". Par exemple, pour la r\u00e9gion d'origine des p\u00e2tisseries, on passe de : P\u00e2tisserie R\u00e9gion Croissant Paris Merveilleux Nord Kouign-amann Bretagne Cannel\u00e9 Sud-Ouest Kougelhopf Est \u00e0 l'encodage one-hot suivant : P\u00e2tisserie Paris Nord Bretagne Sud-Ouest Est Croissant 1 0 0 0 0 Merveilleux 0 1 0 0 0 Kouign-amann 0 0 1 0 0 Cannel\u00e9 0 0 0 1 0 Kougelhopf 0 0 0 0 1 Pour le Merveilleux, on donnera donc en entr\u00e9e d'un mod\u00e8le le binaire 01000. On remarque ici que plus la variable a de noms possibles, et plus les binaires d'encodage one-hot seront longs, ce qui peut \u00eatre probl\u00e9matique. Astuce Python La biblioth\u00e8que Scikit-Learn poss\u00e8de dans son package preprocessing une fonction OrdinalEncoder , permettant d'assigner un entier \u00e0 des variables qualitatives ordinales. Dans ce m\u00eame package, vous trouverez \u00e9galement une fonction OneHotEncoder , permettant d'encoder en one-hot des variables qualitatives nominales. Dans les 2 cas, il vous faut cr\u00e9er une instance de OrdinalEncoder ou de OneHotEncoder , puis utiliser la m\u00e9thode fit_transform() avec vos donn\u00e9es en entr\u00e9e.","title":"Donn\u00e9es de diff\u00e9rentes natures"},{"location":"Chap1_Introduction/#donnees-multidimensionnelles","text":"Les donn\u00e9es \u00e9tudi\u00e9es peuvent aussi \u00eatre multidimensionnelles . En effet, dans la pluplart des situations, notre jeu de donn\u00e9es peut se mettre sous la forme d'un tableau, dont Les colonnes correspondront aux \" variables \". Les lignes correspondront aux \" individus \" : les diff\u00e9rentes r\u00e9alisations de ces variables. L'ensemble des individus sera nomm\u00e9 \" population \", une s\u00e9lection des individus un \" \u00e9chantillon \". Voici un exemple de jeu de donn\u00e9es multidimensionnelles : Brioche n\u00b01 Poids (g) Nombre de p\u00e9pites de chocolat Prix (\u20ac) 1 70 13 3.5 2 80 17 3.6 3 85 15 3.7 4 83 16 3.4 5 76 18 3.3 6 78 13 3.5 Nous avons ici 6 individus, les brioches, pour lesquelles nous avons mesur\u00e9 3 variables, le poids, le nombre de p\u00e9pites de chocolat, et le prix. Astuce Python Pour stocker puis manipuler des donn\u00e9es multidimensionnelles, on utilise souvent en Python un type de conteneur de la biblioth\u00e8que Pandas : les \" DataFrames \". Les DataFrames se pr\u00e9sentent comme des tableaux pouvant contenir des variables de types diff\u00e9rents, avec un label associ\u00e9 \u00e0 chaque colonne du tableau (variable). Nous reparlerons de Pandas plus loin dans ce chapitre.","title":"Donn\u00e9es multidimensionnelles"},{"location":"Chap1_Introduction/#donnees-structurees","text":"Enfin, les donn\u00e9es \u00e9tudi\u00e9es peuvent \u00eatre structur\u00e9es . On entend par l\u00e0 que des donn\u00e9es peuvent avoir un coh\u00e9rence chronologique (s\u00e9rie temporelle, un son) ou spatiale (une carte, une image, un texte, une vid\u00e9o). Par exemple, dans le cas d'une image : Chaque pixel de l'image doit \u00eatre compris dans le contexte global de l'image. Il est \u00e9vident que changer la position des pixels les uns par rapport aux autres change le jeu de donn\u00e9es : Dans certains cas, l' ordre des donn\u00e9es est donc en soit une information n\u00e9cessaire \u00e0 leur interpr\u00e9tation. Vous l'aurez compris, la nature des donn\u00e9es, leur dimensionnalit\u00e9, ainsi que leur structure, peuvent rendre leur compr\u00e9hension difficile . Nous allons dans la suite voir comment on peut essayer de tirer des informations pertinentes de nos donn\u00e9es.","title":"Donn\u00e9es structur\u00e9es"},{"location":"Chap1_Introduction/#visualisation-graphique","text":"La 1\u00e8re \u00e9tape lorsque l'on cherche \u00e0 comprendre ses donn\u00e9es, c'est d'essayer de les visualiser de mani\u00e8re pertinente. Nous allons voir les types de repr\u00e9sentations graphiques les plus classiques pour visualiser un jeu de donn\u00e9es.","title":"Visualisation graphique"},{"location":"Chap1_Introduction/#courbes-et-nuages-de-points","text":"","title":"Courbes et nuages de points :"},{"location":"Chap1_Introduction/#diagrammes-en-barres-et-histogrammes","text":"","title":"Diagrammes en barres et histogrammes :"},{"location":"Chap1_Introduction/#boites-a-moustaches","text":"","title":"Bo\u00eetes \u00e0 moustaches :"},{"location":"Chap1_Introduction/#kernel-density-estimation-kde","text":"","title":"Kernel Density Estimation (KDE) :"},{"location":"Chap1_Introduction/#graphique-en-aires","text":"","title":"Graphique en aires :"},{"location":"Chap1_Introduction/#diagramme-circulaire-camembert","text":"","title":"Diagramme circulaire (camembert) :"},{"location":"Chap1_Introduction/#en-python","text":"Astuce Python La biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, propose une m\u00e9thode \"plot\" qui permet des affichages graphiques \u00e0 partir de jeux de donn\u00e9es. Il suffit donner le bon param\u00e8tre \"kind\" en entr\u00e9e pour obtenir le type d'affichage voulu : - \"line\" : une courbe. - \"scatter\" : un nuage de points. - \"bar\" : un diagramme en barres vertical. - \"barh\" : un diagramme en barres horizontal. - \"hist\" : un histogramme. - \"box\" : des bo\u00eetes \u00e0 moustaches. - \"kde\" : une \"kernel density estimation\". - \"area\" : un graphique en aires. - \"pie\" : un diagramme circulaire.","title":"En Python"},{"location":"Chap1_Introduction/#statistiques-descriptives","text":"Toujours dans l'objectif de comprendre notre jeu de donn\u00e9es, on peut essayer de d\u00e9crire chaque variable par des indicateurs statistiques . Nous allons voir ici les indicateurs les plus communs en statistiques descriptives. Il est important de savoir comment ces indicateurs sont d\u00e9finis afin de comprendre les informations qu'ils donnent ou ne donnent pas sur un jeu de donn\u00e9es.","title":"Statistiques descriptives"},{"location":"Chap1_Introduction/#moyenne-mediane-et-mode","text":"Lorsque l'on veut connaitre l'ordre de grandeur des valeurs d'une variable, l\u00e0 o\u00f9 se rassemblent la plupart des valeurs, on va utiliser un indicateur de tendance centrale : moyenne, m\u00e9diane ou mode. Il existe plusieurs fa\u00e7on de d\u00e9finir la moyenne, mais la plus connue est la moyenne arithm\u00e9tique : \\(\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i\\) On note en effet souvent \\(\\overline{x}\\) la moyenne d'une variable \\(x\\) . La m\u00e9diane est la valeur s\u00e9parant les valeurs de la variable en 2 groupes de m\u00eame taille : la moiti\u00e9 des valeurs sont sup\u00e9rieures \u00e0 la m\u00e9diane, l'autre moiti\u00e9 lui sont inf\u00e9rieures. Le mode est la valeur la plus repr\u00e9sent\u00e9e dans l'ensemble des valeurs de la variable. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".mean()\": la moyenne. - \".median()\": la m\u00e9diane. - \".mode()\": le mode.","title":"Moyenne, m\u00e9diane et mode"},{"location":"Chap1_Introduction/#variance-et-ecart-type","text":"Lorsque l'on veut savoir \u00e0 quel point les valeurs d'une variable fluctuent autour de la valeur centrale, on va utiliser des indicateurs de dispersion . Les valeurs extr\u00eames de la variable, le min et le max , pour connaitre l'\u00e9tendue de la variable. La variance est d\u00e9finie par la moyenne des carr\u00e9es des \u00e9carts \u00e0 la moyenne : \\(\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2\\) L' \u00e9cart-type (souvent not\u00e9 \\(\\sigma\\) ) est la racine carr\u00e9e de la variance, soit a moyenne quadratique de \u00e9carts \u00e0 la moyenne. Contrairement \u00e0 la variance, il a l'avantage d'\u00eatre homog\u00e8ne \u00e0 la variable \u00e9tudi\u00e9e . Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a ces m\u00e9thodes associ\u00e9es aux objets DataFrames : - \".min()\" et \".max()\": le minimum et le maximum. - \".var()\": la variance. - \".std()\": l'\u00e9cart-type.","title":"Variance et \u00e9cart-type"},{"location":"Chap1_Introduction/#quantiles","text":"Afin d'avoir plus d'informations sur la r\u00e9partition de valeurs d'une variable, on peut g\u00e9n\u00e9raliser la notion de m\u00e9diane en utilisant ce que l'on appelle les quantiles : La division des valeurs de la variables en groupes de tailles \u00e9gales. Quartiles : 3 indicateurs en divisant les valeurs de la variable en 4 groupes (25%,50% et 75%). D\u00e9ciles : 9 indicateurs en divisant les valeurs de la variable en 10 groupes (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%). Centiles : 99 indicateurs en divisant les valeurs de la variable en 100 groupes (1%, 2%, 3%, ..., 98%, 99%).","title":"Quantiles"},{"location":"Chap1_Introduction/#asymetrie-et-kurtosis","text":"Enfin, si l'on veut une information sur la r\u00e9partition des valeurs d'une variable, sous la forme d'un indicateur unique, on va utiliser un indicateur de forme . Le coefficient d' asym\u00e9trie (\"skewness\" en anglais, souvent not\u00e9 \\(\\gamma_1\\) ) permet de quantifier le d\u00e9sequilibre de la r\u00e9partition des valeurs de la variable de chaque c\u00f4t\u00e9 de sa valeur centrale. \\(\\gamma_1 = \\frac{1}{N \\sigma^3} \\sum_{i=1}^{N} (x_i - \\overline{x})^3\\) Un coefficient n\u00e9gatif indique un d\u00e9calage \u00e0 droite, un coefficient positif un d\u00e9calage \u00e0 gauche, et un coefficient nul une distribution sym\u00e9trique. NB : Pour la loi normale, on a \\(\\gamma_1 = 0\\) . Le kurtosis (souvent not\u00e9 \\(\\gamma_2\\) ) permet de quantifier l'acuit\u00e9 ou l'applatissement de la r\u00e9partition des valeurs de la variable autour de sa valeur centrale. \\(\\gamma_2 = \\frac{1}{N \\sigma^4} \\sum_{i=1}^{N} (x_i - \\overline{x})^4\\) Un kurtosis positif est un indicateur de valeurs anormales de la variable (aux extr\u00eamit\u00e9s) plus fr\u00e9quentes. Un kurtosis n\u00e9gatif est un indicateur d'une distribution tr\u00e8s applatie des valeurs de la variable. NB : Pour la loi normale, on a \\(\\gamma_2 = 0\\) .","title":"Asym\u00e9trie et kurtosis"},{"location":"Chap1_Introduction/#recherche-de-correlation","text":"Une fois que l'on a d\u00e9crit statistiquement les diff\u00e9rentes variables de notre jeu de donn\u00e9es, on va souvent vouloir essayer des tisser des liens entre ces variables. Cette analyse exploratoire des donn\u00e9es a 2 principales utilit\u00e9s : Voir si une ou plusieurs variables pourraient servir \u00e0 en pr\u00e9dire une ou plusieurs autres. Essayer de r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me bas\u00e9 sur ces variables. En effet, comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es sont souvent multidimensionnels. Quand la dimension des donn\u00e9es devient tr\u00e8s grande, la quantit\u00e9 de donn\u00e9es devient peu dense en comparaison, ce qui rend difficile leur interpr\u00e9tation. On appelle commun\u00e9ment ce probl\u00e8me le \"Fl\u00e9au de la dimension\" (\"curse of dimensionality\" en anglais). Nous allons voir dans un 1er temps comment essayer de d\u00e9terminer ce que l'on appelle des \" corr\u00e9lations \" entre variables. Puis nous verrons une m\u00e9thode classique de r\u00e9duction de dimension appel\u00e9e \" Analyse en Composantes Principales \".","title":"Recherche de corr\u00e9lation"},{"location":"Chap1_Introduction/#matrice-de-correlation","text":"Une 1\u00e8re approche pour essayer de tisser des liens ou \" corr\u00e9lations \" entre les variables et de tracer ce que l'on appelle une matrice de nuages de points, ou \" scatter-matrix \" en anglais. L'id\u00e9e est d'afficher une matrice de graphiques , repr\u00e9sentant chacun une variable en fonction d'une autre , sous la forme d'un nuage de points. La diagonale n'\u00e9tant pas tr\u00e8s utile (une variable en fonction d'elle-m\u00eame), on la remplace en g\u00e9n\u00e9ral par un histogramme de la variable en question. Ce type de repr\u00e9sentation permet de d\u00e9tecter visuellement des relations entre les variables . Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"plotting.scatter_matrix()\", qui permet d'afficher une \"scatter_matrix\". Pour quantifier la corr\u00e9lation entre 2 variables \\(x\\) et \\(y\\) , on va souvent se contenter de mesurer \u00e0 quel point une relation lin\u00e9aire \\(y = a x + b\\) peut \u00eatre tir\u00e9e de ces variables. Pour cela, on va calculer le coefficient de corr\u00e9lation de Pearson : \\(r = \\frac{\\sum_{i=1}^{N} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^{N} (x_i - \\overline{x})^2 \\sum_{i=1}^{N} (y_i - \\overline{y})^2}}\\) La valeur de ce coefficient est toujours compris entre -1 et 1 : Une valeur de 1 signifie une corr\u00e9lation parfaite entre les variables. Une valeur de -1 signifie une anti-corr\u00e9lation parfaite entre les variables. Une valeur de 0 signifie une d\u00e9corr\u00e9lation parfaite entre les variables : elles sont ind\u00e9pendantes . Il y a du sens \u00e0 vouloir pr\u00e9dire une variable \u00e0 partir d'une autre si elles sont corr\u00e9l\u00e9es / anti-corr\u00e9l\u00e9es. On peut aussi imaginer r\u00e9duire la dimensionnalit\u00e9 d'un probl\u00e8me s'il se base sur plusieurs variables qui ne sont pas ind\u00e9pendantes. NB : Attention ! Corr\u00e9lation entre variables n'implique pas causalit\u00e9 entre variables ! On affiche souvent les coefficients de corr\u00e9lation obtenus pour toutes les combinaisons de variables possibles sous la forme d'une matrice : la matrice de corr\u00e9lation de ces variables. La diagonale de la matrice ne contient bien \u00e9videmment que des 1. Voici un exemple : Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"corr()\" associ\u00e9e aux DataFrames. Elle retourne une matrice de corr\u00e9lation du jeu de donn\u00e9es.","title":"Matrice de corr\u00e9lation"},{"location":"Chap1_Introduction/#analyse-en-composantes-principales-acp","text":"Comme mentionn\u00e9 pr\u00e9c\u00e9demment, les jeux de donn\u00e9es que l'on rencontre sont souvent multidimensionels. Ceci rend difficile voir impossible un affichage graphique compr\u00e9hensible des individus d'une variable par rapport \u00e0 une autre (il faudrait un graphique 2D pour 2 variables, 3D pour 3 variables, 4D pour 4 variables, etc.). Afin de repr\u00e9senter des donn\u00e9es multidimensionnelles sous la forme d'un affichage graphique de dimension faible (en g\u00e9n\u00e9ral 1, 2 ou 3), on utilise souvent l' Analyse en Composantes Principales (ACP). L'id\u00e9e est la suivante. Soit un jeu de donn\u00e9es contenant \\(p\\) variables et \\(n\\) individus. On va chercher \\(q\\) nouvelles variables par projections lin\u00e9aires des \\(p\\) variables d'origine, avec \\(q < p\\) , de mani\u00e8re \u00e0 perdre le moins d'information possible sur le jeu de donn\u00e9es. Ces \\(q\\) nouvelles variables sont alors nomm\u00e9es composantes principales . Il existe plusieurs algorithmes pour obtenir ce r\u00e9sultat, celui impl\u00e9ment\u00e9 dans la biblioth\u00e8que Python Scikit-Learn se base sur la D\u00e9composition en Valeurs Singuli\u00e8res (SVD) de la matrice de donn\u00e9es : \\(X = \\begin{pmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} &\\cdots & x_{n,p} \\end{pmatrix}\\) o\u00f9 chaque colonne correspond \u00e0 une variable, et chaque ligne correspond \u00e0 un individu. On peut voir l'ACP comme le choix du sous-espace de dimension \\(q\\) tel que le nuage de points projet\u00e9s ait la variance la plus grande possible. Les r\u00e9sultats d'une ACP peuvent \u00eatre affich\u00e9s sous la forme d'un nuage de points 2D ou 3D ( \\(q = 2\\) ou \\(3\\) ) repr\u00e9sentant les diff\u00e9rents individus, avec pour axes les composantes principales. Voici un exemple : L'id\u00e9e est de voir si on peut s\u00e9parer les individus en diff\u00e9rents groupes \u00e0 partir des composantes principales. Pour juger de la qualit\u00e9 d'une ACP, on utilise un type de graphique appel\u00e9 \" cercle des corr\u00e9lations \". Ce graphique 2D repr\u00e9sente sur chaque axe la corr\u00e9lation des \\(p\\) variables d'origine avec les \\(q\\) composantes principales. Chacune des \\(p\\) variables correspond \u00e0 un vecteur sur ce graphique, et un cercle de rayon 1 est \u00e9galement affich\u00e9 pour comparaison. Voici un exemple : Un cercle des corr\u00e9lations permet donc de juger de la corr\u00e9lation des variables d'origines avec les composantes principales, et de la corr\u00e9lation des variables d'origine entre elles : Plus une variable d'origine est proche du cercle, plus elle est fid\u00e8lement repr\u00e9sent\u00e9e par l'ACP. Dans l'id\u00e9al, on voudrait donc que toutes les variables soient proches du cercle. Pour 2 variables d'origine proches du cercle, si l'angle entre 2 les variables est aigu elles sont corr\u00e9l\u00e9es, s'il est obtu elles sont anti-corr\u00e9l\u00e9es, et s'il est droit elles sont d\u00e9corr\u00e9l\u00e9es. On pourra utiliser la projection des donn\u00e9es renvoy\u00e9e par l'ACP pour entrainer des mod\u00e8les d'apprentissage. Astuce Python La classe \"sklearn.decomposition.PCA\" de la biblioth\u00e8que \"Scikit-Learn\" vous permet de r\u00e9aliser l'ACP d'une matrice de donn\u00e9es. Le nombre de composantes principales \u00e0 trouver est un des attributs de la classe \u00e0 initialiser (\"n_components\"). Pour obtenir les composantes principales d'une matrice de donn\u00e9es, il faut lui appliquer la m\u00e9thode \"fit_transform()\" de la classe. Pour aller plus loin : D'autres m\u00e9thodes de r\u00e9duction de dimensionnalit\u00e9 existent, on peut citer entre autres les \"auto-encodeurs\" et la \"t-SNE\".","title":"Analyse en Composantes Principales (ACP)"},{"location":"Chap1_Introduction/#preparation-des-donnees","text":"Une fois les donn\u00e9es analys\u00e9es, on a normalement une bonne id\u00e9e de ce qu'un outil automatique pourra en \"apprendre\" ou non. Cependant, la plupart de ces outils (dont nous parlerons dans la section suivante), ont besoin que les donn\u00e9es soient \" transform\u00e9es \" d'une certaine mani\u00e8re. C'est pourquoi nous allons voir dans cette section quelques transformations classiques pour pr\u00e9parer nos donn\u00e9es . Tout d'abord, il est possible que le jeu de donn\u00e9es contienne des valeurs erron\u00e9es ou manquantes , souvent marqu\u00e9es par des NaN (\"Not a Number\"). Il convient alors de se d\u00e9barrasser de ces valeurs avant apprentissage, car la plupart des outils ne savent pas g\u00e9rer ce probl\u00e8me. Astuce Python Dans la biblioth\u00e8que Python \"Pandas\", dont nous reparlerons plus tard dans ce chapitre, il y a une m\u00e9thode \"dropna\" associ\u00e9e aux objets DataFrames. Cette m\u00e9thode permet de supprimer les NaN d'un DataFrame. Nous avons aussi vu pr\u00e9c\u00e9demment que les donn\u00e9es qualitatives doivent \u00eatre encod\u00e9es avant apprentissage, soit en \"ordinal\", soit en \"one-hot\". Enfin, les outils d'apprentissage sont affect\u00e9s par les diff\u00e9rences d'ordre de grandeur entre les variables . C'est pourquoi une remise \u00e0 l'\u00e9chelle des diff\u00e9rentes variables d'un jeu de donn\u00e9es est n\u00e9cessaire avant apprentissage. On appelle ce processus recalibration , ou \" feature scaling \" en anglais. Nous allons voir en particulier 2 types de transformation pour recalibrer des donn\u00e9es : la transformation min-max et le centrage-r\u00e9duction .","title":"Pr\u00e9paration des donn\u00e9es"},{"location":"Chap1_Introduction/#transformation-min-max","text":"Certains types de mod\u00e8les d'apprentissage n\u00e9cessitent des valeurs d'entr\u00e9e entre 0 et 1. C'est pourquoi la transformation min-max (\"normalization\" en anglais) va recalibrer toutes les variables de mani\u00e8re \u00e0 ce que leurs valeurs restent entre 0 et 1 . Pour ce faire, on va appliquer la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x_i-min_j}{max_j-min_j}\\) avec \\(min_j\\) le minimum et \\(max_j\\) le maximum des individus de la j-\u00e8me variable. (Il est \u00e9galement possible d'adapter cette transformation pour les mod\u00e8les prenant des valeurs entre -1 et 1 en entr\u00e9e). Le probl\u00e8me majeur avec cette transformation est sa sensibilit\u00e9 aux valeurs aberrantes. En effet, il suffit qu'une variable ait une valeur aberrante pour qu'elle devienne le minimum ou le maximum, impactant ainsi la transformation. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"MinMaxScaler\".","title":"Transformation min-max"},{"location":"Chap1_Introduction/#centrage-reduction","text":"La transformation centrage-reduction (\"standardization\" en anglais) applique la formule suivante au i-\u00e8me individu \\(x_i\\) de la j-\u00e8me variable d'un jeu de donn\u00e9es : \\(\\frac{x-\\overline{x_p}}{\\sigma_p}\\) avec \\(\\overline{x_p}\\) la moyenne et \\(\\sigma_p\\) l'\u00e9cart-type des individus de la j-\u00e8me variable. Cette transformation est beaucoup moins sensible aux valeurs aberrantes, mais elle ne garanti pas que les valeurs des diff\u00e9rentes variables seront entre 0 et 1 (ou -1 et 1). Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" contient une fonction \"StandardScaler\".","title":"Centrage-r\u00e9duction"},{"location":"Chap1_Introduction/#autres-transformations","text":"Nous l'avons pr\u00e9c\u00e9demment, on peut d\u00e9couvrir que les individus d'une variable ont une distribution asym\u00e9trique. Par exemple, la distribution des individus peut avoir une longue tra\u00eene d'un c\u00f4t\u00e9 de la m\u00e9diane. On peut aussi avoir une distribution multimodale (c'est-\u00e0-dire avec plusieurs pics). Ceci peut perturber un apprentissage automatique. Dans ces situations, d'autres types de transformation pourrons alors \u00eatre envisag\u00e9es en addition des 2 pr\u00e9c\u00e9dentes : utiliser la racine carr\u00e9e ou le logarithme de la variable, utiliser les quantiles de la variable, utiliser un encodage de la variable, etc. Astuce Python La classe \"sklearn.preprocessing\" de la biblioth\u00e8que \"Scikit-Learn\" permet de cr\u00e9er sa propre transformation, avec \"FunctionTransformer\".","title":"Autres transformations"},{"location":"Chap1_Introduction/#les-apprentissages","text":"Une fois que l'on a bien cern\u00e9 notre jeu de donn\u00e9es, et qu'on l'a transform\u00e9 de mani\u00e8re ad\u00e9quate, on va en g\u00e9n\u00e9ral vouloir le mod\u00e9liser . L'id\u00e9e du mod\u00e8le sera de prendre une d\u00e9cision sur \u00e0 partir de nouvelles donn\u00e9es, en se basant sur la connaissance des donn\u00e9es de la base d'origine. On parle alors d'\"apprendre\" des donn\u00e9es.","title":"Les apprentissages"},{"location":"Chap1_Introduction/#lapprentissage-automatique","text":"Par \"mod\u00e9liser\", on entend trouver une fonction param\u00e9trique \\(M\\) qui permet de d\u00e9duire une sortie vectorielle \\(y\\) voulue \u00e0 partir d'une entr\u00e9e vectorielle de nouvelles donn\u00e9es \\(x\\) et de nos connaissances sur les donn\u00e9es d'origine : \\(y = M(x,\\theta)\\) avec \\(\\theta\\) les param\u00e8tres du mod\u00e8le, qui correspondent \u00e0 notre connaissance du jeu de donn\u00e9es initial. Il nous faut donc ajuster les param\u00e8tres \\(\\theta\\) pour obtenir la sortie \\(y\\) attendue en fonction de \\(x\\) qui colle le plus aux donn\u00e9es. C'est ce processus d' optimisation de \\(\\theta\\) que l'on appelle \" apprentissage \". Les jeux de donn\u00e9es dont on doit apprendre sont en g\u00e9n\u00e9ral \u00e9normes, ce qui rend souvent un ajustement manuel des param\u00e8tres impossible. C'est pourquoi on va en g\u00e9n\u00e9ral choisir un type de mod\u00e8le , et ajuster automatiquement les param\u00e8tres \u00e0 nos donn\u00e9es. D'o\u00f9 l'expression \" apprentissage automatique \". Suivant les applications, il existe diff\u00e9rents types d'apprentissage, avec pour chacun diff\u00e9rents types de mod\u00e8les possibles. Lors de ce cours, nous verrons 3 grands types d'apprentissage, et nous verrons pour chacun quelques exemples de mod\u00e8les classiques.","title":"L'apprentissage automatique"},{"location":"Chap1_Introduction/#les-3-grands-types-dapprentissages","text":"En apprentissage, on appelle souvent en anglais les entr\u00e9e d'un mod\u00e8le les \" features \", et les sortie des \" labels \". Lors du processus d' apprentissage (ajustement des param\u00e8tres), on va enseigner au mod\u00e8le comment d\u00e9terminer des \"labels\" correspondant \u00e0 des \"features\", en se basant sur ce qu'il a appris d'une base de donn\u00e9es d'\"entrainement\" de \"features\".","title":"Les 3 grands types d'apprentissages"},{"location":"Chap1_Introduction/#apprentissage-supervise-ou-non-supervise","text":"Dans le cas o\u00f9 les donn\u00e9es d'apprentissage ont des \"labels\" d\u00e9finis, le mod\u00e8le va apprendre \u00e0 retrouver ces \"labels\" (connus) pour ces \"features\". On esp\u00e8re alors qu'apr\u00e8s apprentissage, le mod\u00e8le pourra retourner les \"labels\" corrects une fois confront\u00e9 \u00e0 des \"features\" issus de nouvelles donn\u00e9es. On parle alors de \" g\u00e9n\u00e9ralisation \". Comme on peut directement v\u00e9rifier les performances du mod\u00e8le \u00e0 pr\u00e9dire les \"labels\" du jeu de donn\u00e9es d'entrainement, on parle d' apprentissage supervis\u00e9 . Dans le cas o\u00f9 les donn\u00e9es d'apprentissage n'ont pas de \"labels\", on peut tout de m\u00eame essayer de diviser les individus des \"features\" en diff\u00e9rent groupes, auxquels on assignera des \"labels\" plus tard. Comme nous n'avons pas de \"labels\" d'entrainement comme r\u00e9f\u00e9rence, on parle d' apprentissage non-supervis\u00e9 ou \"clustering\" (\"partition de donn\u00e9es\").","title":"Apprentissage supervis\u00e9 ou non-supervis\u00e9"},{"location":"Chap1_Introduction/#classification-et-regression","text":"On peut aussi diviser les apprentissages suivant le type de sortie attendu, et donc de mod\u00e8le \u00e0 entrainer. Si la sortie est un groupe d'individus auxquel on veut assigner un nouvel individu, on va parler de \" classification \". Si la sortie est un vecteur de valeurs de variables pour un nouvel individu, on va parler de \" r\u00e9gression \". On peut entrainer un mod\u00e8le de classification de mani\u00e8re supervis\u00e9e ou non-supervis\u00e9e . On ne peut entrainer un mod\u00e8le de r\u00e9gression que de mani\u00e8re supervis\u00e9e .","title":"Classification et r\u00e9gression"},{"location":"Chap1_Introduction/#pour-aller-plus-loin","text":"Il existe un 3\u00e8me type d'apprentissage, que nous ne d\u00e9taillerons pas dans ce cours, qui s'appelle \"apprentissage par renforcement \". L'id\u00e9e est la suivante : Le mod\u00e8le est directement mis en place sur son cas d'application final. Le mod\u00e8le prend des d\u00e9cisions en fonction des situations, et re\u00e7oit un retour (\"feedback\") sur sa d\u00e9cision, positif ou n\u00e9gatif. Le mod\u00e8le se met \u00e0 jour en fonction du retour qu'il a re\u00e7u. Ce processus se r\u00e9p\u00e8te pour chaque nouvelle situation, et ainsi le mod\u00e8le apprend de ses exp\u00e9riences .","title":"Pour aller plus loin..."},{"location":"Chap1_Introduction/#difficultes-de-lapprentissage","text":"Comme expliqu\u00e9 plus haut, l' apprentissage est un processus d' optimisation , qui consiste en l' ajustement des param\u00e8tres d'un mod\u00e8le en se basant sur les donn\u00e9es disponibles, dans le but de prendre des d\u00e9cisions correctes \u00e0 partir de donn\u00e9es futures ( g\u00e9n\u00e9ralisation ). La phase durant laquelle on ajuste les param\u00e8tres est appel\u00e9e entra\u00eenement , et les donn\u00e9es sur lesquelles cet ajustement est fait sont appel\u00e9es \" base de donn\u00e9es d'entra\u00eenement \". Dans la section qui suit, nous aurons un aper\u00e7u des grandes difficult\u00e9es que l'on peut rencontrer lors de l'entrainement d'un mod\u00e8le, tous types de mod\u00e8les confondus.","title":"Difficult\u00e9s de l'apprentissage"},{"location":"Chap1_Introduction/#quantite-et-qualite-des-donnees","text":"S'il n'y a pas de r\u00e8gle pr\u00e9cise pour d\u00e9terminer la quantit\u00e9 de donn\u00e9es n\u00e9cessaire \u00e0 un apprentissage, il y a 2 maximes \u00e0 retenir : Plus on a donn\u00e9es d'entrainement, meilleur sera l'apprentissage par le mod\u00e8le. Plus le probl\u00e8me complexe, plus il faudra de donn\u00e9es d'entrainement. Pour donner un ordre de grandeur, la quantit\u00e9 d'individus n\u00e9cessaires \u00e0 un apprentissage va en g\u00e9n\u00e9ral de quelques milliers \u00e0 des centaines de millions . Cependant, il n'est pas ais\u00e9 de constituer une base de donn\u00e9es aussi large, et de surcroit une base de donn\u00e9e de qualit\u00e9. En effet, comme on peut facilement le deviner, la qualit\u00e9 des donn\u00e9es aura un impact sur l'apprentissage. La qualit\u00e9 des donn\u00e9es peut par exemple \u00eatre d\u00e9grad\u00e9e par : La pr\u00e9sence d'individus ab\u00e9rrants (\"outliers\"), li\u00e9e \u00e0 des erreurs de mesures ou \u00e0 des cas exceptionnels. Des individus manquants, li\u00e9s \u00e0 notre \u00e9chantillonnage ou a des erreurs de mesures. La pr\u00e9sence de bruit dans les donn\u00e9es. D'o\u00f9 la n\u00e9cessit\u00e9 de proc\u00e9der \u00e0 un nettoyage des donn\u00e9es en amont de l'apprentissage : supprimer certaines donn\u00e9es, les combler, ou faire de nouvelles mesures.","title":"Quantit\u00e9 et qualit\u00e9 des donn\u00e9es"},{"location":"Chap1_Introduction/#representativite-et-equilibre-des-donnees","text":"","title":"Repr\u00e9sentativit\u00e9 et \u00e9quilibre des donn\u00e9es"},{"location":"Chap1_Introduction/#pertinence-des-variables","text":"","title":"Pertinence des variables"},{"location":"Chap1_Introduction/#sur-apprentissage-sous-apprentissage","text":"","title":"Sur-apprentissage / sous-apprentissage"},{"location":"Chap1_Introduction/#test-validation-et-hyperparametres","text":"","title":"Test, validation et hyperparam\u00e8tres"},{"location":"Chap1_Introduction/#import-de-donnees-et-fichiers-csv","text":"","title":"Import de donn\u00e9es et fichiers CSV"},{"location":"Chap1_Introduction/#outils-python-pour-lapprentissage","text":"","title":"Outils Python pour l'apprentissage"},{"location":"Chap1_Introduction/#pandas","text":"","title":"Pandas"},{"location":"Chap1_Introduction/#scikit-learn","text":"","title":"Scikit-Learn"},{"location":"Chap1_Introduction/#keras-tensorflow-pytorch","text":"","title":"Keras-Tensorflow, Pytorch"},{"location":"Chap2_Classification_supervisee/","text":"Chapitre II : Classification supervis\u00e9e Probl\u00e8me de classification","title":"II. Classification supervis\u00e9e"},{"location":"Chap2_Classification_supervisee/#chapitre-ii-classification-supervisee","text":"","title":"Chapitre II : Classification supervis\u00e9e"},{"location":"Chap2_Classification_supervisee/#probleme-de-classification","text":"","title":"Probl\u00e8me de classification"},{"location":"Chap3_Regression/","text":"Chapitre III : R\u00e9gression Probl\u00e8me de r\u00e9gression","title":"III. R\u00e9gression"},{"location":"Chap3_Regression/#chapitre-iii-regression","text":"","title":"Chapitre III : R\u00e9gression"},{"location":"Chap3_Regression/#probleme-de-regression","text":"","title":"Probl\u00e8me de r\u00e9gression"},{"location":"Chap4_Clustering/","text":"Chapitre IV : Clustering Probl\u00e8me de partitionnement","title":"IV. Clustering"},{"location":"Chap4_Clustering/#chapitre-iv-clustering","text":"","title":"Chapitre IV : Clustering"},{"location":"Chap4_Clustering/#probleme-de-partitionnement","text":"","title":"Probl\u00e8me de partitionnement"}]}